% Thesis-related publications
@INPROCEEDINGS{rsds,
    author = {Böhm, Stanislav and Beránek, Jakub},
    booktitle = {2020 IEEE/ACM Workflows in Support of Large-Scale Science (WORKS)},
    title = {Runtime vs Scheduler: Analyzing Dask’s Overheads},
    year = {2020},
    volume = {},
    number = {},
    pages = {1-8},
    doi = "10.1109/WORKS51914.2020.00006",
    meta = {thesis_related},
}
@Article{estee,
    author="Ber{\'a}nek, Jakub
    and B{\"o}hm, Stanislav
    and Cima, Vojt{\v{e}}ch",
    title="Analysis of workflow schedulers in simulated distributed environments",
    journal="The Journal of Supercomputing",
    year="2022",
    month="Sep",
    day="01",
    volume="78",
    number="13",
    pages="15154--15180",
    issn="1573-0484",
    doi="10.1007/s11227-022-04438-y",
    %url="https://doi.org/10.1007/s11227-022-04438-y"
    meta = {thesis_related},
}

@inbook{spin2,
    author = {Di Girolamo, Salvatore and Kurth, Andreas and Calotoiu, Alexandru and Benz, Thomas and Schneider, Timo and Ber\'{a}nek, Jakub and Benini, Luca and Hoefler, Torsten},
    title = {A RISC-V in-Network Accelerator for Flexible High-Performance Low-Power Packet Processing},
    year = {2021},
    isbn = {9781450390866},
    publisher = {IEEE Press},
    %    url = {https://doi.org/10.1109/ISCA52012.2021.00079},
    abstract = {The capacity of offloading data and control tasks to the network is becoming increasingly important, especially if we consider the faster growth of network speed when compared to CPU frequencies. In-network compute alleviates the host CPU load by running tasks directly in the network, enabling additional computation/communication overlap and potentially improving overall application performance. However, sustaining bandwidths provided by next-generation networks, e.g., 400 Gbit/s, can become a challenge. sPIN is a programming model for in-NIC compute, where users specify handler functions that are executed on the NIC, for each incoming packet belonging to a given message or flow. It enables a CUDA-like acceleration, where the NIC is equipped with lightweight processing elements that process network packets in parallel. We investigate the architectural specialties that a sPIN NIC should provide to enable high-performance, low-power, and flexible packet processing. We introduce PsPIN, a first open-source sPIN implementation, based on a multi-cluster RISC-V architecture and designed according to the identified architectural specialties. We investigate the performance of PsPIN with cycle-accurate simulations, showing that it can process packets at 400 Gbit/s for several use cases, introducing minimal latencies (26 ns for 64 B packets) and occupying a total area of 18.5 mm2 (22 nm FDSOI).},
    booktitle = {Proceedings of the 48th Annual International Symposium on Computer Architecture},
    pages = {958–971},
    numpages = {14},
    meta = {thesis_related},
}
@inproceedings{spin,
    author = {Di Girolamo, Salvatore and Taranov, Konstantin and Kurth, Andreas and Schaffner, Michael and Schneider, Timo and Ber\'{a}nek, Jakub and Besta, Maciej and Benini, Luca and Roweth, Duncan and Hoefler, Torsten},
    title = {Network-Accelerated Non-Contiguous Memory Transfers},
    year = {2019},
    isbn = {9781450362290},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    %    url = {https://doi.org/10.1145/3295500.3356189},
    abstract = {Applications often communicate data that is non-contiguous in the send- or the receive-buffer, e.g., when exchanging a column of a matrix stored in row-major order. While non-contiguous transfers are well supported in HPC (e.g., MPI derived datatypes), they can still be up to 5x slower than contiguous transfers of the same size. As we enter the era of network acceleration, we need to investigate which tasks to offload to the NIC: In this work we argue that non-contiguous memory transfers can be transparently network-accelerated, truly achieving zero-copy communications. We implement and extend sPIN, a packet streaming processor, within a Portals 4 NIC SST model, and evaluate strategies for NIC-offloaded processing of MPI datatypes, ranging from datatype-specific handlers to general solutions for any MPI datatype. We demonstrate up to 8x speedup in the unpack throughput of real applications, demonstrating that non-contiguous memory transfers are a first-class candidate for network acceleration.},
    booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
    articleno = {56},
    numpages = {14},
    location = {Denver, Colorado},
    series = {SC '19},
    meta = {thesis_related},
}

% Thesis-unrelated publications
@article{graphminesuite,
    author = {Besta, Maciej and Vonarburg-Shmaria, Zur and Schaffner, Yannick and Schwarz, Leonardo and Kwasniewski, Grzegorz and Gianinazzi, Lukas and Beranek, Jakub and Janda, Kacper and Holenstein, Tobias and Leisinger, Sebastian and Tatkowski, Peter and Ozdemir, Esref and Balla, Adrian and Copik, Marcin and Lindenberger, Philipp and Konieczny, Marek and Mutlu, Onur and Hoefler, Torsten},
    title = {GraphMineSuite: Enabling High-Performance and Programmable Graph Mining Algorithms with Set Algebra},
    year = {2021},
    issue_date = {July 2021},
    publisher = {VLDB Endowment},
    volume = {14},
    number = {11},
    issn = {2150-8097},
    %    url = {https://doi.org/10.14778/3476249.3476252},
    doi = {10.14778/3476249.3476252},
    abstract = {We propose GraphMineSuite (GMS): the first benchmarking suite for graph mining that facilitates evaluating and constructing high-performance graph mining algorithms. First, GMS comes with a benchmark specification based on extensive literature review, prescribing representative problems, algorithms, and datasets. Second, GMS offers a carefully designed software platform for seamless testing of different fine-grained elements of graph mining algorithms, such as graph representations or algorithm subroutines. The platform includes parallel implementations of more than 40 considered baselines, and it facilitates developing complex and fast mining algorithms. High modularity is possible by harnessing set algebra operations such as set intersection and difference, which enables breaking complex graph mining algorithms into simple building blocks that can be separately experimented with. GMS is supported with a broad concurrency analysis for portability in performance insights, and a novel performance metric to assess the throughput of graph mining algorithms, enabling more insightful evaluation. As use cases, we harness GMS to rapidly redesign and accelerate state-of-the-art baselines of core graph mining problems: degeneracy reordering (by &gt;2X), maximal clique listing (by &gt;9\texttimes{}), k-clique listing (by up to 1.1\texttimes{}), and subgraph isomorphism (by 2.5\texttimes{}), also obtaining better theoretical performance bounds.},
    journal = {Proc. VLDB Endow.},
    month = {7},
    pages = {1922–1935},
    numpages = {14},
    meta = {thesis_unrelated},
}
@inbook{sisa,
    author = {Besta, Maciej and Kanakagiri, Raghavendra and Kwasniewski, Grzegorz and Ausavarungnirun, Rachata and Ber\'{a}nek, Jakub and Kanellopoulos, Konstantinos and Janda, Kacper and Vonarburg-Shmaria, Zur and Gianinazzi, Lukas and Stefan, Ioana and Luna, Juan G\'{o}mez and Golinowski, Jakub and Copik, Marcin and Kapp-Schwoerer, Lukas and Di Girolamo, Salvatore and Blach, Nils and Konieczny, Marek and Mutlu, Onur and Hoefler, Torsten},
    title = {SISA: Set-Centric Instruction Set Architecture for Graph Mining on Processing-in-Memory Systems},
    year = {2021},
    isbn = {9781450385572},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    %    url = {https://doi.org/10.1145/3466752.3480133},
    abstract = { Simple graph algorithms such as PageRank have been the target of numerous hardware accelerators. Yet, there also exist much more complex graph mining algorithms for problems such as clustering or maximal clique listing. These algorithms are memory-bound and thus could be accelerated by hardware techniques such as Processing-in-Memory (PIM). However, they also come with non-straightforward parallelism and complicated memory access patterns. In this work, we address this problem with a simple yet surprisingly powerful observation: operations on sets of vertices, such as intersection or union, form a large part of many complex graph mining algorithms, and can offer rich and simple parallelism at multiple levels. This observation drives our cross-layer design, in which we (1) expose set operations using a novel programming paradigm, (2) express and execute these operations efficiently with carefully designed set-centric ISA extensions called SISA, and (3) use PIM to accelerate SISA instructions. The key design idea is to alleviate the bandwidth needs of SISA instructions by mapping set operations to two types of PIM: in-DRAM bulk bitwise computing for bitvectors representing high-degree vertices, and near-memory logic layers for integer arrays representing low-degree vertices. Set-centric SISA-enhanced algorithms are efficient and outperform hand-tuned baselines, offering more than 10 \texttimes{} speedup over the established Bron-Kerbosch algorithm for listing maximal cliques. We deliver more than 10 SISA set-centric algorithm formulations, illustrating SISA’s wide applicability. },
    booktitle = {MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture},
    pages = {282–297},
    numpages = {16},
    meta = {thesis_unrelated},
}
@inproceedings{smi,
    author = {De Matteis, Tiziano and de Fine Licht, Johannes and Ber\'{a}nek, Jakub and Hoefler, Torsten},
    title = {Streaming Message Interface: High-Performance Distributed Memory Programming on Reconfigurable Hardware},
    year = {2019},
    isbn = {9781450362290},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    %    url = {https://doi.org/10.1145/3295500.3356201},
    abstract = {Distributed memory programming is the established paradigm used in high-performance computing (HPC) systems, requiring explicit communication between nodes and devices. When FPGAs are deployed in distributed settings, communication is typically handled either by going through the host machine, sacrificing performance, or by streaming across fixed device-to-device connections, sacrificing flexibility. We present Streaming Message Interface (SMI), a communication model and API that unifies explicit message passing with a hardware-oriented programming model, facilitating minimal-overhead, flexible, and productive inter-FPGA communication. Instead of bulk transmission, messages are streamed across the network during computation, allowing communication to be seamlessly integrated into pipelined designs. We present a high-level synthesis implementation of SMI targeting a dedicated FPGA interconnect, exposing runtime-configurable routing with support for arbitrary network topologies, and implement a set of distributed memory benchmarks. Using SMI, programmers can implement distributed, scalable HPC programs on reconfigurable hardware, without deviating from best practices for hardware design.},
    booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
    articleno = {82},
    numpages = {33},
    location = {Denver, Colorado},
    series = {SC '19},
    meta = {thesis_unrelated},
}
@inproceedings{golasowski_alternative_2020,
    address = {Cham},
    series = {Advances in {Intelligent} {Systems} and {Computing}},
    title = {Alternative {Paths} {Reordering} {Using} {Probabilistic} {Time}-{Dependent} {Routing}},
    copyright = {All rights reserved},
    isbn = {978-3-030-29029-0},
    doi = {10.1007/978-3-030-29029-0_22},
    language = {en},
    booktitle = {Advances in {Networked}-based {Information} {Systems}},
    publisher = {Springer International Publishing},
    author = {Golasowski, Martin and Beránek, Jakub and Šurkovský, Martin and Rapant, Lukáš and Szturcová, Daniela and Martinovič, Jan and Slaninová, Kateřina},
    editor = {Barolli, Leonard and Nishino, Hiroaki and Enokido, Tomoya and Takizawa, Makoto},
    year = {2020},
    pages = {235--246},
}
@inproceedings{martinovic_distributed_2020,
    address = {Cham},
    series = {Advances in {Intelligent} {Systems} and {Computing}},
    title = {A {Distributed} {Environment} for {Traffic} {Navigation} {Systems}},
    copyright = {All rights reserved},
    isbn = {978-3-030-22354-0},
    doi = {10.1007/978-3-030-22354-0_27},
    language = {en},
    booktitle = {Complex, {Intelligent}, and {Software} {Intensive} {Systems}},
    publisher = {Springer International Publishing},
    author = {Martinovič, Jan and Golasowski, Martin and Slaninová, Kateřina and Beránek, Jakub and Šurkovský, Martin and Rapant, Lukáš and Szturcová, Daniela and Cmar, Radim},
    editor = {Barolli, Leonard and Hussain, Farookh Khadeer and Ikeda, Makoto},
    year = {2020},
    pages = {294--304},
}
@InProceedings{haydi,
    author = "B{\"o}hm, Stanislav
    and Ber{\'a}nek, Jakub
    and {\v{S}}urkovsk{\'y}, Martin",
    editor = "Ferrarotti, Flavio
    and Woltran, Stefan",
    title = "Haydi: Rapid Prototyping and Combinatorial Objects",
    booktitle = "Foundations of Information and Knowledge Systems",
    year = "2018",
    publisher = "Springer International Publishing",
    address = "Cham",
    pages = "133--149",
    abstract = "Haydi (http://haydi.readthedocs.io) is a framework for generating discrete structures. It provides a way to define a structure from basic building blocks and then enumerate all elements, all non-isomorphic elements, or generate random elements in the structure. Haydi is designed as a tool for rapid prototyping. It is implemented as a pure Python package and supports execution in distributed environments. The goal of this paper is to give the overall picture of Haydi together with a formal definition for the case of generating canonical forms.",
    isbn = "978-3-319-90050-6",
    meta = {thesis_unrelated},
}

% References
@article{Ullman1975,
    author = {Ullman, J. D.},
    title = {NP-complete Scheduling Problems},
    journal = {J. Comput. Syst. Sci.},
    issue_date = {June, 1975},
    volume = {10},
    number = {3},
    month = jun,
    year = {1975},
    issn = {0022-0000},
    pages = {384--393},
    numpages = {10},
    %    url = {http://dx.doi.org/10.1016/S0022-0000(75)80008-0},
    doi = {10.1016/S0022-0000(75)80008-0},
    acmid = {1740138},
    publisher = {Academic Press, Inc.},
    address = {Orlando, FL, USA},
}
@article{hlfet1974,
    author = {Adam, Thomas L. and Chandy, K. M. and Dickson, J. R.},
    title = {A Comparison of List Schedules for Parallel Processing Systems},
    journal = {Commun. ACM},
    issue_date = {Dec 1974},
    volume = {17},
    number = {12},
    month = dec,
    year = {1974},
    issn = {0001-0782},
    pages = {685--690},
    numpages = {6},
    %    url = {http://doi.acm.org/10.1145/361604.361619},
    doi = {10.1145/361604.361619},
    acmid = {361619},
    publisher = {ACM},
    address = {New York, NY, USA},
}
@inproceedings{kwok1998benchmarking,
    title = {Benchmarking the task graph scheduling algorithms},
    author = {Kwok, Yu-Kwong and Ahmad, Ishfaq},
    booktitle = {ipps},
    pages = {0531},
    year = {1998},
    organization = {IEEE}
}
@article{hagras2003static,
    title = {Static vs. dynamic list-scheduling performance comparison},
    author = {Hagras, Tarek and Jane{\v{c}}ek, J},
    journal = {Acta Polytechnica},
    volume = {43},
    number = {6},
    year = {2003}
}
@article{wang2018list,
    title = {List-Scheduling vs. Cluster-Scheduling},
    author = {Wang, Huijun and Sinnen, Oliver},
    journal = {IEEE Transactions on Parallel and Distributed Systems},
    year = {2018},
    publisher = {IEEE}
}
@inproceedings{dask,
    author = {Rocklin, Matthew},
    year = {2015},
    month = {01},
    pages = {126-132},
    title = {Dask: Parallel Computation with Blocked algorithms and Task Scheduling},
    doi = {10.25080/Majora-7b98e3ed-013}
}
@article{snakemake,
    author = {Köster, Johannes and Rahmann, Sven},
    title = "{Snakemake—a scalable bioinformatics workflow engine}",
    journal = {Bioinformatics},
    volume = {28},
    number = {19},
    pages = {2520-2522},
    year = {2012},
    month = {08},
    abstract = "{Summary: Snakemake is a workflow engine that provides a readable Python-based workflow definition language and a powerful execution environment that scales from single-core workstations to compute clusters without modifying the workflow. It is the first system to support the use of automatically inferred multiple named wildcards (or variables) in input and output filenames.Availability:http://snakemake.googlecode.com.Contact:johannes.koester@uni-due.de}",
    issn = {1367-4803},
    doi = {10.1093/bioinformatics/bts480},
    %    url = "https://doi.org/10.1093/bioinformatics/bts480",
    eprint = {https://academic.oup.com/bioinformatics/article-pdf/28/19/2520/819790/bts480.pdf},
}
@article{nextflow,
    author = {Di Tommaso, Paolo and Chatzou, Maria and Floden, Evan W. and Barja, Pablo and Palumbo, Emilio and Notredame, Cedric},
    year = {2017},
    month = {04},
    pages = {316-319},
    title = {Nextflow enables reproducible computational workflows},
    volume = {35},
    journal = {Nature Biotechnology},
    doi = {10.1038/nbt.3820}
}
@article{pycompss,
    author = {Tejedor, Enric and Becerra, Yolanda and Alomar, Guillem and Queralt, Anna and Badia, Rosa M. and Torres, Jordi and Cortes, Toni and Labarta, Jesús},
    year = {2015},
    month = {08},
    pages = {},
    title = {PyCOMPSs: Parallel computational workflows in Python},
    volume = {31},
    journal = {International Journal of High Performance Computing Applications},
    doi = {10.1177/1094342015594678}
}
@inproceedings{parsl,
    author = {Babuji, Yadu and Woodard, Anna and Li, Zhuozhao and Katz, Daniel S. and Clifford, Ben and Kumar, Rohan and Lacinski, Lukasz and Chard, Ryan and Wozniak, Justin M. and Foster, Ian and Wilde, Michael and Chard, Kyle},
    title = {Parsl: Pervasive Parallel Programming in Python},
    year = {2019},
    isbn = {9781450366700},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    %    url = {https://doi.org/10.1145/3307681.3325400},
    doi = {10.1145/3307681.3325400},
    abstract = {High-level programming languages such as Python are increasingly used to provide intuitive interfaces to libraries written in lower-level languages and for assembling applications from various components. This migration towards orchestration rather than implementation, coupled with the growing need for parallel computing (e.g., due to big data and the end of Moore's law), necessitates rethinking how parallelism is expressed in programs. Here, we present Parsl, a parallel scripting library that augments Python with simple, scalable, and flexible constructs for encoding parallelism. These constructs allow Parsl to construct a dynamic dependency graph of components that it can then execute efficiently on one or many processors. Parsl is designed for scalability, with an extensible set of executors tailored to different use cases, such as low-latency, high-throughput, or extreme-scale execution. We show, via experiments on the Blue Waters supercomputer, that Parsl executors can allow Python scripts to execute components with as little as 5 ms of overhead, scale to more than 250000 workers across more than 8000 nodes, and process upward of 1200 tasks per second. Other Parsl features simplify the construction and execution of composite programs by supporting elastic provisioning and scaling of infrastructure, fault-tolerant execution, and integrated wide-area data management. We show that these capabilities satisfy the needs of many-task, interactive, online, and machine learning applications in fields such as biology, cosmology, and materials science.},
    booktitle = {Proceedings of the 28th International Symposium on High-Performance Parallel and Distributed Computing},
    pages = {25–36},
    numpages = {12},
    location = {Phoenix, AZ, USA},
    series = {HPDC '19}
}
@inproceedings{ray,
    author = {Moritz, Philipp and Nishihara, Robert and Wang, Stephanie and Tumanov, Alexey and Liaw, Richard and Liang, Eric and Elibol, Melih and Yang, Zongheng and Paul, William and Jordan, Michael I. and Stoica, Ion},
    title = {Ray: A Distributed Framework for Emerging AI Applications},
    year = {2018},
    isbn = {9781931971478},
    publisher = {USENIX Association},
    address = {USA},
    abstract = {The next generation of AI applications will continuously interact with the environment and learn from these interactions. These applications impose new and demanding systems requirements, both in terms of performance and flexibility. In this paper, we consider these requirements and present Ray--a distributed system to address them. Ray implements a unified interface that can express both task-parallel and actor-based computations, supported by a single dynamic execution engine. To meet the performance requirements, Ray employs a distributed scheduler and a distributed and fault-tolerant store to manage the system's control state. In our experiments, we demonstrate scaling beyond 1.8 million tasks per second and better performance than existing specialized systems for several challenging reinforcement learning applications.},
    booktitle = {Proceedings of the 13th USENIX Conference on Operating Systems Design and Implementation},
    pages = {561–577},
    numpages = {17},
    location = {Carlsbad, CA, USA},
    series = {OSDI'18}
}
@InProceedings{slurm,
    author = "Yoo, Andy B.
and Jette, Morris A.
and Grondona, Mark",
    doi = "10.1007/10968987_3",
    editor = "Feitelson, Dror
and Rudolph, Larry
and Schwiegelshohn, Uwe",
    title = "SLURM: Simple Linux Utility for Resource Management",
    booktitle = "Job Scheduling Strategies for Parallel Processing",
    year = "2003",
    publisher = "Springer Berlin Heidelberg",
    address = "Berlin, Heidelberg",
    pages = "44--60",
    abstract = "A new cluster resource management system called Simple Linux Utility Resource Management (SLURM) is described in this paper. SLURM, initially developed for large Linux clusters at the Lawrence Livermore National Laboratory (LLNL), is a simple cluster manager that can scale to thousands of processors. SLURM is designed to be flexible and fault-tolerant and can be ported to other clusters of different size and architecture with minimal effort. We are certain that SLURM will benefit both users and system architects by providing them with a simple, robust, and highly scalable parallel job execution environment for their cluster system.",
    isbn = "978-3-540-39727-4"
}
@inproceedings{pbs,
    author = {Staples, Garrick},
    title = {TORQUE Resource Manager},
    year = {2006},
    isbn = {0769527000},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    %    url = {https://doi.org/10.1145/1188455.1188464},
    doi = {10.1145/1188455.1188464},
    abstract = {With TORQUE Resource Manager now reaching over 10,000 downloads per month and use across thousands of leading sites representing commercial, government, and academic organizations, we invite all TORQUE users to meet and discuss TORQUE with the professional developers, community volunteers other members who use and have contributed to the TORQUE project.Here we will discuss the current state of TORQUE including some of the more recent enhancements and capabilities along with the road map for the upcoming year. We will also provide a time for TORQUE users to share experiences, best practices, and new needs.},
    booktitle = {Proceedings of the 2006 ACM/IEEE Conference on Supercomputing},
    pages = {8–es},
    location = {Tampa, Florida},
    series = {SC '06}
}
@inproceedings{mpistudy,
    author = {Laguna, Ignacio and Marshall, Ryan and Mohror, Kathryn and Ruefenacht, Martin and Skjellum, Anthony and Sultana, Nawrin},
    title = {A Large-Scale Study of MPI Usage in Open-Source HPC Applications},
    year = {2019},
    isbn = {9781450362290},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    doi = {10.1145/3295500.3356176},
    booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
    articleno = {31},
    numpages = {14},
    location = {Denver, Colorado},
    series = {SC '19}
}
@online{slurm-schedmd,
    title = {SchedMD Slurm usage statistics},
    url = {https://schedmd.com/},
    urldate = {2022-05-18}
}
@online{dask-client-perf,
    title = {Task graph building performance issue in Dask},
    url = {https://github.com/dask/distributed/issues/3783},
    urldate = {2022-05-25},
}
@online{dask-user-survey,
    title = {2021 Dask User Survey},
    author = {Genevieve Buckley},
    url = {https://blog.dask.org/2021/09/15/user-survey},
    urldate = {2022-06-09},
}
@misc{lumi_it4innovations_2022,
    title = {Lumi, Europe's most powerful supercomputer, is solving global challenges and promoting a green transformation},
    url = {https://www.it4i.cz/en/about/infoservice/press-releases/lumi-europes-most-powerful-supercomputer-is-solving-global-challenges-and-promoting-a-green-transformation},
    year = {2022},
    month = {6},
    urldate = {2022-07-23}
}
@inproceedings{top500analysis,
    author = {Khan, Awais and Sim, Hyogi and Vazhkudai, Sudharshan S. and Butt, Ali R. and Kim, Youngjae},
    title = {An Analysis of System Balance and Architectural Trends Based on Top500 Supercomputers},
    year = {2021},
    isbn = {9781450388429},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3432261.3432263},
    doi = {10.1145/3432261.3432263},
    abstract = {Supercomputer design is a complex, multi-dimensional optimization process, wherein several subsystems need to be reconciled to meet a desired figure of merit performance for a portfolio of applications and a budget constraint. However, overall, the HPC community has been gravitating towards ever more Flops, at the expense of many other subsystems. To draw attention to overall system balance, in this paper, we analyze balance ratios and architectural trends in the world’s most powerful supercomputers. Specifically, we have collected the performance characteristics of systems between 1993 and 2019 based on the Top500 lists and then analyzed their architectures from diverse system design perspectives. Notably, our analysis studies the performance balance of the machines, across a variety of subsystems such as compute, memory, I/O, interconnect, intra-node connectivity and power. Our analysis reveals that balance ratios of the various subsystems need to be considered carefully alongside the application workload portfolio to provision the subsystem capacity and bandwidth specifications, which can help achieve optimal performance.},
    booktitle = {The International Conference on High Performance Computing in Asia-Pacific Region},
    pages = {11–22},
    numpages = {12},
%    keywords = {Architectural Trends and Performance Balance Ratio, High Performance Computing, Top500 Supercomputers},
    location = {Virtual Event, Republic of Korea},
    series = {HPC Asia 2021}
}
@inproceedings{hyperloom,
    author = {Cima, Vojtěch and Böhm, Stanislav and Martinovič, Jan and Dvorský, Jiří and Janurová, Kateřina and Aa, Tom Vander and Ashby, Thomas J. and Chupakhin, Vladimir},
    title = {HyperLoom: A Platform for Defining and Executing Scientific Pipelines in Distributed Environments},
    year = {2018},
    isbn = {9781450364447},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3183767.3183768},
    doi = {10.1145/3183767.3183768},
    pages = {1–6},
    numpages = {6},
%    keywords = {Chemogenomics, Distributed Computing, Task Scheduling, Scientific Pipeline, Big Data, HPC, Machine Learning},
    series = {PARMA-DITAM '18}
}
