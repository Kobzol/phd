% Thesis-related publications
@INPROCEEDINGS{rsds,
  author={Böhm, Stanislav and Beránek, Jakub},
  booktitle={2020 IEEE/ACM Workflows in Support of Large-Scale Science (WORKS)},
  title={Runtime vs Scheduler: Analyzing Dask’s Overheads},
  year={2020},
  volume={},
  number={},
  pages={1-8},
  doi="10.1109/WORKS51914.2020.00006",
  meta = {thesis_related},
}

% Thesis-unrelated publications
@inbook{spin2,
    author = {Di Girolamo, Salvatore and Kurth, Andreas and Calotoiu, Alexandru and Benz, Thomas and Schneider, Timo and Ber\'{a}nek, Jakub and Benini, Luca and Hoefler, Torsten},
    title = {A RISC-V in-Network Accelerator for Flexible High-Performance Low-Power Packet Processing},
    year = {2021},
    isbn = {9781450390866},
    publisher = {IEEE Press},
    url = {https://doi.org/10.1109/ISCA52012.2021.00079},
    abstract = {The capacity of offloading data and control tasks to the network is becoming increasingly important, especially if we consider the faster growth of network speed when compared to CPU frequencies. In-network compute alleviates the host CPU load by running tasks directly in the network, enabling additional computation/communication overlap and potentially improving overall application performance. However, sustaining bandwidths provided by next-generation networks, e.g., 400 Gbit/s, can become a challenge. sPIN is a programming model for in-NIC compute, where users specify handler functions that are executed on the NIC, for each incoming packet belonging to a given message or flow. It enables a CUDA-like acceleration, where the NIC is equipped with lightweight processing elements that process network packets in parallel. We investigate the architectural specialties that a sPIN NIC should provide to enable high-performance, low-power, and flexible packet processing. We introduce PsPIN, a first open-source sPIN implementation, based on a multi-cluster RISC-V architecture and designed according to the identified architectural specialties. We investigate the performance of PsPIN with cycle-accurate simulations, showing that it can process packets at 400 Gbit/s for several use cases, introducing minimal latencies (26 ns for 64 B packets) and occupying a total area of 18.5 mm2 (22 nm FDSOI).},
    booktitle = {Proceedings of the 48th Annual International Symposium on Computer Architecture},
    pages = {958–971},
    numpages = {14},
    meta = {thesis_unrelated},
}
@article{graphminesuite,
    author = {Besta, Maciej and Vonarburg-Shmaria, Zur and Schaffner, Yannick and Schwarz, Leonardo and Kwasniewski, Grzegorz and Gianinazzi, Lukas and Beranek, Jakub and Janda, Kacper and Holenstein, Tobias and Leisinger, Sebastian and Tatkowski, Peter and Ozdemir, Esref and Balla, Adrian and Copik, Marcin and Lindenberger, Philipp and Konieczny, Marek and Mutlu, Onur and Hoefler, Torsten},
    title = {GraphMineSuite: Enabling High-Performance and Programmable Graph Mining Algorithms with Set Algebra},
    year = {2021},
    issue_date = {July 2021},
    publisher = {VLDB Endowment},
    volume = {14},
    number = {11},
    issn = {2150-8097},
    url = {https://doi.org/10.14778/3476249.3476252},
    doi = {10.14778/3476249.3476252},
    abstract = {We propose GraphMineSuite (GMS): the first benchmarking suite for graph mining that facilitates evaluating and constructing high-performance graph mining algorithms. First, GMS comes with a benchmark specification based on extensive literature review, prescribing representative problems, algorithms, and datasets. Second, GMS offers a carefully designed software platform for seamless testing of different fine-grained elements of graph mining algorithms, such as graph representations or algorithm subroutines. The platform includes parallel implementations of more than 40 considered baselines, and it facilitates developing complex and fast mining algorithms. High modularity is possible by harnessing set algebra operations such as set intersection and difference, which enables breaking complex graph mining algorithms into simple building blocks that can be separately experimented with. GMS is supported with a broad concurrency analysis for portability in performance insights, and a novel performance metric to assess the throughput of graph mining algorithms, enabling more insightful evaluation. As use cases, we harness GMS to rapidly redesign and accelerate state-of-the-art baselines of core graph mining problems: degeneracy reordering (by &gt;2X), maximal clique listing (by &gt;9\texttimes{}), k-clique listing (by up to 1.1\texttimes{}), and subgraph isomorphism (by 2.5\texttimes{}), also obtaining better theoretical performance bounds.},
    journal = {Proc. VLDB Endow.},
    month = {jul},
    pages = {1922–1935},
    numpages = {14},
    meta = {thesis_unrelated},
}
@inbook{sisa,
    author = {Besta, Maciej and Kanakagiri, Raghavendra and Kwasniewski, Grzegorz and Ausavarungnirun, Rachata and Ber\'{a}nek, Jakub and Kanellopoulos, Konstantinos and Janda, Kacper and Vonarburg-Shmaria, Zur and Gianinazzi, Lukas and Stefan, Ioana and Luna, Juan G\'{o}mez and Golinowski, Jakub and Copik, Marcin and Kapp-Schwoerer, Lukas and Di Girolamo, Salvatore and Blach, Nils and Konieczny, Marek and Mutlu, Onur and Hoefler, Torsten},
    title = {SISA: Set-Centric Instruction Set Architecture for Graph Mining on Processing-in-Memory Systems},
    year = {2021},
    isbn = {9781450385572},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3466752.3480133},
    abstract = { Simple graph algorithms such as PageRank have been the target of numerous hardware accelerators. Yet, there also exist much more complex graph mining algorithms for problems such as clustering or maximal clique listing. These algorithms are memory-bound and thus could be accelerated by hardware techniques such as Processing-in-Memory (PIM). However, they also come with non-straightforward parallelism and complicated memory access patterns. In this work, we address this problem with a simple yet surprisingly powerful observation: operations on sets of vertices, such as intersection or union, form a large part of many complex graph mining algorithms, and can offer rich and simple parallelism at multiple levels. This observation drives our cross-layer design, in which we (1) expose set operations using a novel programming paradigm, (2) express and execute these operations efficiently with carefully designed set-centric ISA extensions called SISA, and (3) use PIM to accelerate SISA instructions. The key design idea is to alleviate the bandwidth needs of SISA instructions by mapping set operations to two types of PIM: in-DRAM bulk bitwise computing for bitvectors representing high-degree vertices, and near-memory logic layers for integer arrays representing low-degree vertices. Set-centric SISA-enhanced algorithms are efficient and outperform hand-tuned baselines, offering more than 10 \texttimes{} speedup over the established Bron-Kerbosch algorithm for listing maximal cliques. We deliver more than 10 SISA set-centric algorithm formulations, illustrating SISA’s wide applicability. },
    booktitle = {MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture},
    pages = {282–297},
    numpages = {16},
    meta = {thesis_unrelated},
}
@inproceedings{spin,
    author = {Di Girolamo, Salvatore and Taranov, Konstantin and Kurth, Andreas and Schaffner, Michael and Schneider, Timo and Ber\'{a}nek, Jakub and Besta, Maciej and Benini, Luca and Roweth, Duncan and Hoefler, Torsten},
    title = {Network-Accelerated Non-Contiguous Memory Transfers},
    year = {2019},
    isbn = {9781450362290},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3295500.3356189},
    abstract = {Applications often communicate data that is non-contiguous in the send- or the receive-buffer, e.g., when exchanging a column of a matrix stored in row-major order. While non-contiguous transfers are well supported in HPC (e.g., MPI derived datatypes), they can still be up to 5x slower than contiguous transfers of the same size. As we enter the era of network acceleration, we need to investigate which tasks to offload to the NIC: In this work we argue that non-contiguous memory transfers can be transparently network-accelerated, truly achieving zero-copy communications. We implement and extend sPIN, a packet streaming processor, within a Portals 4 NIC SST model, and evaluate strategies for NIC-offloaded processing of MPI datatypes, ranging from datatype-specific handlers to general solutions for any MPI datatype. We demonstrate up to 8x speedup in the unpack throughput of real applications, demonstrating that non-contiguous memory transfers are a first-class candidate for network acceleration.},
    booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
    articleno = {56},
    numpages = {14},
    location = {Denver, Colorado},
    series = {SC '19},
    meta = {thesis_unrelated},
}
@inproceedings{smi,
    author = {De Matteis, Tiziano and de Fine Licht, Johannes and Ber\'{a}nek, Jakub and Hoefler, Torsten},
    title = {Streaming Message Interface: High-Performance Distributed Memory Programming on Reconfigurable Hardware},
    year = {2019},
    isbn = {9781450362290},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3295500.3356201},
    abstract = {Distributed memory programming is the established paradigm used in high-performance computing (HPC) systems, requiring explicit communication between nodes and devices. When FPGAs are deployed in distributed settings, communication is typically handled either by going through the host machine, sacrificing performance, or by streaming across fixed device-to-device connections, sacrificing flexibility. We present Streaming Message Interface (SMI), a communication model and API that unifies explicit message passing with a hardware-oriented programming model, facilitating minimal-overhead, flexible, and productive inter-FPGA communication. Instead of bulk transmission, messages are streamed across the network during computation, allowing communication to be seamlessly integrated into pipelined designs. We present a high-level synthesis implementation of SMI targeting a dedicated FPGA interconnect, exposing runtime-configurable routing with support for arbitrary network topologies, and implement a set of distributed memory benchmarks. Using SMI, programmers can implement distributed, scalable HPC programs on reconfigurable hardware, without deviating from best practices for hardware design.},
    booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
    articleno = {82},
    numpages = {33},
    location = {Denver, Colorado},
    series = {SC '19},
    meta = {thesis_unrelated},
}
@InProceedings{haydi,
    author="B{\"o}hm, Stanislav
    and Ber{\'a}nek, Jakub
    and {\v{S}}urkovsk{\'y}, Martin",
    editor="Ferrarotti, Flavio
    and Woltran, Stefan",
    title="Haydi: Rapid Prototyping and Combinatorial Objects",
    booktitle="Foundations of Information and Knowledge Systems",
    year="2018",
    publisher="Springer International Publishing",
    address="Cham",
    pages="133--149",
    abstract="Haydi (http://haydi.readthedocs.io) is a framework for generating discrete structures. It provides a way to define a structure from basic building blocks and then enumerate all elements, all non-isomorphic elements, or generate random elements in the structure. Haydi is designed as a tool for rapid prototyping. It is implemented as a pure Python package and supports execution in distributed environments. The goal of this paper is to give the overall picture of Haydi together with a formal definition for the case of generating canonical forms.",
    isbn="978-3-319-90050-6",
    meta = {thesis_unrelated},
}

% References
@article{snakemake,
    author = {Köster, Johannes and Rahmann, Sven},
    title = "{Snakemake—a scalable bioinformatics workflow engine}",
    journal = {Bioinformatics},
    volume = {28},
    number = {19},
    pages = {2520-2522},
    year = {2012},
    month = {08},
    abstract = "{Summary: Snakemake is a workflow engine that provides a readable Python-based workflow definition language and a powerful execution environment that scales from single-core workstations to compute clusters without modifying the workflow. It is the first system to support the use of automatically inferred multiple named wildcards (or variables) in input and output filenames.Availability:http://snakemake.googlecode.com.Contact:johannes.koester@uni-due.de}",
    issn = {1367-4803},
    doi = {10.1093/bioinformatics/bts480},
    url = "https://doi.org/10.1093/bioinformatics/bts480",
    eprint = {https://academic.oup.com/bioinformatics/article-pdf/28/19/2520/819790/bts480.pdf},
}
