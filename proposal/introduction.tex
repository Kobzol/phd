High Performance Computing (HPC) infrastructures are crucial for the advancement of scientific
research, as they offer unparalleled computing resources that can be leveraged to perform the most
complex scientific experiments. Over the last several decades, the performance of HPC clusters
has been increasing steadily, effectively doubling every few years. However, such gains are not
always for free. One notable cost associated with the growth of HPC performance is its
increasing complexity. Computing nodes now consist of hundreds of cores; it is quite challenging
to write programs that can scale to such high core counts. They contain hundreds of gigabytes of
memory, which necessitates non-uniform memory architectures (NUMA) that require specialized
programming techniques to achieve optimal performance. HPC clusters are also becoming
increasingly heterogeneous, with devices such as graphics accelerators (GPUs) or reconfigurable
hardware (FPGAs) providing most of their (theoretically attainable) performance. Reaching the
full potential of these accelerators requires using further specialized programming languages
and interfaces.

Historically, optimized HPC software was usually written using system or scientifically focused
programming languages (e.g.~\texttt{C}, \texttt{C++} or \texttt{Fortran}) and specialized libraries
for parallelizing and distributing computation (such as OpenMP and MPI)~\cite{mpistudy}. While
these rather low-level technologies are able to provide the best possible performance, it can be
quite challenging to develop (and maintain) applications that use them. And with the advent of
heterogeneous clusters, even more technologies (such as CUDA for GPUs or HLS for FPGAs) have to
be adopted. It is unreasonable to expect that most scientists that use HPC (who are often not
primarily programmers) will use these low-level technologies directly, as it would make the
development process too slow.

At the same time, the scientific part of HPC is also getting more demanding. Areas such as weather
prediction, machine learning model training or big data analysis require launching thousands or
even millions of simulations and experiments and require a lot of prototyping during development.
The experiments are also quite complex, consisting of multiple dependent steps, such as data
ingestion, preprocessing, computation, postprocessing, visualisation etc. It is thus imperative for
scientists to have a quick way of prototyping these applications, otherwise the development
would just be too slow.

That is why in recent years, HPC users are increasingly moving towards programming paradigms that
allow them to focus on the problem domain, quickly prototype, abstract away most of the complexity
of the underlying system and describe computation that consists of a large number of individual
steps. One such programming paradigm that is very popular is to describe computations
using a set of atomic computing blocks (\emph{tasks}) that are composed together in a task graph
which captures the dependencies between the individual tasks. Using task graphs abstracts away most
of the complexity of network communication and parallelization and allows the user to
express their computation in a simple way. Combined with the fact that
task graphs are usually written in high-level languages, such as Python, or various domain-specific
languages (DSLs), it makes them an ideal tool for rapid scientific prototyping.

While task graphs are already commonly being used and deployed on various distributed systems,
there are certain barriers and factors that limit their development productivity and scalability
when deployed on HPC systems specifically. The primary aim of the proposed thesis is to improve
this situation by identifying and analysing the current productivity and performance bottlenecks
and by researching, designing and implementing tools and approaches that would enable users to
overcome them. The designed approaches should enable executing task graphs on HPC systems in a
way that is both ergonomic for the user and also resource-efficient.

The thesis proposal is structured as follows. Section~\ref{sec:task_graph_definition} introduces
task graphs. Section~\ref{sec:challenges} describes current challenges of task graph execution on
HPC systems and objectives of the proposed thesis. Section~\ref{sec:state_of_the_art} mentions
existing task runtime systems and related works. Section~\ref{sec:current_progress} enumerates
what has been achieved so far. Section~\ref{sec:future-work} summarizes the thesis proposal and
outlines future work.
