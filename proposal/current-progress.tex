This section describes two main bodies of work that have been conducted so far. The first is a
set of publications~\cite{estee,rsds,spin2,spin} related to the first objective of the thesis,
identifying HPC task graph challenges. The second is the work on designing and implementing
\emph{HyperQueue}, a task graph execution tool designed for HPC use-cases.

\subsection{Task scheduler analysis and benchmarking}
The scheduler component, which assigns tasks to individual workers, is one of the
most crucial parts of a task runtime, because scheduling decisions can severely affect the
duration required to execute the whole task graph. Since task scheduling is NP-hard, various
heuristic algorithms are used in practice. These algorithms can suffer from non-obvious edge cases
that produce bad quality schedules and also from low runtime efficiency, which can erase any
speedup gained from producing a higher quality schedule.

To better understand the behaviour and performance of various scheduling algorithms, we have
performed an extensive analysis of several task scheduling algorithms in
\emph{Analysis of workflow schedulers in simulated distributed environments}~\cite{estee}.
We have benchmarked several task schedulers under various conditions, including parameters that
have not been explored so far, like the minimum delay between invoking the
scheduler or the amount of knowledge about task durations available for the scheduler.

Our analysis has shown that despite its simplicity, the foundational HLFET
algorithm~\cite{hlfet1974} produces high quality schedules in various scenarios and should thus
serve as a good baseline scheduler for task runtimes. We have also found out that even a
completely random scheduler can be competitive with other scheduling approaches for certain task
graphs and cluster configurations.

%During our attempts to implement various scheduling algorithms, we have also realized that the
%descriptions of many task scheduling algorithms in existing literature is incomplete. More
%specifically, seemingly inconsequential implementation details that are often missing from the
%algorithm's description can have a very large effect on the final performance of the scheduler,
%which makes it difficult to precisely reproduce and compare the performance of the existing
%algorithms.

One of the contributions of this work was \estee{}, a simulation framework for task
schedulers that is available as an open-source software\footnoteurl{https://github.com/it4innovations/estee}.
It can be used to define a cluster of workers, connect them together using a configurable network
model, implement a custom scheduling algorithm and test its performance on arbitrary task graphs.
\estee{} also contains implementations of several task scheduler baselines from existing literature
and a task graph generator that can be used to generate randomized graphs with similar properties as
real-world task graphs. This tool serves as an experimentation testbed for task runtime and scheduler
developers.

I have collaborated on this work with Stanislav Böhm and Vojtěch Cima, we have all contributed
equally to this work.

\subsection{Task runtime bottleneck analysis and optimization}
The scheduler is not the only part of a task runtime that can cause bottlenecks in task graph
execution. We have analysed an existing and quite popular task runtime \dask{}~\cite{dask} in
\emph{Runtime vs Scheduler: Analyzing Dask's Overheads}~\cite{rsds}, both to find out what
bottlenecks does it have, and also to benchmark various scheduling algorithms with \dask{},
to test them ``in the wild'' and thus better validate our results from~\cite{estee}.

Our analysis has demonstrated that \dask{} was bottlenecked not so much by its scheduler, but
by the runtime (in)efficiency of its central server. The inefficiencies were caused partly by
the design of its communication protocol, but mainly by the fact that \dask{} is implemented in
Python, which makes it difficult to fully utilize the available hardware potential. We have
also found out that it was impractical to swap \dask{}'s scheduler implementation for another one,
since its built-in work-stealing scheduling algorithm was quite firmly integrated into its
components.

In order to measure how could \dask{}'s performance be improved if it had a more efficient runtime,
as a second main contribution of this work we have developed \rsds{}, an open-source drop-in
replacement for the \dask{} central server\footnoteurl{https://github.com/it4innovations/rsds}.
It was built from the ground up with a focus on runtime efficiency and scheduler modularity, but at
the same time we have designed it to be compatible with the \dask{} protocol, so it could be used
by existing \dask{} users to speed up their task workflows.

We have performed a series of experiments where we have compared the performance of \rsds{} vs
\dask{}. Since \rsds{} allows its users to plug in a different scheduling algorithm easily, we
have also compared the performance of \rsds{} with various scheduling algorithms. The experiments
were conducted on task graphs generated from tracing real-world \dask{} task workflows, to make
sure that we were benchmarking realistic use-cases.

The results of our experiments indicate that optimizing the runtime is definitely a worthy effort
to pursuit, as \rsds{} has been able to outperform \dask{} in various scenarios, even though it
used a much simpler work-stealing scheduling algorithm. We have also been able to validate our
results from~\cite{estee}, for example that even a random scheduler is indeed competitive with
other scheduling approaches in many scenarios.

We have contacted the authors and maintainers of \dask{} and discussed our \rsds{} approach with
them\footnoteurl{https://github.com/dask/distributed/issues/3139}\footnoteurl{https://github
.com/dask/distributed/issues/3783}\footnoteurl{https://github.com/dask/distributed/issues/3872}.
Some of its ideas have been adapted in the \dask{} project and led to improving its performance.

An interesting insight regarding task schedulers that we have gained from our work done
in~\cite{estee,rsds} is just how much important are the specific details of scheduling algorithm
implementations. While trying to implement various scheduling algorithms from existing
literature, we have realized that they are often incomplete. Details like how
often should the algorithm be invoked or how to choose between workers which receive an equal
scheduling priority from the algorithm are often left up to the implementor. However, our
experiments have shown us that these seemingly minor details can have a significant effect on
the performance of the scheduler, both in terms of runtime efficiency and the quality of its
generated schedules.

I have collaborated on this work with Stanislav Böhm, we have both contributed equally to this
work.

\subsection{In-network computing}
In-network computing is a relatively recent area of research that attempts to explore the
possibility of offloading certain computing operations directly to network controllers, in order
to massively reduce latency and improve bandwidth of distributed (HPC) applications. The classic
approach of performing operations on data sent over the network is to pass it from the network
interface through all levels of the CPU memory hierarchy to the processor, which performs the
operation, and then send the data all the way back to the network interface and further into the
network. Even though a lot of fundamental networked operations (like collective reductions or
key-value operations) are rather simple to compute, they can suffer from high latency because of
the data movement needed to get the data to the processing element (CPU).

The idea of in-network computing is to move some data processing operations directly onto a
network controller (which can be a NIC connected to a computer, but also e.g.\ a smart network
switch), in order to remove the latency caused by the CPU memory hierarchy. This
approach is similar e.g.\ to offloading expensive computation from the CPU to graphics
accelerators (GPUs), even though in the case of network controller offloading, the latency is
usually more important than bandwidth (for GPUs it is typically the other way around).

We have explored how offloading computation to a network accelerator could improve the
performance of non-contiguous memory transfers in MPI applications~\cite{spin}. This
research was then expanded and generalized to a more general in-network computing framework
in~\cite{spin2}. Even though this research area is still fairly unexplored, the results so far
look quite promising.

Task runtimes need to exchange many kinds of data amongst workers and the scheduler over
the network. Some of these data exchanges could be offloaded to network controllers using either
more traditional RDMA (Remote direct memory access) or more general in-network computing
methods, in order to reduce the overhead of the task runtime. This could be a good fit for HPC
clusters, since network controllers capable of offloading computations are already making their
way into this area. One example of such technology is NVIDIA SHARP (Scalable Hierarchical
Aggregation and Reduction Protocol), which allows offloading collective operations of MPI
programs to networking devices.

I have collaborated on this work with multiple researchers from ETH Zurich. My main
contribution was the design, implementation and benchmarking of a virtualized in-network compute
engine that was used for implementing non-contiguous data transfers.

\subsection{HyperQueue}
\label{subsec:hyperqueue}
\hyperqueue{} is an HPC-tailored task runtime designed for executing task graphs in HPC
environments. Its two primary objectives are to be as performant as possible and to be easy to use
and deploy. It is developed in the Rust programming language and available as an open-source
software\footnoteurl{https://github.com/it4innovations/hyperqueue}.

The key idea of \hyperqueue{} is to disentangle the submission of computation and the provision of
computational resources. With traditional HPC job managers, the computation description is
closely tied to the request of computational resources, which leads to problems mentioned in
Section~\ref{sec:challenges}, such as less efficient load balancing or the need to manually
aggregate tasks into jobs. \hyperqueue{} separates these two actions; users submit task graphs
independently of providing computational resources (workers) and let the task runtime take care of
matching them together, based on requested resource requirements and other constraints.

One of the driving use-cases for \hyperqueue{} is efficient node usage and load balancing. The
latest HPC clusters contain a large number (hundreds) of cores, yet it is quite challenging to
design a single program that can scale effectively with so many threads. Thus, in order to fully
utilize the whole computational node, multiple tasks that each leverage a smaller amount of
threads have to be executed on the same node at once. \hyperqueue{} is able to effectively schedule
tasks to utilize all available computational nodes, and thanks to its design, it is able to do this
not just within a single HPC job, but across many jobs at once.

\hyperqueue{} is being used by users of various HPC centres, and it is also a key
component of the Horizon 2020 European Union projects
LIGATE\footnoteurl{https://www.ligateproject.eu},
EVEREST\footnoteurl{https://everest-h2020.eu} and
ACROSS\footnoteurl{https://across-h2020.eu}. It is also envisioned as one of the primary ways of
executing computations on the LUMI supercomputer~\cite{lumi_it4innovations_2022}.

\hyperqueue{} has been in development for over a year. It should be noted that while I am one of
its two primary authors and contributors, \hyperqueue{} is a team effort, and it is being
developed by multiple people.
