% General
@inproceedings{wrf,
    title = {The {Weather} {Reseach} and {Forecast} {Model}: {Software} {Architecture} and {Performance}},
    author = {Michalakes, John and Dudhia, Jimy and Gill, D. and Henderson, Tom and Klemp, J. and Skamarock, W. and Wang, Wei},
    month = jan,
    year = {2004},
}
@inproceedings{cfd,
    title={CFD Vision 2030 Study: A Path to Revolutionary Computational Aerosciences},
    author={Jeffrey P. Slotnick and Abdollah Khodadoust and Juan J. Alonso and David L. Darmofal and William Gropp and Elizabeth A. Lurie and Dimitri J. Mavriplis},
    year={2014}
}
@article{hpcdl,
    author = {Ben-Nun, Tal and Hoefler, Torsten},
    title = {Demystifying Parallel and Distributed Deep Learning: An In-Depth Concurrency Analysis},
    year = {2019},
    issue_date = {July 2020},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {52},
    number = {4},
    issn = {0360-0300},
%    url = {https://doi.org/10.1145/3320060},
    doi = {10.1145/3320060},
%    abstract = {Deep Neural Networks (DNNs) are becoming an important tool in modern computing applications. Accelerating their training is a major challenge and techniques range from distributed algorithms to low-level circuit design. In this survey, we describe the problem from a theoretical perspective, followed by approaches for its parallelization. We present trends in DNN architectures and the resulting implications on parallelization strategies. We then review and model the different types of concurrency in DNNs: from the single operator, through parallelism in network inference and training, to distributed deep learning. We discuss asynchronous stochastic optimization, distributed system architectures, communication schemes, and neural architecture search. Based on those approaches, we extrapolate potential directions for parallelism in deep learning.},
    journal = {ACM Comput. Surv.},
    month = aug,
    articleno = {65},
    numpages = {43},
%    keywords = {parallel algorithms, Deep learning, distributed computing}
}
@InProceedings{bioinformatics,
    author="Per{\'e}z-S{\'a}nchez, Horacio
and Fassihi, Afshin
and Cecilia, Jos{\'e} M.
and Ali, Hesham H.
and Cannataro, Mario",
%    editor="Ortu{\~{n}}o, Francisco
%and Rojas, Ignacio",
    title="Applications of High Performance Computing in Bioinformatics, Computational Biology and Computational Chemistry",
    booktitle="Bioinformatics and Biomedical Engineering",
    year="2015",
    publisher="Springer International Publishing",
    address="Cham",
    pages="527--541",
%    abstract="In the last 10 years, we are witnessing one of the major revolutions in parallel systems. The consolidation of heterogeneous systems at different levels -from desktop computers to large-scale systems such as supercomputers, clusters or grids, through all kinds of low-power devices- is providing a computational power unimaginable just few years ago, trying to follow the wake of Moore's law. This landscape in the high performance computing arena opens up great opportunities in the simulation of relevant biological systems and for applications in Bioinformatics, Computational Biology and Computational Chemistry. This introductory article shows the last tendencies of this active research field and our perspectives for the forthcoming years.",
    isbn="978-3-319-16480-9"
}
@Inbook{memorywall,
    author="McKee, Sally A.
and Wisniewski, Robert W.",
%    editor="Padua, David",
    title="Memory Wall",
    bookTitle="Encyclopedia of Parallel Computing",
    year="2011",
    publisher="Springer US",
    address="Boston, MA",
    pages="1110--1116",
    isbn="978-0-387-09766-4",
    doi="10.1007/978-0-387-09766-4_234",
%    url="https://doi.org/10.1007/978-0-387-09766-4_234"
}
@Inbook{powerwall,
    author="Bose, Pradip",
%    editor="Padua, David",
    title="Power Wall",
    bookTitle="Encyclopedia of Parallel Computing",
    year="2011",
    publisher="Springer US",
    address="Boston, MA",
    pages="1593--1608",
    isbn="978-0-387-09766-4",
    doi="10.1007/978-0-387-09766-4_499",
%    url="https://doi.org/10.1007/978-0-387-09766-4_499"
}
@article{xeonphi,
    title={Knights Landing: Second-Generation Intel Xeon Phi Product},
    author={Avinash Sodani and Roger Gramunt and Jes{\'u}s Corbal and Ho-Seop Kim and Krishna Vinod and Sundaram Chinthamani and Steven Hutsell and Rajat Agarwal and Yen-Chen Liu},
    journal={IEEE Micro},
    year={2016},
    volume={36},
    pages={34-46}
}
@inproceedings{mpistudy,
    author = {Laguna, Ignacio and Marshall, Ryan and Mohror, Kathryn and Ruefenacht, Martin and Skjellum, Anthony and Sultana, Nawrin},
    title = {A Large-Scale Study of MPI Usage in Open-Source HPC Applications},
    year = {2019},
    isbn = {9781450362290},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    doi = {10.1145/3295500.3356176},
    booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
    articleno = {31},
    numpages = {14},
    location = {Denver, Colorado},
    series = {SC '19}
}
@ARTICLE{mooreslaw,
    author={Schaller, R.R.},
    journal={IEEE Spectrum},
    title={Moore's law: past, present and future},
    year={1997},
    volume={34},
    number={6},
    pages={52-59},
    doi={10.1109/6.591665}
}
@online{top500gpu,
    author={Feldman, Michael},
    url={https://www.top500.org/news/new-gpu-accelerated-supercomputers-change-the-balance-of-power-on-the-top500/},
    urldate = {2023-04-21},
}
@inproceedings{top500analysis,
    author = {Khan, Awais and Sim, Hyogi and Vazhkudai, Sudharshan S. and Butt, Ali R. and Kim, Youngjae},
    title = {An Analysis of System Balance and Architectural Trends Based on Top500 Supercomputers},
    year = {2021},
    isbn = {9781450388429},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
%    url = {https://doi.org/10.1145/3432261.3432263},
    doi = {10.1145/3432261.3432263},
%    abstract = {Supercomputer design is a complex, multi-dimensional optimization process, wherein several subsystems need to be reconciled to meet a desired figure of merit performance for a portfolio of applications and a budget constraint. However, overall, the HPC community has been gravitating towards ever more Flops, at the expense of many other subsystems. To draw attention to overall system balance, in this paper, we analyze balance ratios and architectural trends in the world’s most powerful supercomputers. Specifically, we have collected the performance characteristics of systems between 1993 and 2019 based on the Top500 lists and then analyzed their architectures from diverse system design perspectives. Notably, our analysis studies the performance balance of the machines, across a variety of subsystems such as compute, memory, I/O, interconnect, intra-node connectivity and power. Our analysis reveals that balance ratios of the various subsystems need to be considered carefully alongside the application workload portfolio to provision the subsystem capacity and bandwidth specifications, which can help achieve optimal performance.},
    booktitle = {The International Conference on High Performance Computing in Asia-Pacific Region},
    pages = {11–22},
    numpages = {12},
    %    keywords = {Architectural Trends and Performance Balance Ratio, High Performance Computing, Top500 Supercomputers},
    location = {Virtual Event, Republic of Korea},
    series = {HPC Asia 2021}
}
@online{it4i_node_scheduling_policy,
    title = {IT4Innovations Job Submission Policy},
    url = {https://docs.it4i.cz/general/resource_allocation_and_job_execution/\#job-submission-and-execution},
    urldate = {2023-04-28},
}
@article{graph_partitioning,
    author="Feldmann, Andreas Emil and Foschini, Luca",
    title="Balanced Partitions of Trees and Applications",
    journal="Algorithmica",
    year="2015",
    month="Feb",
    day="01",
    volume="71",
    number="2",
    pages="354--376",
%    abstract="We study the problem of finding the minimum number of edges that, when cut, form a partition of the vertices into k sets of equal size. This is called the k-BALANCED PARTITIONING problem. The problem is known to be inapproximable within any finite factor on general graphs, while little is known about restricted graph classes.",
    issn="1432-0541",
    doi="10.1007/s00453-013-9802-3",
%    url="https://doi.org/10.1007/s00453-013-9802-3"
}
@article{mapreduce,
    author = {Dean, Jeffrey and Ghemawat, Sanjay},
    title = {MapReduce: Simplified Data Processing on Large Clusters},
    year = {2008},
    issue_date = {January 2008},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {51},
    number = {1},
    issn = {0001-0782},
%    url = {https://doi.org/10.1145/1327452.1327492},
    doi = {10.1145/1327452.1327492},
%    abstract = {MapReduce is a programming model and an associated implementation for processing and generating large datasets that is amenable to a broad variety of real-world tasks. Users specify the computation in terms of a map and a reduce function, and the underlying runtime system automatically parallelizes the computation across large-scale clusters of machines, handles machine failures, and schedules inter-machine communication to make efficient use of the network and disks. Programmers find the system easy to use: more than ten thousand distinct MapReduce programs have been implemented internally at Google over the past four years, and an average of one hundred thousand MapReduce jobs are executed on Google's clusters every day, processing a total of more than twenty petabytes of data per day.},
    journal = {Commun. ACM},
    month = {jan},
    pages = {107–113},
    numpages = {7}
}
@inproceedings{workflows_at_scale,
    author = {Bhatia, Harsh and Di Natale, Francesco and Moon, Joseph Y. and Zhang, Xiaohua and Chavez, Joseph R. and Aydin, Fikret and Stanley, Chris and Oppelstrup, Tomas and Neale, Chris and Schumacher, Sara Kokkila and Ahn, Dong H. and Herbein, Stephen and Carpenter, Timothy S. and Gnanakaran, Sandrasegaram and Bremer, Peer-Timo and Glosli, James N. and Lightstone, Felice C. and Ing\'{o}lfsson, Helgi I.},
    title = {Generalizable Coordination of Large Multiscale Workflows: Challenges and Learnings at Scale},
    year = {2021},
    isbn = {9781450384421},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
%    url = {https://doi.org/10.1145/3458817.3476210},
    doi = {10.1145/3458817.3476210},
%    abstract = {The advancement of machine learning techniques and the heterogeneous architectures of most current supercomputers are propelling the demand for large multiscale simulations that can automatically and autonomously couple diverse components and map them to relevant resources to solve complex problems at multiple scales. Nevertheless, despite the recent progress in workflow technologies, current capabilities are limited to coupling two scales. In the first-ever demonstration of using three scales of resolution, we present a scalable and generalizable framework that couples pairs of models using machine learning and in situ feedback. We expand upon the massively parallel Multiscale Machine-Learned Modeling Infrastructure (MuMMI), a recent, award-winning workflow, and generalize the framework beyond its original design. We discuss the challenges and learnings in executing a massive multiscale simulation campaign that utilized over 600,000 node hours on Summit and achieved more than 98\% GPU occupancy for more than 83\% of the time. We present innovations to enable several orders of magnitude scaling, including simultaneously coordinating 24,000 jobs, and managing several TBs of new data per day and over a billion files in total. Finally, we describe the generalizability of our framework and, with an upcoming open-source release, discuss how the presented framework may be used for new applications.},
    booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
    articleno = {10},
    numpages = {16},
%    keywords = {heterogenous architecture, cancer research, adaptive simulations, multiscale simulations, massively parallel, machine learning},
    location = {St. Louis, Missouri},
    series = {SC '21}
}
@article{dragonfly,
    title={Technology-driven, highly-scalable dragonfly topology},
    author={Kim, John and Dally, Wiliam J and Scott, Steve and Abts, Dennis},
    journal={ACM SIGARCH Computer Architecture News},
    volume={36},
    number={3},
    pages={77--88},
    year={2008},
    publisher={ACM New York, NY, USA}
}
@INPROCEEDINGS{slimfly,
    author={Besta, Maciej and Hoefler, Torsten},
    booktitle={SC '14: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
    title={Slim Fly: A Cost Effective Low-Diameter Network Topology},
    year={2014},
    volume={},
    number={},
    pages={348-359},
    doi={10.1109/SC.2014.34}
}
@Article{task_based_taxonomy,
    author="Thoman, Peter
and Dichev, Kiril
and Heller, Thomas
and Iakymchuk, Roman
and Aguilar, Xavier
and Hasanov, Khalid
and Gschwandtner, Philipp
and Lemarinier, Pierre
and Markidis, Stefano
and Jordan, Herbert
and Fahringer, Thomas
and Katrinis, Kostas
and Laure, Erwin
and Nikolopoulos, Dimitrios S.",
    title="A taxonomy of task-based parallel programming technologies for high-performance computing",
    journal="The Journal of Supercomputing",
    year="2018",
    month="Apr",
    day="01",
    volume="74",
    number="4",
    pages="1422--1434",
%    abstract="Task-based programming models for shared memory---such as Cilk Plus and OpenMP 3---are well established and documented. However, with the increase in parallel, many-core, and heterogeneous systems, a number of research-driven projects have developed more diversified task-based support, employing various programming and runtime features. Unfortunately, despite the fact that dozens of different task-based systems exist today and are actively used for parallel and high-performance computing (HPC), no comprehensive overview or classification of task-based technologies for HPC exists. In this paper, we provide an initial task-focused taxonomy for HPC technologies, which covers both programming interfaces and runtime mechanisms. We demonstrate the usefulness of our taxonomy by classifying state-of-the-art task-based environments in use today.",
    issn="1573-0484",
    doi="10.1007/s11227-018-2238-4",
%    url="https://doi.org/10.1007/s11227-018-2238-4"
}
@article{cwl,
    author = {Crusoe, Michael and Abeln, Sanne and Iosup, Alexandru and Amstutz, Peter and Chilton, John and Tijanić, Nebojša and Ménager, Hervé and Soiland-Reyes, Stian and Gavrilović, Bogdan and Goble, Carole and Community, The},
    year = {2021},
    month = {08},
    pages = {},
    title = {Methods Included: Standardizing Computational Reuse and Portability with the Common Workflow Language},
    volume = {65},
    journal = {Communications of the ACM},
    doi = {10.1145/3486897}
}
@Article{large_scale_modelling,
    author="Lampa, Samuel
and Alvarsson, Jonathan
and Spjuth, Ola",
    title="Towards agile large-scale predictive modelling in drug discovery with flow-based programming design principles",
    journal="Journal of Cheminformatics",
    year="2016",
    month="Nov",
    day="24",
    volume="8",
    number="1",
    pages="67",
%    abstract="Predictive modelling in drug discovery is challenging to automate as it often contains multiple analysis steps and might involve cross-validation and parameter tuning that create complex dependencies between tasks. With large-scale data or when using computationally demanding modelling methods, e-infrastructures such as high-performance or cloud computing are required, adding to the existing challenges of fault-tolerant automation. Workflow management systems can aid in many of these challenges, but the currently available systems are lacking in the functionality needed to enable agile and flexible predictive modelling. We here present an approach inspired by elements of the flow-based programming paradigm, implemented as an extension of the Luigi system which we name SciLuigi. We also discuss the experiences from using the approach when modelling a large set of biochemical interactions using a shared computer cluster.Graphical abstract.",
    issn="1758-2946",
    doi="10.1186/s13321-016-0179-6",
%    url="https://doi.org/10.1186/s13321-016-0179-6"
}
@book{bertsekas_1992,
    address = {Upper Saddle River, NJ, USA},
    title = {Data {Networks} (2Nd {Ed}.)},
    isbn = {978-0-13-200916-4},
    publisher = {Prentice-Hall, Inc.},
    author = {Bertsekas, Dimitri and Gallager, Robert},
    year = {1992},
    doi = {10.5555/121104}
}
@INPROCEEDINGS{pegasusgraphs,
    author={Silva, Rafael Ferreira da and Chen, Weiwei and Juve, Gideon and Vahi, Karan and
	Deelman, Ewa},
    booktitle={2014 IEEE 10th International Conference on e-Science},
    title={Community Resources for Enabling Research in Distributed Scientific Workflows},
    year={2014},
    volume={1},
    number={},
    pages={177-184},
    doi={10.1109/eScience.2014.44}
}
@misc{it4i,
    url = {https://docs.it4i.cz/},
    urldate = {2024-01-19},
    title = {IT4Innovations supercomputing centre}
}
@article{taskbench,
    title={Task {B}ench: {A} Parameterized Benchmark for Evaluating Parallel
    Runtime
    Performance},
    author={Slaughter, Elliott and Wu, Wei and Fu, Yuankun and Brandenburg,
    Legend and Garcia, Nicolai and Kautz, Wilhem and Marx, Emily and Morris,
    Kaleb S and Lee, Wonchan and Cao, Qinglei and others},
    journal={arXiv preprint arXiv:1908.05790},
    year={2019}
}
@techreport{pep703,
    author  = {Sam Gross},
    title   = {Making the Global Interpreter Lock Optional in CPython},
    year    = {2023},
    type    = {PEP},
    number  = {703},
    url     = {https://peps.python.org/pep-0703/},
}
@online{wordbatcharticle,
    title = {Benchmarking Python Distributed {AI} Backends with Wordbatch},
    url = {https://towardsdatascience.com/benchmarking-python-distributed-ai-backends-with-wordbatch-9872457b785c},
    urldate = {2020-04-20}
}
@article{spmd,
    author = {F. Darema and D.A. George and V.A. Norton and G.F. Pfister},
    title = {A single-program-multiple-data computational model for EPEX/FORTRAN},
    journal = {Parallel Computing},
    volume = {7},
    number = {1},
    pages = {11-24},
    year = {1988},
    issn = {0167-8191},
    doi = {https://doi.org/10.1016/0167-8191(88)90094-4},
%    url = {https://www.sciencedirect.com/science/article/pii/0167819188900944},
%    keywords = {Shared memory multiprocessor, EPEX/FORTRAN, computational model, parallelization features},
%    abstract = {We present a single-program-multiple-data computational model which we have implemented in the EPEX system to run in parallel mode FORTRAN scientific application programs. The computational model assumes a shared memory organization and is based on the scheme that all processes executing a program in parallel remain in existence for the entire execution; however, the tasks to be executed by each process are determined dynamically during execution by the use of appropriate synchronizing constructs that are imbedded in the program. We have demonstrated the applicability of the model in the parallelization of several applications. We discuss parallelization features of these applications and performance issues such as overhead, speedup, efficiency.}
}
@INPROCEEDINGS{hybrid_openmp_mpi,
    author={Rabenseifner, Rolf and Hager, Georg and Jost, Gabriele},
    booktitle={2009 17th Euromicro International Conference on Parallel, Distributed and Network-based Processing},
    title={Hybrid MPI/OpenMP Parallel Programming on Clusters of Multi-Core SMP Nodes},
    year={2009},
    volume={},
    number={},
    pages={427-436},
%    keywords={Parallel programming;Yarn;High performance computing;Concurrent computing;Distributed computing;Hardware;Computer networks;Message passing;Topology;Taxonomy;Hybrid programming;MPI;OpenMP;multi-core;SMP},
    doi={10.1109/PDP.2009.43}
}
@report{memory_safety_report,
    author = {National Cybersecurity Strategy},
    title = {Back to the Building Blocks: a Path Towards Secure and Measurable Software},
    institution = {National Cybersecurity Strategy},
    url = {https://www.whitehouse.gov/wp-content/uploads/2024/02/Final-ONCD-Technical-Report.pdf},
    urldate = {2024-03-24},
    year = {2024},
}
@article{fault_tolerant_mpi,
    title={Fault tolerance in message passing interface programs},
    author={Gropp, William and Lusk, Ewing},
    journal={The International Journal of High Performance Computing Applications},
    volume={18},
    number={3},
    pages={363--372},
    year={2004},
    publisher={Sage Publications Sage CA: Thousand Oaks, CA}
}
@article{horovod,
    title={Horovod: fast and easy distributed deep learning in TensorFlow},
    author={Alexander Sergeev and Mike Del Balso},
    journal={ArXiv},
    year={2018},
    volume={abs/1802.05799},
    url={https://api.semanticscholar.org/CorpusID:3398835}
}
@Book{cpp11_standard,
    author =       "{ISO}",
    title =        "{ISO\slash IEC 14882:2011 Information technology ---
                 Programming languages --- C++}",
    publisher =    pub-ISO,
    address =      pub-ISO:adr,
    edition =      "Third",
%    pages =        "????",
    day =          "1",
    month =        sep,
    year =         "2011",
%    ISBN =         "????",
%    ISBN-13 =      "????",
%    LCCN =         "????",
%    bibdate =      "Mon Dec 19 11:12:12 2011",
%    bibsource =    "https://www.math.utah.edu/pub/tex/bib/isostd.bib;
%                 https://www.math.utah.edu/pub/tex/bib/mathcw.bib",
    URL =          "http://www.iso.org/iso/iso_catalogue/catalogue_tc/catalogue_detail.htm?csnumber=50372",
%    acknowledgement = ack-nhfb,
%    remark =       "Revises ISO/IEC 14882:2003.",
}
@manual{intel_developer_manual,
    added-at = {2008-09-15T08:33:44.000+0200},
%    author = {?},
%    biburl = {https://www.bibsonomy.org/bibtex/2d1fd8c578b32c046dc1e7de8410665b5/rekstorm},
%    interhash = {eee5c9ed0fff42f3a37bc7bd3375dc39},
%    intrahash = {d1fd8c578b32c046dc1e7de8410665b5},
    keywords = {Architectures Manual},
    month = {August},
    organization = {Intel Corporation},
    timestamp = {2008-09-15T08:52:53.000+0200},
    title = {Intel 64 and IA-32 Architectures Software Developer's Manual - Volume 3B},
    year = 2007
}
@software{hadoop,
    author = {{Apache Software Foundation}},
    title = {Hadoop},
    url = {https://hadoop.apache.org},
    version = {0.20.2},
    date = {2010-02-19},
}
@article{spark,
    author = {Zaharia, Matei and Xin, Reynold S. and Wendell, Patrick and Das, Tathagata and Armbrust, Michael and Dave, Ankur and Meng, Xiangrui and Rosen, Josh and Venkataraman, Shivaram and Franklin, Michael J. and Ghodsi, Ali and Gonzalez, Joseph and Shenker, Scott and Stoica, Ion},
    title = {Apache Spark: a unified engine for big data processing},
    year = {2016},
    issue_date = {November 2016},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {59},
    number = {11},
    issn = {0001-0782},
%    url = {https://doi.org/10.1145/2934664},
    doi = {10.1145/2934664},
%    abstract = {This open source computing framework unifies streaming, batch, and interactive big data workloads to unlock new applications.},
    journal = {Commun. ACM},
    month = {oct},
    pages = {56–65},
    numpages = {10}
}
@techreport{mpi,
    author = {Forum, Message P},
    title = {MPI: A Message-Passing Interface Standard},
    year = {1994},
    publisher = {University of Tennessee},
    address = {USA},
%    abstract = {The Message Passing Interface Forum (MPIF), with participation from over 40 organizations, has been meeting since November 1992 to discuss and define a set of library standards for message passing. MPIF is not sanctioned or supported by any official standards organization. The goal of the Message Passing Interface, simply stated, is to develop a widely used standard for writing message-passing programs. As such the interface should establish a practical, portable, efficient and flexible standard for message passing. , This is the final report, Version 1.0, of the Message Passing Interface Forum. This document contains all the technical features proposed for the interface. This copy of the draft was processed by LATEX on April 21, 1994. , Please send comments on MPI to mpi-comments@cs.utk.edu. Your comment will be forwarded to MPIF committee members who will attempt to respond.}
}
@Inbook{pgas,
    author="Almasi, George",
%    editor="Padua, David",
    title="PGAS (Partitioned Global Address Space) Languages",
    bookTitle="Encyclopedia of Parallel Computing",
    year="2011",
    publisher="Springer US",
    address="Boston, MA",
    pages="1539--1545",
    isbn="978-0-387-09766-4",
    doi="10.1007/978-0-387-09766-4_210",
%    url="https://doi.org/10.1007/978-0-387-09766-4_210"
}
@book{openmp,
    title={Parallel programming in OpenMP},
    author={Chandra, Rohit and Dagum, Leo and Kohr, David and Menon, Ramesh and Maydan, Dror and McDonald, Jeff},
    year={2001},
    publisher={Morgan kaufmann}
}
@article{cuda,
    author = {Nickolls, John and Buck, Ian and Garland, Michael and Skadron, Kevin},
    title = {Scalable Parallel Programming with CUDA.},
    journal = {ACM Queue},
    doi = {10.1145/1365490.1365500},
%    keywords = {dblp},
    number = 2,
    pages = {40-53},
    timestamp = {2018-11-07T11:49:23.000+0100},
%    url = {http://dblp.uni-trier.de/db/journals/queue/queue6.html#NickollsBGS08},
    volume = 6,
    year = 2008
}
@misc{tensorflow,
    title={ {TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},
    url={https://www.tensorflow.org/},
    note={Software available from tensorflow.org},
    author={
    Mart\'{i}n~Abadi and
    Ashish~Agarwal and
    Paul~Barham and
    Eugene~Brevdo and
    Zhifeng~Chen and
    Craig~Citro and
    Greg~S.~Corrado and
    Andy~Davis and
    Jeffrey~Dean and
    Matthieu~Devin and
    Sanjay~Ghemawat and
    Ian~Goodfellow and
    Andrew~Harp and
    Geoffrey~Irving and
    Michael~Isard and
    Yangqing Jia and
    Rafal~Jozefowicz and
    Lukasz~Kaiser and
    Manjunath~Kudlur and
    Josh~Levenberg and
    Dandelion~Man\'{e} and
    Rajat~Monga and
    Sherry~Moore and
    Derek~Murray and
    Chris~Olah and
    Mike~Schuster and
    Jonathon~Shlens and
    Benoit~Steiner and
    Ilya~Sutskever and
    Kunal~Talwar and
    Paul~Tucker and
    Vincent~Vanhoucke and
    Vijay~Vasudevan and
    Fernanda~Vi\'{e}gas and
    Oriol~Vinyals and
    Pete~Warden and
    Martin~Wattenberg and
    Martin~Wicke and
    Yuan~Yu and
    Xiaoqiang~Zheng},
    year={2015},
}
@inproceedings{cost,
    author = {McSherry, Frank and Isard, Michael and Murray, Derek G.},
    title = {Scalability! but at what cost?},
    year = {2015},
    publisher = {USENIX Association},
    address = {USA},
%    abstract = {We offer a new metric for big data platforms, COST, or the Configuration that Outperforms a Single Thread. The COST of a given platform for a given problem is the hardware configuration required before the platform outperforms a competent single-threaded implementation. COST weighs a system's scalability against the overheads introduced by the system, and indicates the actual performance gains of the system, without rewarding systems that bring substantial but parallelizable overheads.We survey measurements of data-parallel systems recently reported in SOSP and OSDI, and find that many systems have either a surprisingly large COST, often hundreds of cores, or simply underperform one thread for all of their reported configurations.},
    booktitle = {Proceedings of the 15th USENIX Conference on Hot Topics in Operating Systems},
    pages = {14},
    numpages = {1},
    location = {Switzerland},
    series = {HOTOS'15}
}
@INPROCEEDINGS{mpiusagestudy1,
    author={Chunduri, Sudheer and Parker, Scott and Balaji, Pavan and Harms, Kevin and Kumaran, Kalyan},
    booktitle={SC18: International Conference for High Performance Computing, Networking, Storage and Analysis},
    title={Characterization of MPI Usage on a Production Supercomputer},
    year={2018},
    volume={},
    number={},
    pages={386-400},
%    keywords={Tools;Hardware;Supercomputers;Libraries;Control systems;Production systems;MPI;monitoring;Autoperf;core-hours},
    doi={10.1109/SC.2018.00033}
}
@article{mpiusagestudy2,
    author = {Bernholdt, David E. and Boehm, Swen and Bosilca, George and Gorentla Venkata, Manjunath and Grant, Ryan E. and Naughton, Thomas and Pritchard, Howard P. and Schulz, Martin and Vallee, Geoffroy R.},
    title = {A survey of MPI usage in the US exascale computing project},
    journal = {Concurrency and Computation: Practice and Experience},
    volume = {32},
    number = {3},
    pages = {e4851},
    keywords = {exascale, MPI},
    doi = {https://doi.org/10.1002/cpe.4851},
%    url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cpe.4851},
%    eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/cpe.4851},
    note = {e4851 cpe.4851},
%    abstract = {Summary The Exascale Computing Project (ECP) is currently the primary effort in the United States focused on developing “exascale” levels of computing capabilities, including hardware, software, and applications. In order to obtain a more thorough understanding of how the software projects under the ECP are using, and planning to use the Message Passing Interface (MPI), and help guide the work of our own project within the ECP, we created a survey. Of the 97 ECP projects active at the time the survey was distributed, we received 77 responses, 56 of which reported that their projects were using MPI. This paper reports the results of that survey for the benefit of the broader community of MPI developers.},
    year = {2020}
}
@inproceedings{mpiusagestudy3,
    author = {Laguna, Ignacio and Marshall, Ryan and Mohror, Kathryn and Ruefenacht, Martin and Skjellum, Anthony and Sultana, Nawrin},
    title = {A large-scale study of MPI usage in open-source HPC applications},
    year = {2019},
    isbn = {9781450362290},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
%    url = {https://doi.org/10.1145/3295500.3356176},
    doi = {10.1145/3295500.3356176},
%    abstract = {Understanding the state-of-the-practice in MPI usage is paramount for many aspects of supercomputing, including optimizing the communication of HPC applications and informing standardization bodies and HPC systems procurements regarding the most important MPI features. Unfortunately, no previous study has characterized the use of MPI on applications at a significant scale; previous surveys focus either on small data samples or on MPI jobs of specific HPC centers. This paper presents the first comprehensive study of MPI usage in applications. We survey more than one hundred distinct MPI programs covering a significantly large space of the population of MPI applications. We focus on understanding the characteristics of MPI usage with respect to the most used features, code complexity, and programming models and languages. Our study corroborates certain findings previously reported on smaller data samples and presents a number of interesting, previously un-reported insights.},
    booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
    articleno = {31},
    numpages = {14},
%    keywords = {MPI, applications survey, program analysis},
    location = {Denver, Colorado},
    series = {SC '19}
}
@InProceedings{task_checkpointing,
    author="Verg{\'e}s, Pere
and Lordan, Francesc
and Ejarque, Jorge
and Badia, Rosa M.",
%    editor="Singer, Jeremy
%and Elkhatib, Yehia
%and Blanco Heras, Dora
%and Diehl, Patrick
%and Brown, Nick
%and Ilic, Aleksandar",
    title="Task-Level Checkpointing System for Task-Based Parallel Workflows",
    booktitle="Euro-Par 2022: Parallel Processing Workshops",
    year="2023",
    publisher="Springer Nature Switzerland",
    address="Cham",
    pages="251--262",
%    abstract="Scientific applications are large and complex; task-based programming models are a popular approach to developing these applications due to their ease of programming and ability to handle complex workflows and distribute their workload across large infrastructures. In these environments, either the hardware or the software may lead to failures from a myriad of origins: application logic, system software, memory, network, or disk. Re-executing a failed application can take hours, days, or even weeks, thus, dragging out the research. This article proposes a recovery system for dynamic task-based models to reduce the re-execution time of failed runs. The design encapsulates in a checkpointing manager the automatic checkpointing of the execution, leveraging different mechanisms that can be arbitrarily defined and tuned to fit the needs of each performance. Additionally, it offers an API call to establish snapshots of the execution from the application code. The experiments executed on a prototype implementation have reached a speedup of 1.9{\$}{\$}{\backslash}times {\$}{\$}{\texttimes}after re-execution and shown no overhead on the execution time on successful first runs of specific applications.",
    isbn="978-3-031-31209-0"
}
@article{dataflow,
    title = {The Dataflow Model: A Practical Approach to Balancing Correctness, Latency, and Cost in Massive-Scale, Unbounded, Out-of-Order Data Processing},
    author = {Tyler Akidau and Robert Bradshaw and Craig Chambers and Slava Chernyak and Rafael J. Fernández-Moctezuma and Reuven Lax and Sam McVeety and Daniel Mills and Frances Perry and Eric Schmidt and Sam Whittle},
    year = 2015,
    journal = {Proceedings of the VLDB Endowment},
    volume = 8,
    pages = {1792--1803}
}
@INPROCEEDINGS{stencil,
    author={Maruyama, Naoya and Sato, Kento and Nomura, Tatsuo and Matsuoka, Satoshi},
    booktitle={SC '11: Proceedings of 2011 International Conference for High Performance Computing, Networking, Storage and Analysis},
    title={Physis: An implicitly parallel programming model for stencil computations on large-scale GPU-accelerated supercomputers},
    year={2011},
    volume={},
    number={},
    pages={1-12},
%    keywords={Programming;Computational modeling;DSL;Graphics processing unit;Runtime;Optimization;Indexes;Domain Specific Languages;Application Framework;High Perforamnce Computing},
    doi={10.1145/2063384.2063398}
}
@Inbook{parallel_haskell,
    author="Hammond, Kevin",
    editor="Padua, David",
    title="Glasgow Parallel Haskell (GpH)",
    bookTitle="Encyclopedia of Parallel Computing",
    year="2011",
    publisher="Springer US",
    address="Boston, MA",
    pages="768--779",
    isbn="978-0-387-09766-4",
    doi="10.1007/978-0-387-09766-4_46",
%    url="https://doi.org/10.1007/978-0-387-09766-4_46"
}
@INPROCEEDINGS{workflows1,
    author={Do, Tu Mai Anh and Pottier, Loïc and Yildiz, Orcun and Vahi, Karan and Krawczuk, Patrycja and Peterka, Tom and Deelman, Ewa},
    booktitle={2022 22nd IEEE International Symposium on Cluster, Cloud and Internet Computing (CCGrid)},
    title={Accelerating Scientific Workflows on HPC Platforms with In Situ Processing},
    year={2022},
    volume={},
    number={},
    pages={1-10},
%    keywords={Cloud computing;Scientific computing;High performance computing;Genomics;Data transfer;Bioinformatics;Task analysis;Scientific Workflows;Workflow Management Systems;In situ;Pegasus;Decaf},
    doi={10.1109/CCGrid54584.2022.00009}
}
@INPROCEEDINGS{bulkparallel1,
    author={Cheatham, T. and Fahmy, A. and Stefanescu, D.C. and Valiant, L.G.},
    booktitle={Proceedings of the Twenty-Eighth Annual Hawaii International Conference on System Sciences},
    title={Bulk synchronous parallel computing-a paradigm for transportable software},
    year={1995},
    volume={2},
    number={},
    pages={268-275 vol.2},
%    keywords={Concurrent computing;Software performance;Computer architecture;Yarn;Throughput;Parallel processing;Computer industry;Contracts;Laboratories;Reactive power},
    doi={10.1109/HICSS.1995.375451}
}
@InProceedings{bulkparallel2,
    author="Gerbessiotis, Alexandras V.
and Valiant, Leslie G.",
    editor="Nurmi, Otto
and Ukkonen, Esko",
    title="Direct bulk-synchronous parallel algorithms",
    booktitle="Algorithm Theory --- SWAT '92",
    year="1992",
    publisher="Springer Berlin Heidelberg",
    address="Berlin, Heidelberg",
    pages="1--18",
%    abstract="We describe a methodology for constructing parallel algorithms that are transportable among parallel computers having different numbers of processors, different bandwidths of interprocessor communication and different periodicity of global synchronisation. We do this for the bulk-synchronous parallel (BSP) model, which abstracts the characteristics of a parallel machine into three numerical parameters p, g, and L, corresponding to processors, bandwidth, and periodicity respectively. The model differentiates memory that is local to a processor from that which is not, but, for the sake of universality, does not differentiate network proximity. The advantages of this model in supporting shared memory or PRAM style programming have been treated elsewhere. Here we emphasise the viability of an alternative direct style of programming where, for the sake of efficiency the programmer retains control of memory allocation. We show that optimality to within a multiplicative factor close to one can be achieved for the problems of Gauss-Jordan elimination and sorting, by transportable algorithms that can be applied for a wide range of values of the parameters p, g, and L. We also give some simulation results for PRAMs on the BSP to identify the level of slack at which corresponding efficiencies can be approached by shared memory simulations, provided the bandwidth parameter g is good enough.",
    isbn="978-3-540-47275-9"
}
@article{split_apply_combine,
    author = {Hadley, Wickham},
    year = {2011},
    month = {04},
    pages = {},
    title = {The Split-Apply-Combine Strategy for Data Analysis},
    volume = {40},
    journal = {Journal of Statistical Software},
    doi = {10.18637/jss.v040.i01}
}
@article{cilk,
    title = {Cilk: An Efficient Multithreaded Runtime System},
    journal = {Journal of Parallel and Distributed Computing},
    volume = {37},
    number = {1},
    pages = {55-69},
    year = {1996},
    issn = {0743-7315},
    doi = {https://doi.org/10.1006/jpdc.1996.0107},
%    url = {https://www.sciencedirect.com/science/article/pii/S0743731596901070},
    author = {Robert D. Blumofe and Christopher F. Joerg and Bradley C. Kuszmaul and Charles E. Leiserson and Keith H. Randall and Yuli Zhou},
%    abstract = {Cilk (pronounced “silk”) is a C-based runtime system for multithreaded parallel programming. In this paper, we document the efficiency of the Cilk work-stealing scheduler, both empirically and analytically. We show that on real and synthetic applications, the “work” and “critical-path length” of a Cilk computation can be used to model performance accurately. Consequently, a Cilk programmer can focus on reducing the computation's work and critical-path length, insulated from load balancing and other runtime scheduling issues. We also prove that for the class of “fully strict” (well-structured) programs, the Cilk scheduler achieves space, time, and communication bounds all within a constant factor of optimal. The Cilk runtime system currently runs on the Connection Machine CM5 MPP, the Intel Paragon MPP, the Sun Sparcstation SMP, and the Cilk-NOW network of workstations. Applications written in Cilk include protein folding, graphic rendering, backtrack search, and the ★Socrates chess program, which won second prize in the 1995 ICCA World Computer Chess Championship.}
}
@inproceedings{qr_factorization,
    author = {Miletto, Marcelo and Schnorr, Lucas},
    year = {2019},
    month = {11},
    pages = {25-36},
    title = {OpenMP and StarPU Abreast: the Impact of Runtime in Task-Based Block QR Factorization Performance},
    doi = {10.5753/wscad.2019.8654}
}
@article{tbb,
    author = {Pheatt, Chuck},
    title = {Intel® threading building blocks},
    year = {2008},
    issue_date = {April 2008},
    publisher = {Consortium for Computing Sciences in Colleges},
    address = {Evansville, IN, USA},
    volume = {23},
    number = {4},
    issn = {1937-4771},
%    abstract = {Intel® Threading Building Blocks [1] is a C++ runtime library that abstracts the low-level threading details necessary for effectively utilizing multi-core processors. It uses C++ templates to eliminate the need to create and manage threads. Applications tend to be more portable since parallelism is achieved through library calls and utilization of a task manager for scheduling. The task manager analyzes the system the software is running on, chooses the optimal number of threads, and performs load balancing that spreads out the work evenly across all processor cores. The library consists of data structures and algorithms that simplify parallel programming in C++ by avoiding requiring a programmer to use native threading packages such as POSIX threads or Windows threads, or even the portable Boost Threads.},
    journal = {J. Comput. Sci. Coll.},
    month = {apr},
    pages = {298},
    numpages = {1}
}

% Allocation managers
@InProceedings{slurm,
    author = "Yoo, Andy B.
and Jette, Morris A.
and Grondona, Mark",
    doi = "10.1007/10968987_3",
%    editor = "Feitelson, Dror
%and Rudolph, Larry
%and Schwiegelshohn, Uwe",
    title = "SLURM: Simple Linux Utility for Resource Management",
    booktitle = "Job Scheduling Strategies for Parallel Processing",
    year = "2003",
    publisher = "Springer Berlin Heidelberg",
    address = "Berlin, Heidelberg",
    pages = "44--60",
%    abstract = "A new cluster resource management system called Simple Linux Utility Resource Management (SLURM) is described in this paper. SLURM, initially developed for large Linux clusters at the Lawrence Livermore National Laboratory (LLNL), is a simple cluster manager that can scale to thousands of processors. SLURM is designed to be flexible and fault-tolerant and can be ported to other clusters of different size and architecture with minimal effort. We are certain that SLURM will benefit both users and system architects by providing them with a simple, robust, and highly scalable parallel job execution environment for their cluster system.",
    isbn = "978-3-540-39727-4"
}
@inproceedings{pbs,
    author = {Staples, Garrick},
    title = {TORQUE Resource Manager},
    year = {2006},
    isbn = {0769527000},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    %    url = {https://doi.org/10.1145/1188455.1188464},
    doi = {10.1145/1188455.1188464},
%    abstract = {With TORQUE Resource Manager now reaching over 10,000 downloads per month and use across thousands of leading sites representing commercial, government, and academic organizations, we invite all TORQUE users to meet and discuss TORQUE with the professional developers, community volunteers other members who use and have contributed to the TORQUE project.Here we will discuss the current state of TORQUE including some of the more recent enhancements and capabilities along with the road map for the upcoming year. We will also provide a time for TORQUE users to share experiences, best practices, and new needs.},
    booktitle = {Proceedings of the 2006 ACM/IEEE Conference on Supercomputing},
    pages = {8–es},
    location = {Tampa, Florida},
    series = {SC '06}
}
@online{slurm-schedmd,
    title = {SchedMD Slurm usage statistics},
    url = {https://schedmd.com/},
    urldate = {2022-05-18}
}

% Scheduling
@article{Ullman1975,
    author = {Ullman, J. D.},
    title = {NP-complete Scheduling Problems},
    journal = {J. Comput. Syst. Sci.},
    issue_date = {June, 1975},
    volume = {10},
    number = {3},
    month = jun,
    year = {1975},
    issn = {0022-0000},
    pages = {384--393},
    numpages = {10},
    %    url = {http://dx.doi.org/10.1016/S0022-0000(75)80008-0},
    doi = {10.1016/S0022-0000(75)80008-0},
    acmid = {1740138},
    publisher = {Academic Press, Inc.},
    address = {Orlando, FL, USA},
}
@article{hlfet1974,
    author = {Adam, Thomas L. and Chandy, K. M. and Dickson, J. R.},
    title = {A Comparison of List Schedules for Parallel Processing Systems},
    journal = {Commun. ACM},
    issue_date = {Dec 1974},
    volume = {17},
    number = {12},
    month = dec,
    year = {1974},
    issn = {0001-0782},
    pages = {685--690},
    numpages = {6},
    %    url = {http://doi.acm.org/10.1145/361604.361619},
    doi = {10.1145/361604.361619},
    acmid = {361619},
    publisher = {ACM},
    address = {New York, NY, USA},
}
@inproceedings{kwok1998benchmarking,
    title = {Benchmarking the task graph scheduling algorithms},
    author = {Kwok, Yu-Kwong and Ahmad, Ishfaq},
    booktitle = {ipps},
    pages = {0531},
    year = {1998},
    organization = {IEEE}
}
@article{kwok1999static,
    author = {Kwok, Yu-Kwong and Ahmad, Ishfaq},
    title = {Static Scheduling Algorithms for Allocating Directed Task Graphs to Multiprocessors},
    year = {1999},
    issue_date = {Dec. 1999},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {31},
    number = {4},
    issn = {0360-0300},
    doi = {10.1145/344588.344618},
    journal = {ACM Comput. Surv.},
    month = {dec},
    pages = {406–471},
    numpages = {66},
%    keywords = {automatic parallelization, DAG, task graphs, static scheduling, software tools,
%	parallel processing, multiprocessors}
}
@ARTICLE{sih1993compile,
    author={Sih, G.C. and Lee, E.A.},
    journal={IEEE Transactions on Parallel and Distributed Systems},
    title={A compile-time scheduling heuristic for interconnection-constrained heterogeneous
	processor architectures},
    year={1993},
    volume={4},
    number={2},
    pages={175-187},
    doi={10.1109/71.207593}
}
@ARTICLE{wu1990hypertool,
    author={Wu, M.-Y. and Gajski, D.D.},
    journal={IEEE Transactions on Parallel and Distributed Systems},
    title={Hypertool: a programming aid for message-passing systems},
    year={1990},
    volume={1},
    number={3},
    pages={330-343},
    doi={10.1109/71.80160}
}
@article{hwang1989scheduling,
    title = {Scheduling precedence graphs of bounded height},
    journal = {Journal of Algorithms},
    volume = {5},
    number = {1},
    pages = {48-59},
    year = {1984},
    issn = {0196-6774},
    doi = {https://doi.org/10.1016/0196-6774(84)90039-7},
    author = {Danny Dolev and Manfred K Warmuth},
}
@article{omara2009genetic,
    title = {Genetic algorithms for task scheduling problem},
    journal = {Journal of Parallel and Distributed Computing},
    volume = {70},
    number = {1},
    pages = {13-22},
    year = {2010},
    issn = {0743-7315},
    doi = {10.1016/j.jpdc.2009.09.009},
    author = {Fatma A. Omara and Mona M. Arafa},
%    keywords = {Evolutionary computing, Genetic algorithms, Scheduling, Task partitioning, Graph
%	algorithms, Parallel processing},
}
@article{hagras2003static,
    title = {Static vs. dynamic list-scheduling performance comparison},
    author = {Hagras, Tarek and Jane{\v{c}}ek, J},
    journal = {Acta Polytechnica},
    volume = {43},
    number = {6},
    year = {2003}
}
@article{wang2018list,
    title = {List-Scheduling vs. Cluster-Scheduling},
    author = {Wang, Huijun and Sinnen, Oliver},
    journal = {IEEE Transactions on Parallel and Distributed Systems},
    year = {2018},
    publisher = {IEEE}
}
@ARTICLE{sinnen2005,
    author={Sinnen, O. and Sousa, L.A.},
    journal={IEEE Transactions on Parallel and Distributed Systems},
    title={Communication contention in task scheduling},
    year={2005},
    volume={16},
    number={6},
    pages={503-515},
    doi={10.1109/TPDS.2005.64}
}
@techreport{dagsim,
    TITLE = {{DAGSim: A Simulator for DAG Scheduling Algorithms}},
    AUTHOR = {Jarry, Aubin and Casanova, Henri and Berman, Francine},
%    URL = {https://hal-lara.archives-ouvertes.fr/hal-02101833},
    TYPE = {Research Report},
    NUMBER = {LIP RR-2000-46},
    PAGES = {2+8p},
    INSTITUTION = {{Laboratoire de l'informatique du parall{\'e}lisme}},
    YEAR = {2000},
    MONTH = Dec,
    KEYWORDS = {Heterogeneous Processors ; Load Balancing ; NWS ; Scheduling ; Simulator ; Grid Computing ; Processeurs de vitesses diff{\'e}rentes ; Equilibrage de charge ; Ordonnancement ; Simulateur},
%    PDF = {https://hal-lara.archives-ouvertes.fr/hal-02101833/file/RR2000-46.pdf},
    HAL_ID = {hal-02101833},
    HAL_VERSION = {v1},
}
@INPROCEEDINGS{simdag,
    author={Zulianto, Arief and Kuspriyanto and Gondokaryono, Yudi S.},
    booktitle={2016 6th International Conference on Electronics Information and Emergency Communication (ICEIEC)},
    title={HPC resources scheduling simulation using SimDAG},
    year={2016},
    volume={},
    number={},
    pages={334-337},
    doi={10.1109/ICEIEC.2016.7589751}
}
@article{simgrid,
    hal_id = {hal-01017319},
%    url = {http://hal.inria.fr/hal-01017319},
    title = {Versatile, Scalable, and Accurate Simulation of Distributed Applications and Platforms},
    author = {Casanova, Henri and Giersch, Arnaud and Legrand, Arnaud and Quinson, Martin and Suter, Fr{\'e}d{\'e}ric},
    publisher = {Elsevier},
    pages = {2899-2917},
    journal = {Journal of Parallel and Distributed Computing},
    volume = {74},
    number = {10},
    year = {2014},
    month = Jun,
%    pdf = {http://hal.inria.fr/hal-01017319/PDF/simgrid3-journal.pdf},
}
@article{tang2010list,
    author = {Tang, Xiaoyong and Li, Kenli and Liao, Guiping and Renfa, Li},
    year = {2010},
    month = {04},
    pages = {323-329},
    title = {List scheduling with duplication for heterogeneous computing systems},
    volume = {70},
    journal = {Journal of Parallel and Distributed Computing},
    doi = {10.1016/j.jpdc.2010.01.003}
}
@INPROCEEDINGS{yao2013task,
    author={Yao, Xuanxia and Geng, Peng and Du, Xiaojiang},
    booktitle={2013 International Conference on Parallel and Distributed Computing, Applications
	and Technologies},
    title={A Task Scheduling Algorithm for Multi-core Processors},
    year={2013},
    volume={},
    number={},
    pages={259-264},
    doi={10.1109/PDCAT.2013.47}
}
@article{kwok1996dynamic,
    author = {Kwok, Yu-Kwong and Ahmad, Ishfaq},
    title = {Dynamic Critical-Path Scheduling: An Effective Technique for Allocating Task Graphs
	to Multiprocessors},
    year = {1996},
    issue_date = {May 1996},
    publisher = {IEEE Press},
    volume = {7},
    number = {5},
    issn = {1045-9219},
    doi = {10.1109/71.503776},
    journal = {IEEE Trans. Parallel Distrib. Syst.},
    month = {may},
    pages = {506–521},
    numpages = {16},
%    keywords = {multiprocessors, clustering, processor allocation, list scheduling, parallel
%	scheduling, task graphs., Algorithms}
}

% Distributed/task tools
@inproceedings{dask,
    author = {Rocklin, Matthew},
    year = {2015},
    month = {01},
    pages = {126-132},
    title = {Dask: Parallel Computation with Blocked algorithms and Task Scheduling},
    doi = {10.25080/Majora-7b98e3ed-013}
}
@online{dask-client-perf,
    title = {Task Graph Building Performance Issue in Dask},
    url = {https://github.com/dask/distributed/issues/3783},
    urldate = {2022-05-25},
}
@online{dask-user-survey,
    title = {2021 Dask User Survey},
    author = {Genevieve Buckley},
    url = {https://blog.dask.org/2021/09/15/user-survey},
    urldate = {2022-06-09},
}
@inproceedings{dasksparkcomparison,
    title={A performance comparison of Dask and Apache Spark for data-intensive
    neuroimaging pipelines},
    author={Dugr{\'e}, Mathieu and Hayot-Sasson, Val{\'e}rie and Glatard,
    Tristan},
    booktitle={2019 IEEE/ACM Workflows in Support of Large-Scale Science (WORKS)},
    year={2019},
    organization={IEEE}
}
@article{snakemake,
    author = {Köster, Johannes and Rahmann, Sven},
    title = "{Snakemake—a scalable bioinformatics workflow engine}",
    journal = {Bioinformatics},
    volume = {28},
    number = {19},
    pages = {2520-2522},
    year = {2012},
    month = {08},
%    abstract = "{Summary: Snakemake is a workflow engine that provides a readable Python-based workflow definition language and a powerful execution environment that scales from single-core workstations to compute clusters without modifying the workflow. It is the first system to support the use of automatically inferred multiple named wildcards (or variables) in input and output filenames.Availability:http://snakemake.googlecode.com.Contact:johannes.koester@uni-due.de}",
    issn = {1367-4803},
    doi = {10.1093/bioinformatics/bts480},
    %    url = "https://doi.org/10.1093/bioinformatics/bts480",
%    eprint = {https://academic.oup.com/bioinformatics/article-pdf/28/19/2520/819790/bts480.pdf},
}
@article{nextflow,
    author = {Di Tommaso, Paolo and Chatzou, Maria and Floden, Evan W. and Barja, Pablo and Palumbo, Emilio and Notredame, Cedric},
    year = {2017},
    month = {04},
    pages = {316-319},
    title = {Nextflow enables reproducible computational workflows},
    volume = {35},
    journal = {Nature Biotechnology},
    doi = {10.1038/nbt.3820}
}
@article{aiida,
    author="Huber, Sebastiaan P.
and Zoupanos, Spyros
and Uhrin, Martin
and Talirz, Leopold
and Kahle, Leonid
and H{\"a}uselmann, Rico
and Gresch, Dominik
and M{\"u}ller, Tiziano
and Yakutovich, Aliaksandr V.
and Andersen, Casper W.
and Ramirez, Francisco F.
and Adorf, Carl S.
and Gargiulo, Fernando
and Kumbhar, Snehal
and Passaro, Elsa
and Johnston, Conrad
and Merkys, Andrius
and Cepellotti, Andrea
and Mounet, Nicolas
and Marzari, Nicola
and Kozinsky, Boris
and Pizzi, Giovanni",
    title="AiiDA 1.0, a scalable computational infrastructure for automated reproducible workflows and data provenance",
    journal="Scientific Data",
    year="2020",
    month="Sep",
    day="08",
    volume="7",
    number="1",
    pages="300",
%    abstract="The ever-growing availability of computing power and the sustained development of advanced computational methods have contributed much to recent scientific progress. These developments present new challenges driven by the sheer number of calculations and data to manage. Next-generation exascale supercomputers will harden these challenges, such that automated and scalable solutions become crucial. In recent years, we have been developing AiiDA (aiida.net), a robust open-source high-throughput infrastructure addressing the challenges arising from the needs of automated workflow management and data provenance recording. Here, we introduce developments and capabilities required to reach sustained performance, with AiiDA supporting throughputs of tens of thousands processes/hour, while automatically preserving and storing the full data provenance in a relational database making it queryable and traversable, thus enabling high-performance data analytics. AiiDA's workflow language provides advanced automation, error handling features and a flexible plugin model to allow interfacing with external simulation software. The associated plugin registry enables seamless sharing of extensions, empowering a vibrant user community dedicated to making simulations more robust, user-friendly and reproducible.",
    issn="2052-4463",
    doi="10.1038/s41597-020-00638-4",
%    url="https://doi.org/10.1038/s41597-020-00638-4"
}
@article{streamflow,
%    abstract = {Workflows are among the most commonly used tools in a variety of execution environments. Many of them target a specific environment; few of them make it possible to execute an entire workflow in different environments, e.g. Kubernetes and batch clusters. We present a novel approach to workflow execution, called StreamFlow, that complements the workflow graph with the declarative description of potentially complex execution environments, and that makes it possible the execution onto multiple sites not sharing a common data space. StreamFlow is then exemplified on a novel bioinformatics pipeline for single cell transcriptomic data analysis workflow.},
    author = {Iacopo Colonnelli and Barbara Cantalupo and Ivan Merelli and Marco Aldinucci},
    doi = {10.1109/TETC.2020.3019202},
    journal = {{IEEE} {T}ransactions on {E}merging {T}opics in {C}omputing},
    number = {4},
    pages = {1723--1737},
    title = {{StreamFlow}: cross-breeding cloud with {HPC}},
    volume = {9},
    year = {2021}
}
@article{gromacs,
    author="Lindahl, Erik
and Hess, Berk
and van der Spoel, David",
    title="GROMACS 3.0: a package for molecular simulation and trajectory analysis",
    journal="Molecular modeling annual",
    year="2001",
    month="Aug",
    day="01",
    volume="7",
    number="8",
    pages="306--317",
%    abstract="GROMACS 3.0 is the latest release of a versatile and very well optimized package for molecular simulation. Much effort has been devoted to achieving extremely high performance on both workstations and parallel computers. The design includes an extraction of virial and periodic boundary conditions from the loops over pairwise interactions, and special software routines to enable rapid calculation of x--1/2. Inner loops are generated automatically in C or Fortran at compile time, with optimizations adapted to each architecture. Assembly loops using SSE and 3DNow! Multimedia instructions are provided for x86 processors, resulting in exceptional performance on inexpensive PC workstations. The interface is simple and easy to use (no scripting language), based on standard command line arguments with self-explanatory functionality and integrated documentation. All binary files are independent of hardware endian and can be read by versions of GROMACS compiled using different floating-point precision. A large collection of flexible tools for trajectory analysis is included, with output in the form of finished Xmgr/Grace graphs. A basic trajectory viewer is included, and several external visualization tools can read the GROMACS trajectory format. Starting with version 3.0, GROMACS is available under the GNU General Public License from http://www.gromacs.org.",
    issn="0948-5023",
    doi="10.1007/s008940100045",
%    url="https://doi.org/10.1007/s008940100045"
}
@article{gromacs_mpi,
    title = {GROMACS: High performance molecular simulations through multi-level parallelism from laptops to supercomputers},
    journal = {SoftwareX},
    volume = {1-2},
    pages = {19-25},
    year = {2015},
    issn = {2352-7110},
    doi = {https://doi.org/10.1016/j.softx.2015.06.001},
%    url = {https://www.sciencedirect.com/science/article/pii/S2352711015000059},
    author = {Mark James Abraham and Teemu Murtola and Roland Schulz and Szilárd Páll and Jeremy C. Smith and Berk Hess and Erik Lindahl},
%    keywords = {Molecular dynamics, GPU, SIMD, Free energy},
%    abstract = {GROMACS is one of the most widely used open-source and free software codes in chemistry, used primarily for dynamical simulations of biomolecules. It provides a rich set of calculation types, preparation and analysis tools. Several advanced techniques for free-energy calculations are supported. In version 5, it reaches new performance heights, through several new and enhanced parallelization algorithms. These work on every level; SIMD registers inside cores, multithreading, heterogeneous CPU–GPU acceleration, state-of-the-art 3D domain decomposition, and ensemble-level parallelization through built-in replica exchange and the separate Copernicus framework. The latest best-in-class compressed trajectory storage format is supported.}
}
@inproceedings{hyperloom,
    author = {Cima, Vojtěch and Böhm, Stanislav and Martinovič, Jan and Dvorský, Jiří and Janurová, Kateřina and Aa, Tom Vander and Ashby, Thomas J. and Chupakhin, Vladimir},
    title = {HyperLoom: A Platform for Defining and Executing Scientific Pipelines in Distributed Environments},
    year = {2018},
    isbn = {9781450364447},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
%    url = {https://doi.org/10.1145/3183767.3183768},
    doi = {10.1145/3183767.3183768},
    pages = {1–6},
    numpages = {6},
    %    keywords = {Chemogenomics, Distributed Computing, Task Scheduling, Scientific Pipeline, Big Data, HPC, Machine Learning},
    series = {PARMA-DITAM '18}
}
@inproceedings{ray,
    author = {Moritz, Philipp and Nishihara, Robert and Wang, Stephanie and Tumanov, Alexey and Liaw, Richard and Liang, Eric and Elibol, Melih and Yang, Zongheng and Paul, William and Jordan, Michael I. and Stoica, Ion},
    title = {Ray: A Distributed Framework for Emerging AI Applications},
    year = {2018},
    isbn = {9781931971478},
    publisher = {USENIX Association},
    address = {USA},
%    abstract = {The next generation of AI applications will continuously interact with the environment and learn from these interactions. These applications impose new and demanding systems requirements, both in terms of performance and flexibility. In this paper, we consider these requirements and present Ray--a distributed system to address them. Ray implements a unified interface that can express both task-parallel and actor-based computations, supported by a single dynamic execution engine. To meet the performance requirements, Ray employs a distributed scheduler and a distributed and fault-tolerant store to manage the system's control state. In our experiments, we demonstrate scaling beyond 1.8 million tasks per second and better performance than existing specialized systems for several challenging reinforcement learning applications.},
    booktitle = {Proceedings of the 13th USENIX Conference on Operating Systems Design and Implementation},
    pages = {561–577},
    numpages = {17},
    location = {Carlsbad, CA, USA},
    series = {OSDI'18}
}
@inproceedings{parsl,
    author = {Babuji, Yadu and Woodard, Anna and Li, Zhuozhao and Katz, Daniel S. and Clifford, Ben and Kumar, Rohan and Lacinski, Lukasz and Chard, Ryan and Wozniak, Justin M. and Foster, Ian and Wilde, Michael and Chard, Kyle},
    title = {Parsl: Pervasive Parallel Programming in Python},
    year = {2019},
    isbn = {9781450366700},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    %    url = {https://doi.org/10.1145/3307681.3325400},
    doi = {10.1145/3307681.3325400},
%    abstract = {High-level programming languages such as Python are increasingly used to provide intuitive interfaces to libraries written in lower-level languages and for assembling applications from various components. This migration towards orchestration rather than implementation, coupled with the growing need for parallel computing (e.g., due to big data and the end of Moore's law), necessitates rethinking how parallelism is expressed in programs. Here, we present Parsl, a parallel scripting library that augments Python with simple, scalable, and flexible constructs for encoding parallelism. These constructs allow Parsl to construct a dynamic dependency graph of components that it can then execute efficiently on one or many processors. Parsl is designed for scalability, with an extensible set of executors tailored to different use cases, such as low-latency, high-throughput, or extreme-scale execution. We show, via experiments on the Blue Waters supercomputer, that Parsl executors can allow Python scripts to execute components with as little as 5 ms of overhead, scale to more than 250000 workers across more than 8000 nodes, and process upward of 1200 tasks per second. Other Parsl features simplify the construction and execution of composite programs by supporting elastic provisioning and scaling of infrastructure, fault-tolerant execution, and integrated wide-area data management. We show that these capabilities satisfy the needs of many-task, interactive, online, and machine learning applications in fields such as biology, cosmology, and materials science.},
    booktitle = {Proceedings of the 28th International Symposium on High-Performance Parallel and Distributed Computing},
    pages = {25–36},
    numpages = {12},
    location = {Phoenix, AZ, USA},
    series = {HPDC '19}
}
@article{pycompss,
    author = {Tejedor, Enric and Becerra, Yolanda and Alomar, Guillem and Queralt, Anna and Badia, Rosa M. and Torres, Jordi and Cortes, Toni and Labarta, Jesús},
    year = {2015},
    month = {08},
    pages = {},
    title = {PyCOMPSs: Parallel computational workflows in Python},
    volume = {31},
    journal = {International Journal of High Performance Computing Applications},
    doi = {10.1177/1094342015594678}
}
@article{pegasus,
    author={Deelman, Ewa and Vahi, Karan and Rynge, Mats and Mayani, Rajiv and da Silva, Rafael Ferreira and Papadimitriou, George and Livny, Miron},
    journal={Computing in Science \& Engineering},
    title={The Evolution of the Pegasus Workflow Management Software},
    year={2019},
    volume={21},
    number={4},
    pages={22-36},
    doi={10.1109/MCSE.2019.2919690}
}
@Article{starpu,
    author = {C{\'e}dric Augonnet and Samuel Thibault and Raymond Namyst and Pierre-Andr{\'e} Wacrenier},
    title = {{StarPU: A Unified Platform for Task Scheduling on Heterogeneous Multicore Architectures}},
    journal = {CCPE - Concurrency and Computation: Practice and Experience, Special Issue: Euro-Par 2009},
    volume = 23,
    issue = 2,
    pages = {187--198},
    year = 2011,
    month = FEB,
    publisher = {John Wiley \& Sons, Ltd.},
    doi = {10.1002/cpe.1631},
    %    url = {http://hal.inria.fr/inria-00550877},
    %    pdf = {http://hal.inria.fr/inria-00550877/document},
    KEYWORDS = {General Presentations;StarPU}
}
@inproceedings{dace,
    author = {Ben-Nun, Tal and de Fine Licht, Johannes and Ziogas, Alexandros N. and Schneider, Timo and Hoefler, Torsten},
    title = {Stateful Dataflow Multigraphs: A Data-Centric Model for Performance Portability on Heterogeneous Architectures},
    year = {2019},
    isbn = {9781450362290},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    %    url = {https://doi.org/10.1145/3295500.3356173},
    doi = {10.1145/3295500.3356173},
    %    abstract = {The ubiquity of accelerators in high-performance computing has driven programming complexity beyond the skill-set of the average domain scientist. To maintain performance portability in the future, it is imperative to decouple architecture-specific programming paradigms from the underlying scientific computations. We present the Stateful DataFlow multiGraph (SDFG), a data-centric intermediate representation that enables separating program definition from its optimization. By combining fine-grained data dependencies with high-level control-flow, SDFGs are both expressive and amenable to program transformations, such as tiling and double-buffering. These transformations are applied to the SDFG in an interactive process, using extensible pattern matching, graph rewriting, and a graphical user interface. We demonstrate SDFGs on CPUs, GPUs, and FPGAs over various motifs --- from fundamental computational kernels to graph analytics. We show that SDFGs deliver competitive performance, allowing domain scientists to develop applications naturally and port them to approach peak hardware performance without modifying the original scientific code.},
    booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
    articleno = {81},
    numpages = {14},
    location = {Denver, Colorado},
    series = {SC '19}
}
@inproceedings{legate,
    author = {Bauer, Michael and Garland, Michael},
    title = {Legate NumPy: Accelerated and Distributed Array Computing},
    year = {2019},
    isbn = {9781450362290},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
%    url = {https://doi.org/10.1145/3295500.3356175},
    doi = {10.1145/3295500.3356175},
%    abstract = {NumPy is a popular Python library used for performing array-based numerical computations. The canonical implementation of NumPy used by most programmers runs on a single CPU core and is parallelized to use multiple cores for some operations. This restriction to a single-node CPU-only execution limits both the size of data that can be handled and the potential speed of NumPy code. In this work we introduce Legate, a drop-in replacement for NumPy that requires only a single-line code change and can scale up to an arbitrary number of GPU accelerated nodes. Legate works by translating NumPy programs to the Legion programming model and then leverages the scalability of the Legion runtime system to distribute data and computations across an arbitrary sized machine. Compared to similar programs written in the distributed Dask array library in Python, Legate achieves speed-ups of up to 10X on 1280 CPUs and 100X on 256 GPUs.},
    booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
    articleno = {23},
    numpages = {23},
%    keywords = {HPC, control replication, logical regions, NumPy, distributed execution, GPU, Python, legate, task-based runtimes, legion},
    location = {Denver, Colorado},
    series = {SC '19}
}
@INPROCEEDINGS{pygion,
    author={Slaughter, Elliott and Aiken, Alex},
    booktitle={2019 IEEE/ACM Parallel Applications Workshop, Alternatives To MPI (PAW-ATM)},
    title={Pygion: Flexible, Scalable Task-Based Parallelism with Python},
    year={2019},
    volume={},
    number={},
    pages={58-72},
    doi={10.1109/PAW-ATM49560.2019.00011}
}
@article{flink,
    author = {Carbone, Paris and Katsifodimos, Asterios and Kth, † and Sweden, Sics and Ewen, Stephan and Markl, Volker and Haridi, Seif and Tzoumas, Kostas},
    year = {2015},
    month = {01},
%    pages = {},
    title = {Apache Flink™: Stream and Batch Processing in a Single Engine},
    volume = {38},
    journal = {IEEE Data Engineering Bulletin}
}
@INPROCEEDINGS{kafka,
    author={Shree, Rishika and Choudhury, Tanupriya and Gupta, Subhash Chand and Kumar, Praveen},
    booktitle={2017 2nd International Conference on Telecommunication and Networks (TEL-NET)},
    title={KAFKA: The modern platform for data management and analysis in big data domain},
    year={2017},
    volume={},
    number={},
    pages={1-5},
    %    keywords={Real-time systems;Distributed databases;Tools;File systems;Ecosystems;Servers;Reliability;Apache Kafka;hadoop;stream processing;low latency;flume;high throughput;publish-subscribe;producer;consumer},
    doi={10.1109/TEL-NET.2017.8343593}
}
@inproceedings{timely_dataflow,
    author = {Murray, Derek G. and McSherry, Frank and Isaacs, Rebecca and Isard, Michael and Barham, Paul and Abadi, Mart\'{\i}n},
    title = {Naiad: a timely dataflow system},
    year = {2013},
    isbn = {9781450323888},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
%    url = {https://doi.org/10.1145/2517349.2522738},
    doi = {10.1145/2517349.2522738},
%    abstract = {Naiad is a distributed system for executing data parallel, cyclic dataflow programs. It offers the high throughput of batch processors, the low latency of stream processors, and the ability to perform iterative and incremental computations. Although existing systems offer some of these features, applications that require all three have relied on multiple platforms, at the expense of efficiency, maintainability, and simplicity. Naiad resolves the complexities of combining these features in one framework.A new computational model, timely dataflow, underlies Naiad and captures opportunities for parallelism across a wide class of algorithms. This model enriches dataflow computation with timestamps that represent logical points in the computation and provide the basis for an efficient, lightweight coordination mechanism.We show that many powerful high-level programming models can be built on Naiad's low-level primitives, enabling such diverse tasks as streaming data analysis, iterative machine learning, and interactive graph mining. Naiad outperforms specialized systems in their target application domains, and its unique features enable the development of new high-performance applications.},
    booktitle = {Proceedings of the Twenty-Fourth ACM Symposium on Operating Systems Principles},
    pages = {439–455},
    numpages = {17},
    location = {Farminton, Pennsylvania},
    series = {SOSP '13}
}
@article{sciluigi,
    author = {Lampa, Samuel and Alvarsson, Jonathan and Spjuth, Ola},
    year = {2016},
    month = {11},
    pages = {},
    title = {Towards agile large-scale predictive modelling in drug discovery with flow-based programming design principles},
    volume = {8},
    journal = {Journal of Cheminformatics},
    doi = {10.1186/s13321-016-0179-6}
}
@article{vaex,
    author = {Breddels, Maarten and Veljanoski, Jovan},
    year = {2018},
    month = {01},
    pages = {},
    title = {Vaex: Big Data exploration in the era of Gaia},
    volume = {618},
    journal = {Astronomy \& Astrophysics},
    doi = {10.1051/0004-6361/201732493}
}
@Manual{cudf,
    title = {RAPIDS: Libraries for End to End GPU Data Science},
    author = {RAPIDS Development Team},
    year = {2023},
    url = {https://rapids.ai}
}
@phdthesis{modin,
    Author = {Petersohn, Devin},
    Title = {Dataframe Systems: Theory, Architecture, and Implementation},
    School = {EECS Department, University of California, Berkeley},
    Year = {2021},
    Month = {Aug},
    URL = {http://www2.eecs.berkeley.edu/Pubs/TechRpts/2021/EECS-2021-193.html},
    Number = {UCB/EECS-2021-193},
%    Abstract = {Dataframes are a popular abstraction to represent, prepare, and analyze data. Despite the remarkable success of dataframe libraries in R and Python, dataframes face performance issues even on moderately large datasets. Moreover, there is significant ambiguity regarding dataframe semantics. In this thesis, we discuss the implications of signature dataframe features including flexible schemas, ordering, row/column equivalence, and data/meta-data fluidity, as well as the piecemeal, trial-and-error-based approach to interacting with dataframes. While most modern systems aim to scale dataframe workloads by changing properties of dataframes – or by adding new distributed systems knowledge requirements– we believe it is important to support scalable ops on dataframes without changing their semantics. This thesis takes a ground-up approach towards scaling dataframe systems,starting with a formal data model and algebra, and ending with a reference implementation. This implementation, Modin, has already accumulated a significant amount of community support: over 6,000 GitHub stars and over 1 million installs to date. This interest shows the need for systems that solve modern data science problems without changing semantics. Included in this thesis are several of our insights into how to build systems for data scientists and what data scientists prioritize. We believe these insights were instrumental in unlocking the interest and support from the community in our open source work.}
}
@article{merlin,
    title = {Enabling machine learning-ready HPC ensembles with Merlin},
    journal = {Future Generation Computer Systems},
    volume = {131},
    pages = {255-268},
    year = {2022},
    issn = {0167-739X},
    doi = {https://doi.org/10.1016/j.future.2022.01.024},
%    url = {https://www.sciencedirect.com/science/article/pii/S0167739X22000322},
    author = {J. Luc Peterson and Ben Bay and Joe Koning and Peter Robinson and Jessica Semler and Jeremy White and Rushil Anirudh and Kevin Athey and Peer-Timo Bremer and Francesco {Di Natale} and David Fox and Jim A. Gaffney and Sam A. Jacobs and Bhavya Kailkhura and Bogdan Kustowski and Steven Langer and Brian Spears and Jayaraman Thiagarajan and Brian {Van Essen} and Jae-Seung Yeom},
%    keywords = {Workflow management, Scientific computing, Machine learning, High performance computing, High throughput computing, Distributed computing},
%    abstract = {With the growing complexity of computational and experimental facilities, many scientific researchers are turning to machine learning (ML) techniques to analyze large scale ensemble data. With complexities such as multi-component workflows, heterogeneous machine architectures, parallel file systems, and batch scheduling, care must be taken to facilitate this analysis in a high performance computing (HPC) environment. In this paper, we present Merlin, a workflow framework to enable large ML-friendly ensembles of scientific HPC simulations. By augmenting traditional HPC with distributed compute technologies, Merlin aims to lower the barrier for scientific subject matter experts to incorporate ML into their analysis. As a producer–consumer workflow model, Merlin enables multi-machine, cross-batch job, dynamically allocated yet persistent workflows capable of utilizing surge-compute resources. Key features of Merlin are a flexible HPC-centric interface, low per-task overhead, multi-tiered fault recovery, and a hierarchical sampling algorithm that allows for O(N) task execution and O(NlnN) task queuing to ensembles of millions of tasks. In addition to Merlin’s design, we test the algorithm’s performance in an HPC center and demonstrate the ability to enqueue 40 million simulations in 100 s, with a 30 millisecond per-task overhead that is independent of ensemble size. Finally, we describe some example applications that Merlin has enabled on leadership-class HPC resources, such as the ML-augmented optimization of nuclear fusion experiments and the calibration of infectious disease models to study the progression of and possible mitigation strategies for COVID-19.}
}
@article {fireworks,
    author = {Jain, Anubhav and Ong, Shyue Ping and Chen, Wei and Medasani, Bharat and Qu, Xiaohui and Kocher, Michael and Brafman, Miriam and Petretto, Guido and Rignanese, Gian-Marco and Hautier, Geoffroy and Gunter, Daniel and Persson, Kristin A.},
    title = {FireWorks: a dynamic workflow system designed for high-throughput applications},
    journal = {Concurrency and Computation: Practice and Experience},
    volume = {27},
    number = {17},
    issn = {1532-0634},
%    url = {http://dx.doi.org/10.1002/cpe.3505},
    doi = {10.1002/cpe.3505},
    pages = {5037--5059},
%    keywords = {scientific workflows, high-throughput computing, fault-tolerant computing},
    year = {2015},
    note = {CPE-14-0307.R2},
}
@INPROCEEDINGS{autosubmit,
    author={Manubens-Gil, Domingo and Vegas-Regidor, Javier and Prodhomme, Chloe and Mula-Valls, Oriol and Doblas-Reyes, Francisco J.},
    booktitle={2016 International Conference on High Performance Computing & Simulation (HPCS)},
    title={Seamless management of ensemble climate prediction experiments on HPC platforms},
    year={2016},
    volume={},
    number={},
    pages={895-900},
%    keywords={Computational modeling;Atmospheric modeling;Monitoring;Earth;Data models;Weather forecasting;Earth System Modelling;Climate;Weather;High Performance Computing;Workflow manager},
    doi={10.1109/HPCSim.2016.7568429}
}
@inproceedings{pydra,
    author = {Jarecka, Dorota and Goncalves, Mathias and Markiewicz, Christopher and Esteban, Oscar and Lo, Nicole and Kaczmarzyk, Jakub and Ghosh, Satrajit},
    year = {2020},
    month = {01},
    pages = {132-139},
    title = {Pydra - a flexible and lightweight dataflow engine for scientific analyses},
    doi = {10.25080/Majora-342d178e-012}
}
@inproceedings{hpx,
    author = {Kaiser, Hartmut and Heller, Thomas and Adelstein-Lelbach, Bryce and Serio, Adrian and Fey, Dietmar},
    title = {HPX: A Task Based Programming Model in a Global Address Space},
    year = {2014},
    isbn = {9781450332477},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
%    url = {https://doi.org/10.1145/2676870.2676883},
    doi = {10.1145/2676870.2676883},
%    abstract = {The significant increase in complexity of Exascale platforms due to energy-constrained, billion-way parallelism, with major changes to processor and memory architecture, requires new energy-efficient and resilient programming techniques that are portable across multiple future generations of machines. We believe that guaranteeing adequate scalability, programmability, performance portability, resilience, and energy efficiency requires a fundamentally new approach, combined with a transition path for existing scientific applications, to fully explore the rewards of todays and tomorrows systems. We present HPX -- a parallel runtime system which extends the C++11/14 standard to facilitate distributed operations, enable fine-grained constraint based parallelism, and support runtime adaptive resource management. This provides a widely accepted API enabling programmability, composability and performance portability of user applications. By employing a global address space, we seamlessly augment the standard to apply to a distributed case. We present HPX's architecture, design decisions, and results selected from a diverse set of application runs showing superior performance, scalability, and efficiency over conventional practice.},
    booktitle = {Proceedings of the 8th International Conference on Partitioned Global Address Space Programming Models},
    articleno = {6},
    numpages = {11},
%    keywords = {Exascale, Global Address Space, High Performance Computing, Parallel Runtime Systems, Programming Models},
    location = {Eugene, OR, USA},
    series = {PGAS '14}
}
@ARTICLE{parsec,
    author={Bosilca, George and Bouteiller, Aurelien and Danalis, Anthony and Faverge, Mathieu and Herault, Thomas and Dongarra, Jack J.},
    journal={Computing in Science & Engineering},
    title={PaRSEC: Exploiting Heterogeneity to Enhance Scalability},
    year={2013},
    volume={15},
    number={6},
    pages={36-45},
%    keywords={Programming;Computer architecture;Runtime;Computational modeling;Parallel processing;Biological system modeling;Adaptation models;Scalability;Programming;Computer architecture;Runtime;Computational modeling;Parallel processing;Biological system modeling;Adaptation models;Scalability;scientific computing;high-performance computing;HPC;scheduling and task partitioning;distributed programming;programming paradigms},
    doi={10.1109/MCSE.2013.98}
}
@INPROCEEDINGS{legion,
    author={Bauer, Michael and Treichler, Sean and Slaughter, Elliott and Aiken, Alex},
    booktitle={SC '12: Proceedings of the International Conference on High Performance Computing, Networking, Storage and Analysis},
    title={Legion: Expressing locality and independence with logical regions},
    year={2012},
    volume={},
    number={},
    pages={1-11},
%    keywords={Wires;Coherence;Vegetation;Circuit simulation;Out of order;Programming},
    doi={10.1109/SC.2012.71}
}
@article{airfow,
    author = {Kotliar, Michael and Kartashov, Andrey V and Barski, Artem},
    title = "{CWL-Airflow: a lightweight pipeline manager supporting Common Workflow Language}",
    journal = {GigaScience},
    volume = {8},
    number = {7},
    pages = {giz084},
    year = {2019},
    month = {07},
%    abstract = "{Massive growth in the amount of research data and computational analysis has led to increased use of pipeline managers in biomedical computational research. However, each of the \\&gt;100 such managers uses its own way to describe pipelines, leading to difficulty porting workflows to different environments and therefore poor reproducibility of computational studies. For this reason, the Common Workflow Language (CWL) was recently introduced as a specification for platform-independent workflow description, and work began to transition existing pipelines and workflow managers to CWL.Herein, we present CWL-Airflow, a package that adds support for CWL to the Apache Airflow pipeline manager. CWL-Airflow uses CWL version 1.0 specification and can run workflows on stand-alone MacOS/Linux servers, on clusters, or on a variety of cloud platforms. A sample CWL pipeline for processing of chromatin immunoprecipitation sequencing data is provided.CWL-Airflow will provide users with the features of a fully fledged pipeline manager and the ability to execute CWL workflows anywhere Airflow can run—from a laptop to a cluster or cloud environment. CWL-Airflow is available under Apache License, version 2.0 (Apache-2.0), and can be downloaded from https://barski-lab.github.io/cwl-airflow, https://scicrunch.org/resolver/RRID:SCR\_017196.}",
    issn = {2047-217X},
    doi = {10.1093/gigascience/giz084},
%    url = {https://doi.org/10.1093/gigascience/giz084},
%    eprint = {https://academic.oup.com/gigascience/article-pdf/8/7/giz084/28954484/giz084.pdf},
}

% Datasets
@dataset{estee_graphs,
    author       = {Jakub Beránek and
                  Stanislav Böhm and
                  Vojtěch Cima},
    title        = {Task graphs for benchmarking schedulers},
    month        = apr,
    year         = 2019,
    publisher    = {Zenodo},
    version      = {1.0},
    doi          = {10.5281/zenodo.2630385},
}
@dataset{estee_results,
    author       = {Jakub Beránek and
                  Stanislav Böhm and
                  Vojtěch Cima},
    title        = {Task Scheduler Performance Survey Results},
    month        = apr,
    year         = 2019,
    publisher    = {Zenodo},
    version      = {1.0},
    doi          = {10.5281/zenodo.2630589},
}
@dataset{airdataset,
    author = {Kalnay et al.},
    title = {The NCEP/NCAR 40-year reanalysis project},
    howpublished = {Bull. Amer. Meteor. Soc., 77, 437-470},
    year = {1996}
}
