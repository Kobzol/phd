% General
@inproceedings{wrf,
    title = {The {Weather} {Reseach} and {Forecast} {Model}: {Software} {Architecture} and {Performance}},
    author = {Michalakes, John and Dudhia, Jimy and Gill, D. and Henderson, Tom and Klemp, J. and Skamarock, W. and Wang, Wei},
    month = jan,
    year = {2004},
}
@inproceedings{cfd,
    title={CFD Vision 2030 Study: A Path to Revolutionary Computational Aerosciences},
    author={Jeffrey P. Slotnick and Abdollah Khodadoust and Juan J. Alonso and David L. Darmofal and William Gropp and Elizabeth A. Lurie and Dimitri J. Mavriplis},
    year={2014}
}
@article{hpcdl,
    author = {Ben-Nun, Tal and Hoefler, Torsten},
    title = {Demystifying Parallel and Distributed Deep Learning: An In-Depth Concurrency Analysis},
    year = {2019},
    issue_date = {July 2020},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {52},
    number = {4},
    issn = {0360-0300},
%    url = {https://doi.org/10.1145/3320060},
    doi = {10.1145/3320060},
%    abstract = {Deep Neural Networks (DNNs) are becoming an important tool in modern computing applications. Accelerating their training is a major challenge and techniques range from distributed algorithms to low-level circuit design. In this survey, we describe the problem from a theoretical perspective, followed by approaches for its parallelization. We present trends in DNN architectures and the resulting implications on parallelization strategies. We then review and model the different types of concurrency in DNNs: from the single operator, through parallelism in network inference and training, to distributed deep learning. We discuss asynchronous stochastic optimization, distributed system architectures, communication schemes, and neural architecture search. Based on those approaches, we extrapolate potential directions for parallelism in deep learning.},
    journal = {ACM Comput. Surv.},
    month = aug,
    articleno = {65},
    numpages = {43},
%    keywords = {parallel algorithms, Deep learning, distributed computing}
}
@InProceedings{bioinformatics,
    author="Per{\'e}z-S{\'a}nchez, Horacio
and Fassihi, Afshin
and Cecilia, Jos{\'e} M.
and Ali, Hesham H.
and Cannataro, Mario",
%    editor="Ortu{\~{n}}o, Francisco
%and Rojas, Ignacio",
    title="Applications of High Performance Computing in Bioinformatics, Computational Biology and Computational Chemistry",
    booktitle="Bioinformatics and Biomedical Engineering",
    year="2015",
    publisher="Springer International Publishing",
    address="Cham",
    pages="527--541",
%    abstract="In the last 10 years, we are witnessing one of the major revolutions in parallel systems. The consolidation of heterogeneous systems at different levels -from desktop computers to large-scale systems such as supercomputers, clusters or grids, through all kinds of low-power devices- is providing a computational power unimaginable just few years ago, trying to follow the wake of Moore's law. This landscape in the high performance computing arena opens up great opportunities in the simulation of relevant biological systems and for applications in Bioinformatics, Computational Biology and Computational Chemistry. This introductory article shows the last tendencies of this active research field and our perspectives for the forthcoming years.",
    isbn="978-3-319-16480-9"
}
@Inbook{memorywall,
    author="McKee, Sally A.
and Wisniewski, Robert W.",
%    editor="Padua, David",
    title="Memory Wall",
    bookTitle="Encyclopedia of Parallel Computing",
    year="2011",
    publisher="Springer US",
    address="Boston, MA",
    pages="1110--1116",
    isbn="978-0-387-09766-4",
    doi="10.1007/978-0-387-09766-4_234",
%    url="https://doi.org/10.1007/978-0-387-09766-4_234"
}
@Inbook{powerwall,
    author="Bose, Pradip",
%    editor="Padua, David",
    title="Power Wall",
    bookTitle="Encyclopedia of Parallel Computing",
    year="2011",
    publisher="Springer US",
    address="Boston, MA",
    pages="1593--1608",
    isbn="978-0-387-09766-4",
    doi="10.1007/978-0-387-09766-4_499",
%    url="https://doi.org/10.1007/978-0-387-09766-4_499"
}
@article{xeonphi,
    title={Knights Landing: Second-Generation Intel Xeon Phi Product},
    author={Avinash Sodani and Roger Gramunt and Jes{\'u}s Corbal and Ho-Seop Kim and Krishna Vinod and Sundaram Chinthamani and Steven Hutsell and Rajat Agarwal and Yen-Chen Liu},
    journal={IEEE Micro},
    year={2016},
    volume={36},
    pages={34-46}
}
@inproceedings{mpistudy,
    author = {Laguna, Ignacio and Marshall, Ryan and Mohror, Kathryn and Ruefenacht, Martin and Skjellum, Anthony and Sultana, Nawrin},
    title = {A Large-Scale Study of MPI Usage in Open-Source HPC Applications},
    year = {2019},
    isbn = {9781450362290},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    doi = {10.1145/3295500.3356176},
    booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
    articleno = {31},
    numpages = {14},
    location = {Denver, Colorado},
    series = {SC '19}
}
@ARTICLE{mooreslaw,
    author={Schaller, R.R.},
    journal={IEEE Spectrum},
    title={Moore's law: past, present and future},
    year={1997},
    volume={34},
    number={6},
    pages={52-59},
    doi={10.1109/6.591665}
}
@online{top500gpu,
    author={Feldman, Michael},
    url={https://www.top500.org/news/new-gpu-accelerated-supercomputers-change-the-balance-of-power-on-the-top500/},
    urldate = {2023-04-21},
}
@inproceedings{top500analysis,
    author = {Khan, Awais and Sim, Hyogi and Vazhkudai, Sudharshan S. and Butt, Ali R. and Kim, Youngjae},
    title = {An Analysis of System Balance and Architectural Trends Based on Top500 Supercomputers},
    year = {2021},
    isbn = {9781450388429},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
%    url = {https://doi.org/10.1145/3432261.3432263},
    doi = {10.1145/3432261.3432263},
%    abstract = {Supercomputer design is a complex, multi-dimensional optimization process, wherein several subsystems need to be reconciled to meet a desired figure of merit performance for a portfolio of applications and a budget constraint. However, overall, the HPC community has been gravitating towards ever more Flops, at the expense of many other subsystems. To draw attention to overall system balance, in this paper, we analyze balance ratios and architectural trends in the world’s most powerful supercomputers. Specifically, we have collected the performance characteristics of systems between 1993 and 2019 based on the Top500 lists and then analyzed their architectures from diverse system design perspectives. Notably, our analysis studies the performance balance of the machines, across a variety of subsystems such as compute, memory, I/O, interconnect, intra-node connectivity and power. Our analysis reveals that balance ratios of the various subsystems need to be considered carefully alongside the application workload portfolio to provision the subsystem capacity and bandwidth specifications, which can help achieve optimal performance.},
    booktitle = {The International Conference on High Performance Computing in Asia-Pacific Region},
    pages = {11–22},
    numpages = {12},
    %    keywords = {Architectural Trends and Performance Balance Ratio, High Performance Computing, Top500 Supercomputers},
    location = {Virtual Event, Republic of Korea},
    series = {HPC Asia 2021}
}
@online{it4i_node_scheduling_policy,
    title = {IT4Innovations Job Submission Policy},
    url = {https://docs.it4i.cz/general/resource_allocation_and_job_execution/\#job-submission-and-execution},
    urldate = {2023-04-28},
}
@article{graph_partitioning,
    author="Feldmann, Andreas Emil and Foschini, Luca",
    title="Balanced Partitions of Trees and Applications",
    journal="Algorithmica",
    year="2015",
    month="Feb",
    day="01",
    volume="71",
    number="2",
    pages="354--376",
%    abstract="We study the problem of finding the minimum number of edges that, when cut, form a partition of the vertices into k sets of equal size. This is called the k-BALANCED PARTITIONING problem. The problem is known to be inapproximable within any finite factor on general graphs, while little is known about restricted graph classes.",
    issn="1432-0541",
    doi="10.1007/s00453-013-9802-3",
%    url="https://doi.org/10.1007/s00453-013-9802-3"
}
@article{mapreduce,
    author = {Dean, Jeffrey and Ghemawat, Sanjay},
    title = {MapReduce: Simplified Data Processing on Large Clusters},
    year = {2008},
    issue_date = {January 2008},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {51},
    number = {1},
    issn = {0001-0782},
%    url = {https://doi.org/10.1145/1327452.1327492},
    doi = {10.1145/1327452.1327492},
%    abstract = {MapReduce is a programming model and an associated implementation for processing and generating large datasets that is amenable to a broad variety of real-world tasks. Users specify the computation in terms of a map and a reduce function, and the underlying runtime system automatically parallelizes the computation across large-scale clusters of machines, handles machine failures, and schedules inter-machine communication to make efficient use of the network and disks. Programmers find the system easy to use: more than ten thousand distinct MapReduce programs have been implemented internally at Google over the past four years, and an average of one hundred thousand MapReduce jobs are executed on Google's clusters every day, processing a total of more than twenty petabytes of data per day.},
    journal = {Commun. ACM},
    month = {jan},
    pages = {107–113},
    numpages = {7}
}
@inproceedings{workflows_at_scale,
    author = {Bhatia, Harsh and Di Natale, Francesco and Moon, Joseph Y. and Zhang, Xiaohua and Chavez, Joseph R. and Aydin, Fikret and Stanley, Chris and Oppelstrup, Tomas and Neale, Chris and Schumacher, Sara Kokkila and Ahn, Dong H. and Herbein, Stephen and Carpenter, Timothy S. and Gnanakaran, Sandrasegaram and Bremer, Peer-Timo and Glosli, James N. and Lightstone, Felice C. and Ing\'{o}lfsson, Helgi I.},
    title = {Generalizable Coordination of Large Multiscale Workflows: Challenges and Learnings at Scale},
    year = {2021},
    isbn = {9781450384421},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
%    url = {https://doi.org/10.1145/3458817.3476210},
    doi = {10.1145/3458817.3476210},
%    abstract = {The advancement of machine learning techniques and the heterogeneous architectures of most current supercomputers are propelling the demand for large multiscale simulations that can automatically and autonomously couple diverse components and map them to relevant resources to solve complex problems at multiple scales. Nevertheless, despite the recent progress in workflow technologies, current capabilities are limited to coupling two scales. In the first-ever demonstration of using three scales of resolution, we present a scalable and generalizable framework that couples pairs of models using machine learning and in situ feedback. We expand upon the massively parallel Multiscale Machine-Learned Modeling Infrastructure (MuMMI), a recent, award-winning workflow, and generalize the framework beyond its original design. We discuss the challenges and learnings in executing a massive multiscale simulation campaign that utilized over 600,000 node hours on Summit and achieved more than 98\% GPU occupancy for more than 83\% of the time. We present innovations to enable several orders of magnitude scaling, including simultaneously coordinating 24,000 jobs, and managing several TBs of new data per day and over a billion files in total. Finally, we describe the generalizability of our framework and, with an upcoming open-source release, discuss how the presented framework may be used for new applications.},
    booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
    articleno = {10},
    numpages = {16},
%    keywords = {heterogenous architecture, cancer research, adaptive simulations, multiscale simulations, massively parallel, machine learning},
    location = {St. Louis, Missouri},
    series = {SC '21}
}
@article{dragonfly,
    title={Technology-driven, highly-scalable dragonfly topology},
    author={Kim, John and Dally, Wiliam J and Scott, Steve and Abts, Dennis},
    journal={ACM SIGARCH Computer Architecture News},
    volume={36},
    number={3},
    pages={77--88},
    year={2008},
    publisher={ACM New York, NY, USA}
}
@INPROCEEDINGS{slimfly,
    author={Besta, Maciej and Hoefler, Torsten},
    booktitle={SC '14: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
    title={Slim Fly: A Cost Effective Low-Diameter Network Topology},
    year={2014},
    volume={},
    number={},
    pages={348-359},
    doi={10.1109/SC.2014.34}
}
@Article{task_based_taxonomy,
    author="Thoman, Peter
and Dichev, Kiril
and Heller, Thomas
and Iakymchuk, Roman
and Aguilar, Xavier
and Hasanov, Khalid
and Gschwandtner, Philipp
and Lemarinier, Pierre
and Markidis, Stefano
and Jordan, Herbert
and Fahringer, Thomas
and Katrinis, Kostas
and Laure, Erwin
and Nikolopoulos, Dimitrios S.",
    title="A taxonomy of task-based parallel programming technologies for high-performance computing",
    journal="The Journal of Supercomputing",
    year="2018",
    month="Apr",
    day="01",
    volume="74",
    number="4",
    pages="1422--1434",
%    abstract="Task-based programming models for shared memory---such as Cilk Plus and OpenMP 3---are well established and documented. However, with the increase in parallel, many-core, and heterogeneous systems, a number of research-driven projects have developed more diversified task-based support, employing various programming and runtime features. Unfortunately, despite the fact that dozens of different task-based systems exist today and are actively used for parallel and high-performance computing (HPC), no comprehensive overview or classification of task-based technologies for HPC exists. In this paper, we provide an initial task-focused taxonomy for HPC technologies, which covers both programming interfaces and runtime mechanisms. We demonstrate the usefulness of our taxonomy by classifying state-of-the-art task-based environments in use today.",
    issn="1573-0484",
    doi="10.1007/s11227-018-2238-4",
%    url="https://doi.org/10.1007/s11227-018-2238-4"
}
@article{cwl,
    author = {Crusoe, Michael and Abeln, Sanne and Iosup, Alexandru and Amstutz, Peter and Chilton, John and Tijanić, Nebojša and Ménager, Hervé and Soiland-Reyes, Stian and Gavrilović, Bogdan and Goble, Carole and Community, The},
    year = {2021},
    month = {08},
    pages = {},
    title = {Methods Included: Standardizing Computational Reuse and Portability with the Common Workflow Language},
    volume = {65},
    journal = {Communications of the ACM},
    doi = {10.1145/3486897}
}
@Article{large_scale_modelling,
    author="Lampa, Samuel
and Alvarsson, Jonathan
and Spjuth, Ola",
    title="Towards agile large-scale predictive modelling in drug discovery with flow-based programming design principles",
    journal="Journal of Cheminformatics",
    year="2016",
    month="Nov",
    day="24",
    volume="8",
    number="1",
    pages="67",
%    abstract="Predictive modelling in drug discovery is challenging to automate as it often contains multiple analysis steps and might involve cross-validation and parameter tuning that create complex dependencies between tasks. With large-scale data or when using computationally demanding modelling methods, e-infrastructures such as high-performance or cloud computing are required, adding to the existing challenges of fault-tolerant automation. Workflow management systems can aid in many of these challenges, but the currently available systems are lacking in the functionality needed to enable agile and flexible predictive modelling. We here present an approach inspired by elements of the flow-based programming paradigm, implemented as an extension of the Luigi system which we name SciLuigi. We also discuss the experiences from using the approach when modelling a large set of biochemical interactions using a shared computer cluster.Graphical abstract.",
    issn="1758-2946",
    doi="10.1186/s13321-016-0179-6",
%    url="https://doi.org/10.1186/s13321-016-0179-6"
}
@book{bertsekas_1992,
    address = {Upper Saddle River, NJ, USA},
    title = {Data {Networks} (2Nd {Ed}.)},
    isbn = {978-0-13-200916-4},
    publisher = {Prentice-Hall, Inc.},
    author = {Bertsekas, Dimitri and Gallager, Robert},
    year = {1992},
    doi = {10.5555/121104}
}
@INPROCEEDINGS{pegasusgraphs,
    author={Silva, Rafael Ferreira da and Chen, Weiwei and Juve, Gideon and Vahi, Karan and
	Deelman, Ewa},
    booktitle={2014 IEEE 10th International Conference on e-Science},
    title={Community Resources for Enabling Research in Distributed Scientific Workflows},
    year={2014},
    volume={1},
    number={},
    pages={177-184},
    doi={10.1109/eScience.2014.44}
}
@online{it4i,
    title = {IT4Innovations supercomputing center},
    url = {https://docs.it4i.cz/},
    urldate = {2024-01-19}
}
@article{taskbench,
    title={Task {B}ench: {A} Parameterized Benchmark for Evaluating Parallel
    Runtime
    Performance},
    author={Slaughter, Elliott and Wu, Wei and Fu, Yuankun and Brandenburg,
    Legend and Garcia, Nicolai and Kautz, Wilhem and Marx, Emily and Morris,
    Kaleb S and Lee, Wonchan and Cao, Qinglei and others},
    journal={arXiv preprint arXiv:1908.05790},
    year={2019}
}
@techreport{pep703,
    author  = {Sam Gross},
    title   = {Making the Global Interpreter Lock Optional in CPython},
    year    = {2023},
    type    = {PEP},
    number  = {703},
    url     = {https://peps.python.org/pep-0703/},
}
@online{wordbatcharticle,
    title = {Benchmarking Python Distributed {AI} Backends with Wordbatch},
    url = {https://towardsdatascience.com/benchmarking-python-distributed-ai-backends-with-wordbatch-9872457b785c},
    urldate = {2020-04-20}
}
@article{spmd,
    author = {F. Darema and D.A. George and V.A. Norton and G.F. Pfister},
    title = {A single-program-multiple-data computational model for EPEX/FORTRAN},
    journal = {Parallel Computing},
    volume = {7},
    number = {1},
    pages = {11-24},
    year = {1988},
    issn = {0167-8191},
    doi = {https://doi.org/10.1016/0167-8191(88)90094-4},
%    url = {https://www.sciencedirect.com/science/article/pii/0167819188900944},
%    keywords = {Shared memory multiprocessor, EPEX/FORTRAN, computational model, parallelization features},
%    abstract = {We present a single-program-multiple-data computational model which we have implemented in the EPEX system to run in parallel mode FORTRAN scientific application programs. The computational model assumes a shared memory organization and is based on the scheme that all processes executing a program in parallel remain in existence for the entire execution; however, the tasks to be executed by each process are determined dynamically during execution by the use of appropriate synchronizing constructs that are imbedded in the program. We have demonstrated the applicability of the model in the parallelization of several applications. We discuss parallelization features of these applications and performance issues such as overhead, speedup, efficiency.}
}
@INPROCEEDINGS{hybrid_openmp_mpi,
    author={Rabenseifner, Rolf and Hager, Georg and Jost, Gabriele},
    booktitle={2009 17th Euromicro International Conference on Parallel, Distributed and Network-based Processing},
    title={Hybrid MPI/OpenMP Parallel Programming on Clusters of Multi-Core SMP Nodes},
    year={2009},
    volume={},
    number={},
    pages={427-436},
%    keywords={Parallel programming;Yarn;High performance computing;Concurrent computing;Distributed computing;Hardware;Computer networks;Message passing;Topology;Taxonomy;Hybrid programming;MPI;OpenMP;multi-core;SMP},
    doi={10.1109/PDP.2009.43}
}
@report{memory_safety_report,
    author = {National Cybersecurity Strategy},
    title = {Back to the Building Blocks: a Path Towards Secure and Measurable Software},
    institution = {National Cybersecurity Strategy},
    url = {https://www.whitehouse.gov/wp-content/uploads/2024/02/Final-ONCD-Technical-Report.pdf},
    urldate = {2024-03-24},
    year = {2024},
}
@article{fault_tolerant_mpi,
    title={Fault tolerance in message passing interface programs},
    author={Gropp, William and Lusk, Ewing},
    journal={The International Journal of High Performance Computing Applications},
    volume={18},
    number={3},
    pages={363--372},
    year={2004},
    publisher={Sage Publications Sage CA: Thousand Oaks, CA}
}
@article{horovod,
    title={Horovod: fast and easy distributed deep learning in TensorFlow},
    author={Alexander Sergeev and Mike Del Balso},
    journal={ArXiv},
    year={2018},
    volume={abs/1802.05799},
    url={https://api.semanticscholar.org/CorpusID:3398835}
}
@Book{cpp11_standard,
    author =       "{ISO}",
    title =        "{ISO\slash IEC 14882:2011 Information technology ---
                 Programming languages --- C++}",
    publisher =    pub-ISO,
    address =      pub-ISO:adr,
    edition =      "Third",
%    pages =        "????",
    day =          "1",
    month =        sep,
    year =         "2011",
%    ISBN =         "????",
%    ISBN-13 =      "????",
%    LCCN =         "????",
%    bibdate =      "Mon Dec 19 11:12:12 2011",
%    bibsource =    "https://www.math.utah.edu/pub/tex/bib/isostd.bib;
%                 https://www.math.utah.edu/pub/tex/bib/mathcw.bib",
    URL =          "http://www.iso.org/iso/iso_catalogue/catalogue_tc/catalogue_detail.htm?csnumber=50372",
%    acknowledgement = ack-nhfb,
%    remark =       "Revises ISO/IEC 14882:2003.",
}
@manual{intel_developer_manual,
    added-at = {2008-09-15T08:33:44.000+0200},
%    author = {?},
%    biburl = {https://www.bibsonomy.org/bibtex/2d1fd8c578b32c046dc1e7de8410665b5/rekstorm},
%    interhash = {eee5c9ed0fff42f3a37bc7bd3375dc39},
%    intrahash = {d1fd8c578b32c046dc1e7de8410665b5},
    keywords = {Architectures Manual},
    month = {August},
    organization = {Intel Corporation},
    timestamp = {2008-09-15T08:52:53.000+0200},
    title = {Intel 64 and IA-32 Architectures Software Developer's Manual - Volume 3B},
    year = 2007
}
@software{hadoop,
    author = {{Apache Software Foundation}},
    title = {Hadoop},
    url = {https://hadoop.apache.org},
    version = {0.20.2},
    date = {2010-02-19},
}
@article{spark,
    author = {Zaharia, Matei and Xin, Reynold S. and Wendell, Patrick and Das, Tathagata and Armbrust, Michael and Dave, Ankur and Meng, Xiangrui and Rosen, Josh and Venkataraman, Shivaram and Franklin, Michael J. and Ghodsi, Ali and Gonzalez, Joseph and Shenker, Scott and Stoica, Ion},
    title = {Apache Spark: a unified engine for big data processing},
    year = {2016},
    issue_date = {November 2016},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {59},
    number = {11},
    issn = {0001-0782},
%    url = {https://doi.org/10.1145/2934664},
    doi = {10.1145/2934664},
%    abstract = {This open source computing framework unifies streaming, batch, and interactive big data workloads to unlock new applications.},
    journal = {Commun. ACM},
    month = {oct},
    pages = {56–65},
    numpages = {10}
}
@techreport{mpi,
    author = {Forum, Message P},
    title = {MPI: A Message-Passing Interface Standard},
    year = {1994},
    publisher = {University of Tennessee},
    address = {USA},
%    abstract = {The Message Passing Interface Forum (MPIF), with participation from over 40 organizations, has been meeting since November 1992 to discuss and define a set of library standards for message passing. MPIF is not sanctioned or supported by any official standards organization. The goal of the Message Passing Interface, simply stated, is to develop a widely used standard for writing message-passing programs. As such the interface should establish a practical, portable, efficient and flexible standard for message passing. , This is the final report, Version 1.0, of the Message Passing Interface Forum. This document contains all the technical features proposed for the interface. This copy of the draft was processed by LATEX on April 21, 1994. , Please send comments on MPI to mpi-comments@cs.utk.edu. Your comment will be forwarded to MPIF committee members who will attempt to respond.}
}
@Inbook{pgas,
    author="Almasi, George",
%    editor="Padua, David",
    title="PGAS (Partitioned Global Address Space) Languages",
    bookTitle="Encyclopedia of Parallel Computing",
    year="2011",
    publisher="Springer US",
    address="Boston, MA",
    pages="1539--1545",
    isbn="978-0-387-09766-4",
    doi="10.1007/978-0-387-09766-4_210",
%    url="https://doi.org/10.1007/978-0-387-09766-4_210"
}
@book{openmp,
    title={Parallel programming in OpenMP},
    author={Chandra, Rohit and Dagum, Leo and Kohr, David and Menon, Ramesh and Maydan, Dror and McDonald, Jeff},
    year={2001},
    publisher={Morgan kaufmann}
}
@article{cuda,
    author = {Nickolls, John and Buck, Ian and Garland, Michael and Skadron, Kevin},
    title = {Scalable Parallel Programming with CUDA.},
    journal = {ACM Queue},
    doi = {10.1145/1365490.1365500},
%    keywords = {dblp},
    number = 2,
    pages = {40-53},
    timestamp = {2018-11-07T11:49:23.000+0100},
%    url = {http://dblp.uni-trier.de/db/journals/queue/queue6.html#NickollsBGS08},
    volume = 6,
    year = 2008
}
@misc{tensorflow,
    title={ {TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},
    url={https://www.tensorflow.org/},
    note={Software available from tensorflow.org},
    author={
    Mart\'{i}n~Abadi and
    Ashish~Agarwal and
    Paul~Barham and
    Eugene~Brevdo and
    Zhifeng~Chen and
    Craig~Citro and
    Greg~S.~Corrado and
    Andy~Davis and
    Jeffrey~Dean and
    Matthieu~Devin and
    Sanjay~Ghemawat and
    Ian~Goodfellow and
    Andrew~Harp and
    Geoffrey~Irving and
    Michael~Isard and
    Yangqing Jia and
    Rafal~Jozefowicz and
    Lukasz~Kaiser and
    Manjunath~Kudlur and
    Josh~Levenberg and
    Dandelion~Man\'{e} and
    Rajat~Monga and
    Sherry~Moore and
    Derek~Murray and
    Chris~Olah and
    Mike~Schuster and
    Jonathon~Shlens and
    Benoit~Steiner and
    Ilya~Sutskever and
    Kunal~Talwar and
    Paul~Tucker and
    Vincent~Vanhoucke and
    Vijay~Vasudevan and
    Fernanda~Vi\'{e}gas and
    Oriol~Vinyals and
    Pete~Warden and
    Martin~Wattenberg and
    Martin~Wicke and
    Yuan~Yu and
    Xiaoqiang~Zheng},
    year={2015},
}
@inproceedings{cost,
    author = {McSherry, Frank and Isard, Michael and Murray, Derek G.},
    title = {Scalability! but at what cost?},
    year = {2015},
    publisher = {USENIX Association},
    address = {USA},
%    abstract = {We offer a new metric for big data platforms, COST, or the Configuration that Outperforms a Single Thread. The COST of a given platform for a given problem is the hardware configuration required before the platform outperforms a competent single-threaded implementation. COST weighs a system's scalability against the overheads introduced by the system, and indicates the actual performance gains of the system, without rewarding systems that bring substantial but parallelizable overheads.We survey measurements of data-parallel systems recently reported in SOSP and OSDI, and find that many systems have either a surprisingly large COST, often hundreds of cores, or simply underperform one thread for all of their reported configurations.},
    booktitle = {Proceedings of the 15th USENIX Conference on Hot Topics in Operating Systems},
    pages = {14},
    numpages = {1},
    location = {Switzerland},
    series = {HOTOS'15}
}
@INPROCEEDINGS{mpiusagestudy1,
    author={Chunduri, Sudheer and Parker, Scott and Balaji, Pavan and Harms, Kevin and Kumaran, Kalyan},
    booktitle={SC18: International Conference for High Performance Computing, Networking, Storage and Analysis},
    title={Characterization of MPI Usage on a Production Supercomputer},
    year={2018},
    volume={},
    number={},
    pages={386-400},
%    keywords={Tools;Hardware;Supercomputers;Libraries;Control systems;Production systems;MPI;monitoring;Autoperf;core-hours},
    doi={10.1109/SC.2018.00033}
}
@article{mpiusagestudy2,
    author = {Bernholdt, David E. and Boehm, Swen and Bosilca, George and Gorentla Venkata, Manjunath and Grant, Ryan E. and Naughton, Thomas and Pritchard, Howard P. and Schulz, Martin and Vallee, Geoffroy R.},
    title = {A survey of MPI usage in the US exascale computing project},
    journal = {Concurrency and Computation: Practice and Experience},
    volume = {32},
    number = {3},
    pages = {e4851},
    keywords = {exascale, MPI},
    doi = {https://doi.org/10.1002/cpe.4851},
%    url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cpe.4851},
%    eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/cpe.4851},
    note = {e4851 cpe.4851},
%    abstract = {Summary The Exascale Computing Project (ECP) is currently the primary effort in the United States focused on developing “exascale” levels of computing capabilities, including hardware, software, and applications. In order to obtain a more thorough understanding of how the software projects under the ECP are using, and planning to use the Message Passing Interface (MPI), and help guide the work of our own project within the ECP, we created a survey. Of the 97 ECP projects active at the time the survey was distributed, we received 77 responses, 56 of which reported that their projects were using MPI. This paper reports the results of that survey for the benefit of the broader community of MPI developers.},
    year = {2020}
}
@inproceedings{mpiusagestudy3,
    author = {Laguna, Ignacio and Marshall, Ryan and Mohror, Kathryn and Ruefenacht, Martin and Skjellum, Anthony and Sultana, Nawrin},
    title = {A large-scale study of MPI usage in open-source HPC applications},
    year = {2019},
    isbn = {9781450362290},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
%    url = {https://doi.org/10.1145/3295500.3356176},
    doi = {10.1145/3295500.3356176},
%    abstract = {Understanding the state-of-the-practice in MPI usage is paramount for many aspects of supercomputing, including optimizing the communication of HPC applications and informing standardization bodies and HPC systems procurements regarding the most important MPI features. Unfortunately, no previous study has characterized the use of MPI on applications at a significant scale; previous surveys focus either on small data samples or on MPI jobs of specific HPC centers. This paper presents the first comprehensive study of MPI usage in applications. We survey more than one hundred distinct MPI programs covering a significantly large space of the population of MPI applications. We focus on understanding the characteristics of MPI usage with respect to the most used features, code complexity, and programming models and languages. Our study corroborates certain findings previously reported on smaller data samples and presents a number of interesting, previously un-reported insights.},
    booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
    articleno = {31},
    numpages = {14},
%    keywords = {MPI, applications survey, program analysis},
    location = {Denver, Colorado},
    series = {SC '19}
}
@InProceedings{task_checkpointing,
    author="Verg{\'e}s, Pere
and Lordan, Francesc
and Ejarque, Jorge
and Badia, Rosa M.",
%    editor="Singer, Jeremy
%and Elkhatib, Yehia
%and Blanco Heras, Dora
%and Diehl, Patrick
%and Brown, Nick
%and Ilic, Aleksandar",
    title="Task-Level Checkpointing System for Task-Based Parallel Workflows",
    booktitle="Euro-Par 2022: Parallel Processing Workshops",
    year="2023",
    publisher="Springer Nature Switzerland",
    address="Cham",
    pages="251--262",
%    abstract="Scientific applications are large and complex; task-based programming models are a popular approach to developing these applications due to their ease of programming and ability to handle complex workflows and distribute their workload across large infrastructures. In these environments, either the hardware or the software may lead to failures from a myriad of origins: application logic, system software, memory, network, or disk. Re-executing a failed application can take hours, days, or even weeks, thus, dragging out the research. This article proposes a recovery system for dynamic task-based models to reduce the re-execution time of failed runs. The design encapsulates in a checkpointing manager the automatic checkpointing of the execution, leveraging different mechanisms that can be arbitrarily defined and tuned to fit the needs of each performance. Additionally, it offers an API call to establish snapshots of the execution from the application code. The experiments executed on a prototype implementation have reached a speedup of 1.9{\$}{\$}{\backslash}times {\$}{\$}{\texttimes}after re-execution and shown no overhead on the execution time on successful first runs of specific applications.",
    isbn="978-3-031-31209-0"
}
@article{dataflow,
    title = {The Dataflow Model: A Practical Approach to Balancing Correctness, Latency, and Cost in Massive-Scale, Unbounded, Out-of-Order Data Processing},
    author = {Tyler Akidau and Robert Bradshaw and Craig Chambers and Slava Chernyak and Rafael J. Fernández-Moctezuma and Reuven Lax and Sam McVeety and Daniel Mills and Frances Perry and Eric Schmidt and Sam Whittle},
    year = 2015,
    journal = {Proceedings of the VLDB Endowment},
    volume = 8,
    pages = {1792--1803}
}
@INPROCEEDINGS{stencil,
    author={Maruyama, Naoya and Sato, Kento and Nomura, Tatsuo and Matsuoka, Satoshi},
    booktitle={SC '11: Proceedings of 2011 International Conference for High Performance Computing, Networking, Storage and Analysis},
    title={Physis: An implicitly parallel programming model for stencil computations on large-scale GPU-accelerated supercomputers},
    year={2011},
    volume={},
    number={},
    pages={1-12},
%    keywords={Programming;Computational modeling;DSL;Graphics processing unit;Runtime;Optimization;Indexes;Domain Specific Languages;Application Framework;High Perforamnce Computing},
    doi={10.1145/2063384.2063398}
}
@Inbook{parallel_haskell,
    author="Hammond, Kevin",
    editor="Padua, David",
    title="Glasgow Parallel Haskell (GpH)",
    bookTitle="Encyclopedia of Parallel Computing",
    year="2011",
    publisher="Springer US",
    address="Boston, MA",
    pages="768--779",
    isbn="978-0-387-09766-4",
    doi="10.1007/978-0-387-09766-4_46",
%    url="https://doi.org/10.1007/978-0-387-09766-4_46"
}
@INPROCEEDINGS{workflows1,
    author={Do, Tu Mai Anh and Pottier, Loïc and Yildiz, Orcun and Vahi, Karan and Krawczuk, Patrycja and Peterka, Tom and Deelman, Ewa},
    booktitle={2022 22nd IEEE International Symposium on Cluster, Cloud and Internet Computing (CCGrid)},
    title={Accelerating Scientific Workflows on HPC Platforms with In Situ Processing},
    year={2022},
    volume={},
    number={},
    pages={1-10},
%    keywords={Cloud computing;Scientific computing;High performance computing;Genomics;Data transfer;Bioinformatics;Task analysis;Scientific Workflows;Workflow Management Systems;In situ;Pegasus;Decaf},
    doi={10.1109/CCGrid54584.2022.00009}
}
@INPROCEEDINGS{bulkparallel1,
    author={Cheatham, T. and Fahmy, A. and Stefanescu, D.C. and Valiant, L.G.},
    booktitle={Proceedings of the Twenty-Eighth Annual Hawaii International Conference on System Sciences},
    title={Bulk synchronous parallel computing-a paradigm for transportable software},
    year={1995},
    volume={2},
    number={},
    pages={268-275 vol.2},
%    keywords={Concurrent computing;Software performance;Computer architecture;Yarn;Throughput;Parallel processing;Computer industry;Contracts;Laboratories;Reactive power},
    doi={10.1109/HICSS.1995.375451}
}
@InProceedings{bulkparallel2,
    author="Gerbessiotis, Alexandras V.
and Valiant, Leslie G.",
    editor="Nurmi, Otto
and Ukkonen, Esko",
    title="Direct bulk-synchronous parallel algorithms",
    booktitle="Algorithm Theory --- SWAT '92",
    year="1992",
    publisher="Springer Berlin Heidelberg",
    address="Berlin, Heidelberg",
    pages="1--18",
%    abstract="We describe a methodology for constructing parallel algorithms that are transportable among parallel computers having different numbers of processors, different bandwidths of interprocessor communication and different periodicity of global synchronisation. We do this for the bulk-synchronous parallel (BSP) model, which abstracts the characteristics of a parallel machine into three numerical parameters p, g, and L, corresponding to processors, bandwidth, and periodicity respectively. The model differentiates memory that is local to a processor from that which is not, but, for the sake of universality, does not differentiate network proximity. The advantages of this model in supporting shared memory or PRAM style programming have been treated elsewhere. Here we emphasise the viability of an alternative direct style of programming where, for the sake of efficiency the programmer retains control of memory allocation. We show that optimality to within a multiplicative factor close to one can be achieved for the problems of Gauss-Jordan elimination and sorting, by transportable algorithms that can be applied for a wide range of values of the parameters p, g, and L. We also give some simulation results for PRAMs on the BSP to identify the level of slack at which corresponding efficiencies can be approached by shared memory simulations, provided the bandwidth parameter g is good enough.",
    isbn="978-3-540-47275-9"
}
@article{split_apply_combine,
    author = {Hadley, Wickham},
    year = {2011},
    month = {04},
    pages = {},
    title = {The Split-Apply-Combine Strategy for Data Analysis},
    volume = {40},
    journal = {Journal of Statistical Software},
    doi = {10.18637/jss.v040.i01}
}
@article{cilk,
    title = {Cilk: An Efficient Multithreaded Runtime System},
    journal = {Journal of Parallel and Distributed Computing},
    volume = {37},
    number = {1},
    pages = {55-69},
    year = {1996},
    issn = {0743-7315},
    doi = {https://doi.org/10.1006/jpdc.1996.0107},
%    url = {https://www.sciencedirect.com/science/article/pii/S0743731596901070},
    author = {Robert D. Blumofe and Christopher F. Joerg and Bradley C. Kuszmaul and Charles E. Leiserson and Keith H. Randall and Yuli Zhou},
%    abstract = {Cilk (pronounced “silk”) is a C-based runtime system for multithreaded parallel programming. In this paper, we document the efficiency of the Cilk work-stealing scheduler, both empirically and analytically. We show that on real and synthetic applications, the “work” and “critical-path length” of a Cilk computation can be used to model performance accurately. Consequently, a Cilk programmer can focus on reducing the computation's work and critical-path length, insulated from load balancing and other runtime scheduling issues. We also prove that for the class of “fully strict” (well-structured) programs, the Cilk scheduler achieves space, time, and communication bounds all within a constant factor of optimal. The Cilk runtime system currently runs on the Connection Machine CM5 MPP, the Intel Paragon MPP, the Sun Sparcstation SMP, and the Cilk-NOW network of workstations. Applications written in Cilk include protein folding, graphic rendering, backtrack search, and the ★Socrates chess program, which won second prize in the 1995 ICCA World Computer Chess Championship.}
}
@inproceedings{qr_factorization,
    author = {Miletto, Marcelo and Schnorr, Lucas},
    year = {2019},
    month = {11},
    pages = {25-36},
    title = {OpenMP and StarPU Abreast: the Impact of Runtime in Task-Based Block QR Factorization Performance},
    doi = {10.5753/wscad.2019.8654}
}
@article{tbb,
    author = {Pheatt, Chuck},
    title = {Intel® threading building blocks},
    year = {2008},
    issue_date = {April 2008},
    publisher = {Consortium for Computing Sciences in Colleges},
    address = {Evansville, IN, USA},
    volume = {23},
    number = {4},
    issn = {1937-4771},
%    abstract = {Intel® Threading Building Blocks [1] is a C++ runtime library that abstracts the low-level threading details necessary for effectively utilizing multi-core processors. It uses C++ templates to eliminate the need to create and manage threads. Applications tend to be more portable since parallelism is achieved through library calls and utilization of a task manager for scheduling. The task manager analyzes the system the software is running on, chooses the optimal number of threads, and performs load balancing that spreads out the work evenly across all processor cores. The library consists of data structures and algorithms that simplify parallel programming in C++ by avoiding requiring a programmer to use native threading packages such as POSIX threads or Windows threads, or even the portable Boost Threads.},
    journal = {J. Comput. Sci. Coll.},
    month = {apr},
    pages = {298},
    numpages = {1}
}
@book{gof,
    asin = {0201633612},
    author = {Gamma, Erich and Helm, Richard and Johnson, Ralph and Vlissides, John M.},
%    biburl = {https://www.bibsonomy.org/bibtex/27e3f1154ab1fbce54752a46dba7f2217/pnk},
%    description = {Amazon.com: Design Patterns: Elements of Reusable Object-Oriented Software (9780201633610): Erich Gamma, Richard Helm, Ralph Johnson, John M. Vlissides: Books},
    dewey = {005.12},
    ean = {9780201633610},
    edition = 1,
%    interhash = {7fe32957be97afaf4ecb38b5490d23b4},
%    intrahash = {7e3f1154ab1fbce54752a46dba7f2217},
    isbn = {0201633612},
%    keywords = {DBIS Design Object-Oriented Patterns SS2010 Seminar Software},
    publisher = {Addison-Wesley Professional},
    timestamp = {2010-06-05T16:40:25.000+0200},
    title = {Design Patterns: Elements of Reusable Object-Oriented Software},
%    url = {http://www.amazon.com/Design-Patterns-Elements-Reusable-Object-Oriented/dp/0201633612/ref=ntt_at_ep_dpi_1},
    year = 1994
}
@article{hpc_tasks,
    title={Enabling Dynamic and Intelligent Workflows for HPC, Data Analytics, and AI Convergence},
    author={Jorge Ejarque and Rosa M. Badia and Loic Albertin and Giovanni Aloisio and Enrico Baglione and Yolanda Becerra and Stefan Boschert and J. R. Berlin and Alessandro D’Anca and Donatello Elia and Franccois Exertier and Sandro Fiore and Jos{\'e} Flich and Arnau Folch and Steven J. Gibbons and Nikolay Koldunov and Francesc Lordan and Stefano Lorito and Finn L{\o}vholt and Jorge Mac'ias and Fabrizio Marozzo and Alberto Michelini and Marisol Monterrubio-Velasco and Marta Pienkowska and Josep de la Puente and Anna Queralt and Enrique S. Quintana-Ort'i and Juan E. Rodr'iguez and Fabrizio Romano and Riccardo Rossi and Jedrzej Rybicki and Miroslaw Kupczyk and Jacopo Selva and Domenico Talia and Roberto Tonini and Paolo Trunfio and Manuela Volp},
    journal={ArXiv},
    year={2022},
    volume={abs/2204.09287},
    url={https://api.semanticscholar.org/CorpusID:248266377}
}
@article{hpc_tasks_2,
    title={Task-parallel Analysis of Molecular Dynamics Trajectories},
    author={Ioannis Paraskevakos and Andr{\'e} Luckow and George Chantzialexiou and Mahzad Khoshlessan and Oliver Beckstein and Geoffrey Charles Fox and Shantenu Jha},
    journal={Proceedings of the 47th International Conference on Parallel Processing},
    year={2018},
    url={https://api.semanticscholar.org/CorpusID:4756113}
}
@inproceedings{hpc_tasks_3,
    author = {Ogasawara, Eduardo and de Oliveira, Daniel and Chirigati, Fernando and Barbosa, Carlos Eduardo and Elias, Renato and Braganholo, Vanessa and Coutinho, Alvaro and Mattoso, Marta},
    title = {Exploring many task computing in scientific workflows},
    year = {2009},
    isbn = {9781605587141},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
%    url = {https://doi.org/10.1145/1646468.1646470},
    doi = {10.1145/1646468.1646470},
%    abstract = {One of the main advantages of using a scientific workflow management system (SWfMS) to orchestrate data flows among scientific activities is to control and register the whole workflow execution. The execution of activities within a workflow with high performance computing (HPC) presents challenges in SWfMS execution control. Current solutions leave the scheduling to the HPC queue system. Since the workflow execution engine does not run on remote clusters, SWfMS are not aware of the parallel strategy of the workflow execution. Consequently, remote execution control and provenance registry of the parallel activities is very limited from the SWfMS side. This work presents a set of components to be included on the workflow specification of any SWMfS to control parallelization of activities as MTC. In addition, these components can gather provenance data during remote workflow execution. Through these MTC components, the parallelization strategy can be registered and reused, and provenance data can be uniformly queried. We have evaluated our approach by performing parameter sweep parallelization in solving the incompressible 3D Navier-Stokes equations. Experimental results show the performance gains with the additional benefits of distributed provenance support.},
    booktitle = {Proceedings of the 2nd Workshop on Many-Task Computing on Grids and Supercomputers},
    articleno = {2},
    numpages = {10},
%    keywords = {computational fluid dynamics, parallelization, provenance, scientific workflows},
    location = {Portland, Oregon},
    series = {MTAGS '09}
}
@online{salomon,
    title = {Salomon supercomputer},
    url = {https://docs.it4i.cz/salomon/introduction},
    urldate = {2024-07-21},
}
@online{barbora,
    title = {Barbora supercomputer},
    url = {https://docs.it4i.cz/barbora/introduction},
    urldate = {2024-07-21},
}
@online{karolina,
    title = {Karolina supercomputer},
    url = {https://docs.it4i.cz/karolina/introduction/},
    urldate = {2024-06-01},
}
@online{leonardo_time_limit,
    title = {CPU time limit on login nodes of the LEONARDO cluster},
    url = {https://wiki.u-gov.it/confluence/display/SCAIUS/UG3.2.2%3A+LEONARDO+DCGP+UserGuide},
    urldate = {2024-06-02},
}
@online{karolina_scratch,
    title = {Karolina SCRATCH filesystem},
    url = {https://docs.it4i.cz/karolina/storage/#scratch-file-system},
    urldate = {2024-07-16},
}
@online{karolina_project,
    title = {Karolina PROJECT filesystem},
    url = {https://docs.it4i.cz/storage/project-storage/},
    urldate = {2024-07-16},
}
@online{lumi,
    title = {LUMI supercomputer},
    url = {https://www.lumi-supercomputer.eu/},
    urldate = {2024-07-19}
}
@article{leonardo,
    author = {Turisini, M. and Cestari, Mirko and Amati, Giorgio},
    year = {2024},
    month = {01},
    pages = {},
    title = {LEONARDO: A Pan-European Pre-Exascale Supercomputer for HPC and AI applications},
    volume = {9},
    journal = {Journal of large-scale research facilities JLSRF},
    doi = {10.17815/jlsrf-8-186}
}
@online{e4,
    title = {E4 Computer Engineering cluster},
    url = {https://www.e4company.com/en/high-performance-computing/},
    urldate = {2024-07-19},
}
@online{galileo,
    title = {Galileo 100 supercomputer},
    url = {https://www.hpc.cineca.it/systems/hardware/galileo/},
    urldate = {2024-07-19}
}
@online{meluxina,
    title = {MeluXina supercomputer},
    url = {https://www.luxprovide.lu/meluxina/},
    urldate = {2024-07-19},
}
@article{pdbbind,
    author = {Su, Minyi and Yang, Qifan and Du, Yu and Feng, Guoqin and Liu, Zhihai and Li, Yan and Wang, Renxiao},
    title = {Comparative Assessment of Scoring Functions: The CASF-2016 Update},
    journal = {Journal of Chemical Information and Modeling},
    volume = {59},
    number = {2},
    pages = {895-913},
    year = {2019},
    doi = {10.1021/acs.jcim.8b00545},
%    URL = {https://doi.org/10.1021/acs.jcim.8b00545},
%    eprint = {https://doi.org/10.1021/acs.jcim.8b00545}
}
@article{smiles,
    author = {Weininger, David},
    title = {SMILES, a chemical language and information system. 1. Introduction to methodology and encoding rules},
    journal = {Journal of Chemical Information and Computer Sciences},
    volume = {28},
    number = {1},
    pages = {31-36},
    year = {1988},
    doi = {10.1021/ci00057a005},
%    URL = {https://doi.org/10.1021/ci00057a005},
%    eprint = {https://doi.org/10.1021/ci00057a005}
}
@online{mol2,
    title = {Tripos Mol2 File Format},
    url = {https://zhanggroup.org/DockRMSD/mol2.pdf},
    urldate = {2024-07-20},
}
@INBOOK{infiniband,
    author={Buyya, Rajkumar and Cortes, Toni and Jin, Hai},
    booktitle={High Performance Mass Storage and Parallel I/O: Technologies and Applications},
    title={An Introduction to the InfiniBand Architecture},
    year={2002},
    volume={},
    number={},
    pages={616-632},
    doi={10.1109/9780470544839.ch42}
}
@InProceedings{pandas,
    author    = { {W}es {M}c{K}inney },
    title     = { {D}ata {S}tructures for {S}tatistical {C}omputing in {P}ython },
    booktitle = { {P}roceedings of the 9th {P}ython in {S}cience {C}onference },
    pages     = { 56 - 61 },
    year      = { 2010 },
    editor    = { {S}t\'efan van der {W}alt and {J}arrod {M}illman },
    doi       = { 10.25080/Majora-92bf1922-00a }
}
@Article{numpy,
    title         = {Array programming with {NumPy}},
    author        = {Charles R. Harris and K. Jarrod Millman and St{\'{e}}fan J.
                 van der Walt and Ralf Gommers and Pauli Virtanen and David
                 Cournapeau and Eric Wieser and Julian Taylor and Sebastian
                 Berg and Nathaniel J. Smith and Robert Kern and Matti Picus
                 and Stephan Hoyer and Marten H. van Kerkwijk and Matthew
                 Brett and Allan Haldane and Jaime Fern{\'{a}}ndez del
                 R{\'{i}}o and Mark Wiebe and Pearu Peterson and Pierre
                 G{\'{e}}rard-Marchant and Kevin Sheppard and Tyler Reddy and
                 Warren Weckesser and Hameer Abbasi and Christoph Gohlke and
                 Travis E. Oliphant},
    year          = {2020},
    month         = sep,
    journal       = {Nature},
    volume        = {585},
    number        = {7825},
    pages         = {357--362},
    doi           = {10.1038/s41586-020-2649-2},
    publisher     = {Springer Science and Business Media {LLC}},
%    url           = {https://doi.org/10.1038/s41586-020-2649-2}
}
@article{scikit-learn,
    title={Scikit-learn: Machine Learning in {P}ython},
    author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
          and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
          and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
          Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
    journal={Journal of Machine Learning Research},
    volume={12},
    pages={2825--2830},
    year={2011}
}

% Allocation managers
@InProceedings{slurm,
    author = "Yoo, Andy B.
and Jette, Morris A.
and Grondona, Mark",
    doi = "10.1007/10968987_3",
%    editor = "Feitelson, Dror
%and Rudolph, Larry
%and Schwiegelshohn, Uwe",
    title = "SLURM: Simple Linux Utility for Resource Management",
    booktitle = "Job Scheduling Strategies for Parallel Processing",
    year = "2003",
    publisher = "Springer Berlin Heidelberg",
    address = "Berlin, Heidelberg",
    pages = "44--60",
%    abstract = "A new cluster resource management system called Simple Linux Utility Resource Management (SLURM) is described in this paper. SLURM, initially developed for large Linux clusters at the Lawrence Livermore National Laboratory (LLNL), is a simple cluster manager that can scale to thousands of processors. SLURM is designed to be flexible and fault-tolerant and can be ported to other clusters of different size and architecture with minimal effort. We are certain that SLURM will benefit both users and system architects by providing them with a simple, robust, and highly scalable parallel job execution environment for their cluster system.",
    isbn = "978-3-540-39727-4"
}
@online{slurm-schedmd,
    title = {SchedMD Slurm usage statistics},
    url = {https://schedmd.com/},
    urldate = {2022-05-18}
}
@online{slurm-throughput,
    title = {Slurm High Throughput Computing},
    url = {https://www.schedmd.com/slurm-support/our-services/high-performance-computing},
    urldate = {2024-04-22}
}
@inproceedings{slurm-workflow,
    author = {Rodrigo, Gonzalo P. and Elmroth, Erik and \"{O}stberg, Per-Olov and Ramakrishnan, Lavanya},
    title = {Enabling Workflow-Aware Scheduling on HPC Systems},
    year = {2017},
    isbn = {9781450346993},
%    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
%    url = {https://doi.org/10.1145/3078597.3078604},
    doi = {10.1145/3078597.3078604},
%    abstract = {Scientific workflows are increasingly common in the workloads of current High Performance Computing (HPC) systems. However, HPC schedulers do not incorporate workflow-specific mechanisms beyond the capacity to declare dependencies between their jobs. Thus, workflows are run as sets of batch jobs with dependencies, which induces long intermediate wait times and, consequently, long workflow turnaround times. Alternatively, to reduce their turnaround time, workflows may be submitted as single pilot jobs that are allocated their maximum required resources for their entire runtime. Pilot jobs achieve shorter turnaround times but reduce the HPC system's utilization because resources may idle during the workflow's execution. We present a workflow-aware scheduling (WoAS) system that enables existing scheduling algorithms to exploit fine-grained information on a workflow's resource requirements and structure without modification. The current implementation of WoAS is integrated into Slurm, a widely used HPC batch scheduler. We evaluate the system using a simulator using real and synthetic workflows and a synthetic baseline workload that captures job patterns observed over three years of workload data from Edison, a large supercomputer hosted at the National Energy Research Scientific Computing Center. Our results show that WoAS reduces workflow turnaround times and improves system utilization without significantly slowing down conventional jobs.},
    booktitle = {Proceedings of the 26th International Symposium on High-Performance Parallel and Distributed Computing},
    pages = {3–14},
    numpages = {12},
%    keywords = {high performance computing, hpc, scheduling, simulation, slurm, workflows},
    location = {Washington, DC, USA},
    series = {HPDC '17}
}
@inproceedings{pbs,
    author = {Staples, Garrick},
    title = {TORQUE Resource Manager},
    year = {2006},
    isbn = {0769527000},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    %    url = {https://doi.org/10.1145/1188455.1188464},
    doi = {10.1145/1188455.1188464},
%    abstract = {With TORQUE Resource Manager now reaching over 10,000 downloads per month and use across thousands of leading sites representing commercial, government, and academic organizations, we invite all TORQUE users to meet and discuss TORQUE with the professional developers, community volunteers other members who use and have contributed to the TORQUE project.Here we will discuss the current state of TORQUE including some of the more recent enhancements and capabilities along with the road map for the upcoming year. We will also provide a time for TORQUE users to share experiences, best practices, and new needs.},
    booktitle = {Proceedings of the 2006 ACM/IEEE Conference on Supercomputing},
    pages = {8–es},
    location = {Tampa, Florida},
    series = {SC '06}
}
@online{torque,
    title = {TORQUE resource manager},
    url = {https://adaptivecomputing.com/cherry-services/torque-resource-manager/},
    urldate = {2024-04-21},
}
@online{openpbs,
    title = {OpenPBS Open Source Project},
    url = {https://www.openpbs.org/},
    urldate = {2024-04-21},
}
@INPROCEEDINGS{flux,
    author={Ahn, Dong H. and Garlick, Jim and Grondona, Mark and Lipari, Don and Springmeyer, Becky and Schulz, Martin},
    booktitle={2014 43rd International Conference on Parallel Processing Workshops},
    title={Flux: A Next-Generation Resource Management Framework for Large HPC Centers},
    year={2014},
    volume={},
    number={},
    pages={9-17},
%    keywords={Resource management;Scheduling;Computational modeling;Software;Synchronization;Processor scheduling;Prototypes;resource management;communication framework;run-time;key value store;scalable process management services},
    doi={10.1109/ICPPW.2014.15}
}
@article{allocation-duration-prediction,
    title = {A Machine Learning Approach for an HPC Use Case: the Jobs Queuing Time Prediction},
    journal = {Future Generation Computer Systems},
    volume = {143},
    pages = {215-230},
    year = {2023},
    issn = {0167-739X},
    doi = {https://doi.org/10.1016/j.future.2023.01.020},
%    url = {https://www.sciencedirect.com/science/article/pii/S0167739X23000274},
    author = {Chiara Vercellino and Alberto Scionti and Giuseppe Varavallo and Paolo Viviani and Giacomo Vitali and Olivier Terzo},
%    keywords = {High performance computing, Queues, Batch scheduler, Automatism, Machine learning, Uncertainty quantification},
%    abstract = {High-Performance Computing (HPC) domain provided the necessary tools to support the scientific and industrial advancements we all have seen during the last decades. HPC is a broad domain targeting to provide both software and hardware solutions as well as envisioning methodologies that allow achieving goals of interest, such as system performance and energy efficiency. In this context, supercomputers have been the vehicle for developing and testing the most advanced technologies since their first appearance. Unlike cloud computing resources that are provided to the end-users in an on-demand fashion in the form of virtualized resources (i.e., virtual machines and containers), supercomputers’ resources are generally served through State-of-the-Art batch schedulers (e.g., SLURM, PBS, LSF, HTCondor). As such, the users submit their computational jobs to the system, which manages their execution with the support of queues. In this regard, predicting the behaviour of the jobs in the batch scheduler queues becomes worth it. Indeed, there are many cases where a deeper knowledge of the time experienced by a job in a queue (e.g., the submission of check-pointed jobs or the submission of jobs with execution dependencies) allows exploring more effective workflow orchestration policies. In this work, we focused on applying machine learning (ML) techniques to learn from the historical data collected from the queuing system of real supercomputers, aiming at predicting the time spent on a queue by a given job. Specifically, we applied both unsupervised learning (UL) and supervised learning (SL) techniques to define the most effective features for the prediction task and the actual prediction of the queue waiting time. For this purpose, two approaches have been explored: on one side, the prediction of ranges on jobs’ queuing times (classification approach) and, on the other side, the prediction of the waiting time at the minutes level (regression approach). Experimental results highlight the strong relationship between the SL models’ performances and the way the dataset is split. At the end of the prediction step, we present the uncertainty quantification approach, i.e., a tool to associate the predictions with reliability metrics, based on variance estimation.}
}
@INPROCEEDINGS{task-duration-prediction,
    author={Hilman, Muhammad Hafizhuddin and Rodriguez, Maria Alejandra and Buyya, Rajkumar},
    booktitle={2018 IEEE/ACM 11th International Conference on Utility and Cloud Computing (UCC)},
    title={Task Runtime Prediction in Scientific Workflows Using an Online Incremental Learning Approach},
    year={2018},
    volume={},
    number={},
    pages={93-102},
%    keywords={Task analysis;Runtime;Cloud computing;Machine learning;Monitoring;Computational modeling;Hardware;task runtime prediction;online incremental learning;scientific workflow},
    doi={10.1109/UCC.2018.00018}
}
@INPROCEEDINGS {psij,
    author = {M. Hategan-Marandiuc and A. Merzky and N. Collier and K. Maheshwari and J. Ozik and M. Turilli and A. Wilke and J. M. Wozniak and K. Chard and I. Foster and R. da Silva and S. Jha and D. Laney},
    booktitle = {2023 IEEE 19th International Conference on e-Science (e-Science)},
    title = {PSI/J: A Portable Interface for Submitting, Monitoring, and Managing Jobs},
    year = {2023},
    volume = {},
    issn = {},
    pages = {1-10},
%    abstract = {It is generally desirable for high-performance computing (HPC) applications to be portable between HPC systems, for example to make use of more performant hardware, make effective use of allocations, and to co-locate compute jobs with large datasets. Unfortunately, moving scientific applications between HPC systems is challenging for various reasons, most notably that HPC systems have different HPC schedulers. We introduce PSI/J, a job management abstraction API intended to simplify the construction of software components and applications that are portable over various HPC scheduler implementations. We argue that such a system is both necessary and that no viable alternative currently exists. We analyze similar notable APIs and attempt to determine the factors that influenced their evolution and adoption by the HPC community. We base the design of PSI/J on that analysis. We describe how PSI/J has been integrated in three workflow systems and one application, and also show via experiments that PSI/J imposes minimal overhead.},
%    keywords = {high performance computing;software;hardware;resource management;monitoring},
    doi = {10.1109/e-Science58273.2023.10254912},
%    url = {https://doi.ieeecomputersociety.org/10.1109/e-Science58273.2023.10254912},
    publisher = {IEEE Computer Society},
    address = {Los Alamitos, CA, USA},
    month = {oct}
}
@INPROCEEDINGS{workflow-alloc-manager-comm,
    author = {F. Lehmann and J. Bader and F. Tschirpke and L. Thamsen and U. Leser},
    booktitle = {2023 IEEE/ACM 23rd International Symposium on Cluster, Cloud and Internet Computing (CCGrid)},
    title = {How Workflow Engines Should Talk to Resource Managers: A Proposal for a Common Workflow Scheduling Interface},
    year = {2023},
    volume = {},
    issn = {},
    pages = {166-179},
%    abstract = {Scientific workflow management systems (SWMSs) and resource managers together ensure that tasks are scheduled on provisioned resources so that all dependencies are obeyed, and some optimization goal, such as makespan minimization, is achieved. In practice, however, there is no clear separation of scheduling responsibilities between an SWMS and a resource manager because there exists no agreed-upon separation of concerns between their different components. This has two consequences. First, the lack of a standardized API to exchange scheduling information between SWMSs and resource managers hinders portability. It incurs costly adaptations when a component should be replaced by a different one (e.g., an SWMS with another SWMS on the same resource manager). Second, due to overlapping functionalities, current installations often actually have two schedulers, both making partial scheduling decisions under incomplete information, leading to suboptimal workflow scheduling. In this paper, we propose a simple REST interface between SWMSs and resource managers, which allows any SWMS to pass dynamic workflow information to a resource manager, enabling maximally informed scheduling decisions. We provide an implementation of this API as an example, using Nextflow as an SWMS and Kubernetes as a resource manager. Our experiments with nine real-world workflows show that this strategy reduces makespan by up to 25.1% and 10.8% on average compared to the standard Nextflow/Kubernetes configuration. Furthermore, a more widespread implementation of this API would enable leaner code bases, a simpler exchange of components of workflow systems, and a unified place to implement new scheduling algorithms.},
%    keywords = {runtime;scheduling algorithms;prototypes;dynamic scheduling;minimization;proposals;task analysis},
    doi = {10.1109/CCGrid57682.2023.00025},
%    url = {https://doi.ieeecomputersociety.org/10.1109/CCGrid57682.2023.00025},
    publisher = {IEEE Computer Society},
    address = {Los Alamitos, CA, USA},
    month = {may}
}

% Scheduling
@article{Ullman1975,
    author = {Ullman, J. D.},
    title = {NP-complete Scheduling Problems},
    journal = {J. Comput. Syst. Sci.},
    issue_date = {June, 1975},
    volume = {10},
    number = {3},
    month = jun,
    year = {1975},
    issn = {0022-0000},
    pages = {384--393},
    numpages = {10},
    %    url = {http://dx.doi.org/10.1016/S0022-0000(75)80008-0},
    doi = {10.1016/S0022-0000(75)80008-0},
    acmid = {1740138},
    publisher = {Academic Press, Inc.},
    address = {Orlando, FL, USA},
}
@article{hlfet1974,
    author = {Adam, Thomas L. and Chandy, K. M. and Dickson, J. R.},
    title = {A Comparison of List Schedules for Parallel Processing Systems},
    journal = {Commun. ACM},
    issue_date = {Dec 1974},
    volume = {17},
    number = {12},
    month = dec,
    year = {1974},
    issn = {0001-0782},
    pages = {685--690},
    numpages = {6},
    %    url = {http://doi.acm.org/10.1145/361604.361619},
    doi = {10.1145/361604.361619},
    acmid = {361619},
    publisher = {ACM},
    address = {New York, NY, USA},
}
@inproceedings{kwok1998benchmarking,
    title = {Benchmarking the task graph scheduling algorithms},
    author = {Kwok, Yu-Kwong and Ahmad, Ishfaq},
    booktitle = {ipps},
    pages = {0531},
    year = {1998},
    organization = {IEEE}
}
@article{kwok1999static,
    author = {Kwok, Yu-Kwong and Ahmad, Ishfaq},
    title = {Static Scheduling Algorithms for Allocating Directed Task Graphs to Multiprocessors},
    year = {1999},
    issue_date = {Dec. 1999},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {31},
    number = {4},
    issn = {0360-0300},
    doi = {10.1145/344588.344618},
    journal = {ACM Comput. Surv.},
    month = {dec},
    pages = {406–471},
    numpages = {66},
%    keywords = {automatic parallelization, DAG, task graphs, static scheduling, software tools,
%	parallel processing, multiprocessors}
}
@ARTICLE{sih1993compile,
    author={Sih, G.C. and Lee, E.A.},
    journal={IEEE Transactions on Parallel and Distributed Systems},
    title={A compile-time scheduling heuristic for interconnection-constrained heterogeneous
	processor architectures},
    year={1993},
    volume={4},
    number={2},
    pages={175-187},
    doi={10.1109/71.207593}
}
@ARTICLE{wu1990hypertool,
    author={Wu, M.-Y. and Gajski, D.D.},
    journal={IEEE Transactions on Parallel and Distributed Systems},
    title={Hypertool: a programming aid for message-passing systems},
    year={1990},
    volume={1},
    number={3},
    pages={330-343},
    doi={10.1109/71.80160}
}
@article{hwang1989scheduling,
    title = {Scheduling precedence graphs of bounded height},
    journal = {Journal of Algorithms},
    volume = {5},
    number = {1},
    pages = {48-59},
    year = {1984},
    issn = {0196-6774},
    doi = {https://doi.org/10.1016/0196-6774(84)90039-7},
    author = {Danny Dolev and Manfred K Warmuth},
}
@article{omara2009genetic,
    title = {Genetic algorithms for task scheduling problem},
    journal = {Journal of Parallel and Distributed Computing},
    volume = {70},
    number = {1},
    pages = {13-22},
    year = {2010},
    issn = {0743-7315},
    doi = {10.1016/j.jpdc.2009.09.009},
    author = {Fatma A. Omara and Mona M. Arafa},
%    keywords = {Evolutionary computing, Genetic algorithms, Scheduling, Task partitioning, Graph
%	algorithms, Parallel processing},
}
@article{hagras2003static,
    title = {Static vs. dynamic list-scheduling performance comparison},
    author = {Hagras, Tarek and Jane{\v{c}}ek, J},
    journal = {Acta Polytechnica},
    volume = {43},
    number = {6},
    year = {2003}
}
@article{wang2018list,
    title = {List-Scheduling vs. Cluster-Scheduling},
    author = {Wang, Huijun and Sinnen, Oliver},
    journal = {IEEE Transactions on Parallel and Distributed Systems},
    year = {2018},
    publisher = {IEEE}
}
@ARTICLE{sinnen2005,
    author={Sinnen, O. and Sousa, L.A.},
    journal={IEEE Transactions on Parallel and Distributed Systems},
    title={Communication contention in task scheduling},
    year={2005},
    volume={16},
    number={6},
    pages={503-515},
    doi={10.1109/TPDS.2005.64}
}
@techreport{dagsim,
    TITLE = {{DAGSim: A Simulator for DAG Scheduling Algorithms}},
    AUTHOR = {Jarry, Aubin and Casanova, Henri and Berman, Francine},
%    URL = {https://hal-lara.archives-ouvertes.fr/hal-02101833},
    TYPE = {Research Report},
    NUMBER = {LIP RR-2000-46},
    PAGES = {2+8p},
    INSTITUTION = {{Laboratoire de l'informatique du parall{\'e}lisme}},
    YEAR = {2000},
    MONTH = Dec,
    KEYWORDS = {Heterogeneous Processors ; Load Balancing ; NWS ; Scheduling ; Simulator ; Grid Computing ; Processeurs de vitesses diff{\'e}rentes ; Equilibrage de charge ; Ordonnancement ; Simulateur},
%    PDF = {https://hal-lara.archives-ouvertes.fr/hal-02101833/file/RR2000-46.pdf},
    HAL_ID = {hal-02101833},
    HAL_VERSION = {v1},
}
@INPROCEEDINGS{simdag,
    author={Zulianto, Arief and Kuspriyanto and Gondokaryono, Yudi S.},
    booktitle={2016 6th International Conference on Electronics Information and Emergency Communication (ICEIEC)},
    title={HPC resources scheduling simulation using SimDAG},
    year={2016},
    volume={},
    number={},
    pages={334-337},
    doi={10.1109/ICEIEC.2016.7589751}
}
@article{simgrid,
    hal_id = {hal-01017319},
%    url = {http://hal.inria.fr/hal-01017319},
    title = {Versatile, Scalable, and Accurate Simulation of Distributed Applications and Platforms},
    author = {Casanova, Henri and Giersch, Arnaud and Legrand, Arnaud and Quinson, Martin and Suter, Fr{\'e}d{\'e}ric},
    publisher = {Elsevier},
    pages = {2899-2917},
    journal = {Journal of Parallel and Distributed Computing},
    volume = {74},
    number = {10},
    year = {2014},
    month = Jun,
%    pdf = {http://hal.inria.fr/hal-01017319/PDF/simgrid3-journal.pdf},
}
@article{tang2010list,
    author = {Tang, Xiaoyong and Li, Kenli and Liao, Guiping and Renfa, Li},
    year = {2010},
    month = {04},
    pages = {323-329},
    title = {List scheduling with duplication for heterogeneous computing systems},
    volume = {70},
    journal = {Journal of Parallel and Distributed Computing},
    doi = {10.1016/j.jpdc.2010.01.003}
}
@INPROCEEDINGS{yao2013task,
    author={Yao, Xuanxia and Geng, Peng and Du, Xiaojiang},
    booktitle={2013 International Conference on Parallel and Distributed Computing, Applications
	and Technologies},
    title={A Task Scheduling Algorithm for Multi-core Processors},
    year={2013},
    volume={},
    number={},
    pages={259-264},
    doi={10.1109/PDCAT.2013.47}
}
@article{kwok1996dynamic,
    author = {Kwok, Yu-Kwong and Ahmad, Ishfaq},
    title = {Dynamic Critical-Path Scheduling: An Effective Technique for Allocating Task Graphs
	to Multiprocessors},
    year = {1996},
    issue_date = {May 1996},
    publisher = {IEEE Press},
    volume = {7},
    number = {5},
    issn = {1045-9219},
    doi = {10.1109/71.503776},
    journal = {IEEE Trans. Parallel Distrib. Syst.},
    month = {may},
    pages = {506–521},
    numpages = {16},
%    keywords = {multiprocessors, clustering, processor allocation, list scheduling, parallel
%	scheduling, task graphs., Algorithms}
}
@InProceedings{glume,
    author="Hataishi, Evan
and Dutot, Pierre-Fran{\c{c}}ois
and Silva, Rafael Ferreira da
and Casanova, Henri",
    editor="Klus{\'a}{\v{c}}ek, Dalibor
and Cirne, Walfredo
and Rodrigo, Gonzalo P.",
    title="GLUME: A Strategy for Reducing Workflow Execution Times on Batch-Scheduled Platforms",
    booktitle="Job Scheduling Strategies for Parallel Processing",
    year="2021",
    publisher="Springer International Publishing",
    address="Cham",
    pages="210--230",
%    abstract="Many scientific workflows have computational demands that require the use of compute platforms managed by batch schedulers, which are unfortunately poorly suited to these applications. This work proposes GLUME, a strategy for partitioning a workflow into batch jobs. The novelty is that these jobs are explicitly constructed to minimize overall workflow execution time. Experimental evaluation via simulation of production batch workloads and workflows shows that our heuristic is more effective than previously proposed strategies when executing workflows with moderate to high computational demand.",
    isbn="978-3-030-88224-2"
}

% Distributed/task tools
@inproceedings{dask,
    author = {Rocklin, Matthew},
    year = {2015},
    month = {01},
    pages = {126-132},
    title = {Dask: Parallel Computation with Blocked algorithms and Task Scheduling},
    doi = {10.25080/Majora-7b98e3ed-013}
}
@online{dask-client-perf,
    title = {Task Graph Building Performance Issue in Dask},
    url = {https://github.com/dask/distributed/issues/3783},
    urldate = {2022-05-25},
}
@online{dask-user-survey,
    title = {2021 Dask User Survey},
    author = {Genevieve Buckley},
    url = {https://blog.dask.org/2021/09/15/user-survey},
    urldate = {2022-06-09},
}
@inproceedings{dasksparkcomparison,
    title={A performance comparison of Dask and Apache Spark for data-intensive
    neuroimaging pipelines},
    author={Dugr{\'e}, Mathieu and Hayot-Sasson, Val{\'e}rie and Glatard,
    Tristan},
    booktitle={2019 IEEE/ACM Workflows in Support of Large-Scale Science (WORKS)},
    year={2019},
    organization={IEEE}
}
@online{dask-ucx,
    title = {Dask UCX Integration},
    url = {https://dask-cuda.readthedocs.io/en/stable/ucx.html},
    urldate = {2024-04-21},
}
@online{dask-thread-recommendation,
    title = {Dask Best Practices - Processes and Threads},
    url = {https://docs.dask.org/en/stable/best-practices.html#processes-and-threads},
    urldate = {2024-07-18}
}
@online{dask-jobqueue,
    title = {Dask Jobqueue},
    url = {https://jobqueue.dask.org/en/latest/index.html},
    urldate = {2024-07-24},
}
@article{snakemake,
    author = {Köster, Johannes and Rahmann, Sven},
    title = "{Snakemake—a scalable bioinformatics workflow engine}",
    journal = {Bioinformatics},
    volume = {28},
    number = {19},
    pages = {2520-2522},
    year = {2012},
    month = {08},
%    abstract = "{Summary: Snakemake is a workflow engine that provides a readable Python-based workflow definition language and a powerful execution environment that scales from single-core workstations to compute clusters without modifying the workflow. It is the first system to support the use of automatically inferred multiple named wildcards (or variables) in input and output filenames.Availability:http://snakemake.googlecode.com.Contact:johannes.koester@uni-due.de}",
    issn = {1367-4803},
    doi = {10.1093/bioinformatics/bts480},
    %    url = "https://doi.org/10.1093/bioinformatics/bts480",
%    eprint = {https://academic.oup.com/bioinformatics/article-pdf/28/19/2520/819790/bts480.pdf},
}
@article{nextflow,
    author = {Di Tommaso, Paolo and Chatzou, Maria and Floden, Evan W. and Barja, Pablo and Palumbo, Emilio and Notredame, Cedric},
    year = {2017},
    month = {04},
    pages = {316-319},
    title = {Nextflow enables reproducible computational workflows},
    volume = {35},
    journal = {Nature Biotechnology},
    doi = {10.1038/nbt.3820}
}
@article{aiida,
    author="Huber, Sebastiaan P.
and Zoupanos, Spyros
and Uhrin, Martin
and Talirz, Leopold
and Kahle, Leonid
and H{\"a}uselmann, Rico
and Gresch, Dominik
and M{\"u}ller, Tiziano
and Yakutovich, Aliaksandr V.
and Andersen, Casper W.
and Ramirez, Francisco F.
and Adorf, Carl S.
and Gargiulo, Fernando
and Kumbhar, Snehal
and Passaro, Elsa
and Johnston, Conrad
and Merkys, Andrius
and Cepellotti, Andrea
and Mounet, Nicolas
and Marzari, Nicola
and Kozinsky, Boris
and Pizzi, Giovanni",
    title="AiiDA 1.0, a scalable computational infrastructure for automated reproducible workflows and data provenance",
    journal="Scientific Data",
    year="2020",
    month="Sep",
    day="08",
    volume="7",
    number="1",
    pages="300",
%    abstract="The ever-growing availability of computing power and the sustained development of advanced computational methods have contributed much to recent scientific progress. These developments present new challenges driven by the sheer number of calculations and data to manage. Next-generation exascale supercomputers will harden these challenges, such that automated and scalable solutions become crucial. In recent years, we have been developing AiiDA (aiida.net), a robust open-source high-throughput infrastructure addressing the challenges arising from the needs of automated workflow management and data provenance recording. Here, we introduce developments and capabilities required to reach sustained performance, with AiiDA supporting throughputs of tens of thousands processes/hour, while automatically preserving and storing the full data provenance in a relational database making it queryable and traversable, thus enabling high-performance data analytics. AiiDA's workflow language provides advanced automation, error handling features and a flexible plugin model to allow interfacing with external simulation software. The associated plugin registry enables seamless sharing of extensions, empowering a vibrant user community dedicated to making simulations more robust, user-friendly and reproducible.",
    issn="2052-4463",
    doi="10.1038/s41597-020-00638-4",
%    url="https://doi.org/10.1038/s41597-020-00638-4"
}
@article{streamflow,
%    abstract = {Workflows are among the most commonly used tools in a variety of execution environments. Many of them target a specific environment; few of them make it possible to execute an entire workflow in different environments, e.g. Kubernetes and batch clusters. We present a novel approach to workflow execution, called StreamFlow, that complements the workflow graph with the declarative description of potentially complex execution environments, and that makes it possible the execution onto multiple sites not sharing a common data space. StreamFlow is then exemplified on a novel bioinformatics pipeline for single cell transcriptomic data analysis workflow.},
    author = {Iacopo Colonnelli and Barbara Cantalupo and Ivan Merelli and Marco Aldinucci},
    doi = {10.1109/TETC.2020.3019202},
    journal = {{IEEE} {T}ransactions on {E}merging {T}opics in {C}omputing},
    number = {4},
    pages = {1723--1737},
    title = {{StreamFlow}: cross-breeding cloud with {HPC}},
    volume = {9},
    year = {2021}
}
@article{gromacs,
    author="Lindahl, Erik
and Hess, Berk
and van der Spoel, David",
    title="GROMACS 3.0: a package for molecular simulation and trajectory analysis",
    journal="Molecular modeling annual",
    year="2001",
    month="Aug",
    day="01",
    volume="7",
    number="8",
    pages="306--317",
%    abstract="GROMACS 3.0 is the latest release of a versatile and very well optimized package for molecular simulation. Much effort has been devoted to achieving extremely high performance on both workstations and parallel computers. The design includes an extraction of virial and periodic boundary conditions from the loops over pairwise interactions, and special software routines to enable rapid calculation of x--1/2. Inner loops are generated automatically in C or Fortran at compile time, with optimizations adapted to each architecture. Assembly loops using SSE and 3DNow! Multimedia instructions are provided for x86 processors, resulting in exceptional performance on inexpensive PC workstations. The interface is simple and easy to use (no scripting language), based on standard command line arguments with self-explanatory functionality and integrated documentation. All binary files are independent of hardware endian and can be read by versions of GROMACS compiled using different floating-point precision. A large collection of flexible tools for trajectory analysis is included, with output in the form of finished Xmgr/Grace graphs. A basic trajectory viewer is included, and several external visualization tools can read the GROMACS trajectory format. Starting with version 3.0, GROMACS is available under the GNU General Public License from http://www.gromacs.org.",
    issn="0948-5023",
    doi="10.1007/s008940100045",
%    url="https://doi.org/10.1007/s008940100045"
}
@article{gromacs_mpi,
    title = {GROMACS: High performance molecular simulations through multi-level parallelism from laptops to supercomputers},
    journal = {SoftwareX},
    volume = {1-2},
    pages = {19-25},
    year = {2015},
    issn = {2352-7110},
    doi = {https://doi.org/10.1016/j.softx.2015.06.001},
%    url = {https://www.sciencedirect.com/science/article/pii/S2352711015000059},
    author = {Mark James Abraham and Teemu Murtola and Roland Schulz and Szilárd Páll and Jeremy C. Smith and Berk Hess and Erik Lindahl},
%    keywords = {Molecular dynamics, GPU, SIMD, Free energy},
%    abstract = {GROMACS is one of the most widely used open-source and free software codes in chemistry, used primarily for dynamical simulations of biomolecules. It provides a rich set of calculation types, preparation and analysis tools. Several advanced techniques for free-energy calculations are supported. In version 5, it reaches new performance heights, through several new and enhanced parallelization algorithms. These work on every level; SIMD registers inside cores, multithreading, heterogeneous CPU–GPU acceleration, state-of-the-art 3D domain decomposition, and ensemble-level parallelization through built-in replica exchange and the separate Copernicus framework. The latest best-in-class compressed trajectory storage format is supported.}
}
@article{ligen,
    author = {Beccari, Andrea and Cavazzoni, Carlo and Beato, Claudia and Costantino, Gabriele},
    year = {2013},
    month = {04},
    pages = {},
    title = {LiGen: A High Performance Workflow for Chemistry Driven de Novo Design},
    volume = {53},
    journal = {Journal of chemical information and modeling},
    doi = {10.1021/ci400078g}
}
@article{ligen_exscalate,
    author = {Gadioli, Davide and Vitali, Emanuele and Ficarelli, Federico and Latini, Chiara and Manelfi, Candida and Talarico, Carmine and Silvano, Cristina and Cavazzoni, Carlo and Palermo, Gianluca and Beccari, Andrea},
    year = {2022},
    month = {01},
    pages = {1-12},
    title = {EXSCALATE: An Extreme-Scale Virtual Screening Platform for Drug Discovery Targeting Polypharmacology to Fight SARS-CoV-2},
    volume = {PP},
    journal = {IEEE Transactions on Emerging Topics in Computing},
    doi = {10.1109/TETC.2022.3187134}
}
@inproceedings{hyperloom,
    author = {Cima, Vojtěch and Böhm, Stanislav and Martinovič, Jan and Dvorský, Jiří and Janurová, Kateřina and Aa, Tom Vander and Ashby, Thomas J. and Chupakhin, Vladimir},
    title = {HyperLoom: A Platform for Defining and Executing Scientific Pipelines in Distributed Environments},
    year = {2018},
    isbn = {9781450364447},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
%    url = {https://doi.org/10.1145/3183767.3183768},
    doi = {10.1145/3183767.3183768},
    pages = {1–6},
    numpages = {6},
    %    keywords = {Chemogenomics, Distributed Computing, Task Scheduling, Scientific Pipeline, Big Data, HPC, Machine Learning},
    series = {PARMA-DITAM '18}
}
@inproceedings{ray,
    author = {Moritz, Philipp and Nishihara, Robert and Wang, Stephanie and Tumanov, Alexey and Liaw, Richard and Liang, Eric and Elibol, Melih and Yang, Zongheng and Paul, William and Jordan, Michael I. and Stoica, Ion},
    title = {Ray: A Distributed Framework for Emerging AI Applications},
    year = {2018},
    isbn = {9781931971478},
    publisher = {USENIX Association},
    address = {USA},
%    abstract = {The next generation of AI applications will continuously interact with the environment and learn from these interactions. These applications impose new and demanding systems requirements, both in terms of performance and flexibility. In this paper, we consider these requirements and present Ray--a distributed system to address them. Ray implements a unified interface that can express both task-parallel and actor-based computations, supported by a single dynamic execution engine. To meet the performance requirements, Ray employs a distributed scheduler and a distributed and fault-tolerant store to manage the system's control state. In our experiments, we demonstrate scaling beyond 1.8 million tasks per second and better performance than existing specialized systems for several challenging reinforcement learning applications.},
    booktitle = {Proceedings of the 13th USENIX Conference on Operating Systems Design and Implementation},
    pages = {561–577},
    numpages = {17},
    location = {Carlsbad, CA, USA},
    series = {OSDI'18}
}
@inproceedings{parsl,
    author = {Babuji, Yadu and Woodard, Anna and Li, Zhuozhao and Katz, Daniel S. and Clifford, Ben and Kumar, Rohan and Lacinski, Lukasz and Chard, Ryan and Wozniak, Justin M. and Foster, Ian and Wilde, Michael and Chard, Kyle},
    title = {Parsl: Pervasive Parallel Programming in Python},
    year = {2019},
    isbn = {9781450366700},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    %    url = {https://doi.org/10.1145/3307681.3325400},
    doi = {10.1145/3307681.3325400},
%    abstract = {High-level programming languages such as Python are increasingly used to provide intuitive interfaces to libraries written in lower-level languages and for assembling applications from various components. This migration towards orchestration rather than implementation, coupled with the growing need for parallel computing (e.g., due to big data and the end of Moore's law), necessitates rethinking how parallelism is expressed in programs. Here, we present Parsl, a parallel scripting library that augments Python with simple, scalable, and flexible constructs for encoding parallelism. These constructs allow Parsl to construct a dynamic dependency graph of components that it can then execute efficiently on one or many processors. Parsl is designed for scalability, with an extensible set of executors tailored to different use cases, such as low-latency, high-throughput, or extreme-scale execution. We show, via experiments on the Blue Waters supercomputer, that Parsl executors can allow Python scripts to execute components with as little as 5 ms of overhead, scale to more than 250000 workers across more than 8000 nodes, and process upward of 1200 tasks per second. Other Parsl features simplify the construction and execution of composite programs by supporting elastic provisioning and scaling of infrastructure, fault-tolerant execution, and integrated wide-area data management. We show that these capabilities satisfy the needs of many-task, interactive, online, and machine learning applications in fields such as biology, cosmology, and materials science.},
    booktitle = {Proceedings of the 28th International Symposium on High-Performance Parallel and Distributed Computing},
    pages = {25–36},
    numpages = {12},
    location = {Phoenix, AZ, USA},
    series = {HPDC '19}
}
@article{pycompss,
    author = {Tejedor, Enric and Becerra, Yolanda and Alomar, Guillem and Queralt, Anna and Badia, Rosa M. and Torres, Jordi and Cortes, Toni and Labarta, Jesús},
    year = {2015},
    month = {08},
    pages = {},
    title = {PyCOMPSs: Parallel computational workflows in Python},
    volume = {31},
    journal = {International Journal of High Performance Computing Applications},
    doi = {10.1177/1094342015594678}
}
@article{pegasus,
    author={Deelman, Ewa and Vahi, Karan and Rynge, Mats and Mayani, Rajiv and da Silva, Rafael Ferreira and Papadimitriou, George and Livny, Miron},
    journal={Computing in Science \& Engineering},
    title={The Evolution of the Pegasus Workflow Management Software},
    year={2019},
    volume={21},
    number={4},
    pages={22-36},
    doi={10.1109/MCSE.2019.2919690}
}
@Article{starpu,
    author = {C{\'e}dric Augonnet and Samuel Thibault and Raymond Namyst and Pierre-Andr{\'e} Wacrenier},
    title = {{StarPU: A Unified Platform for Task Scheduling on Heterogeneous Multicore Architectures}},
    journal = {CCPE - Concurrency and Computation: Practice and Experience, Special Issue: Euro-Par 2009},
    volume = 23,
    issue = 2,
    pages = {187--198},
    year = 2011,
    month = FEB,
    publisher = {John Wiley \& Sons, Ltd.},
    doi = {10.1002/cpe.1631},
    %    url = {http://hal.inria.fr/inria-00550877},
    %    pdf = {http://hal.inria.fr/inria-00550877/document},
    KEYWORDS = {General Presentations;StarPU}
}
@inproceedings{dace,
    author = {Ben-Nun, Tal and de Fine Licht, Johannes and Ziogas, Alexandros N. and Schneider, Timo and Hoefler, Torsten},
    title = {Stateful Dataflow Multigraphs: A Data-Centric Model for Performance Portability on Heterogeneous Architectures},
    year = {2019},
    isbn = {9781450362290},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    %    url = {https://doi.org/10.1145/3295500.3356173},
    doi = {10.1145/3295500.3356173},
    %    abstract = {The ubiquity of accelerators in high-performance computing has driven programming complexity beyond the skill-set of the average domain scientist. To maintain performance portability in the future, it is imperative to decouple architecture-specific programming paradigms from the underlying scientific computations. We present the Stateful DataFlow multiGraph (SDFG), a data-centric intermediate representation that enables separating program definition from its optimization. By combining fine-grained data dependencies with high-level control-flow, SDFGs are both expressive and amenable to program transformations, such as tiling and double-buffering. These transformations are applied to the SDFG in an interactive process, using extensible pattern matching, graph rewriting, and a graphical user interface. We demonstrate SDFGs on CPUs, GPUs, and FPGAs over various motifs --- from fundamental computational kernels to graph analytics. We show that SDFGs deliver competitive performance, allowing domain scientists to develop applications naturally and port them to approach peak hardware performance without modifying the original scientific code.},
    booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
    articleno = {81},
    numpages = {14},
    location = {Denver, Colorado},
    series = {SC '19}
}
@inproceedings{legate,
    author = {Bauer, Michael and Garland, Michael},
    title = {Legate NumPy: Accelerated and Distributed Array Computing},
    year = {2019},
    isbn = {9781450362290},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
%    url = {https://doi.org/10.1145/3295500.3356175},
    doi = {10.1145/3295500.3356175},
%    abstract = {NumPy is a popular Python library used for performing array-based numerical computations. The canonical implementation of NumPy used by most programmers runs on a single CPU core and is parallelized to use multiple cores for some operations. This restriction to a single-node CPU-only execution limits both the size of data that can be handled and the potential speed of NumPy code. In this work we introduce Legate, a drop-in replacement for NumPy that requires only a single-line code change and can scale up to an arbitrary number of GPU accelerated nodes. Legate works by translating NumPy programs to the Legion programming model and then leverages the scalability of the Legion runtime system to distribute data and computations across an arbitrary sized machine. Compared to similar programs written in the distributed Dask array library in Python, Legate achieves speed-ups of up to 10X on 1280 CPUs and 100X on 256 GPUs.},
    booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
    articleno = {23},
    numpages = {23},
%    keywords = {HPC, control replication, logical regions, NumPy, distributed execution, GPU, Python, legate, task-based runtimes, legion},
    location = {Denver, Colorado},
    series = {SC '19}
}
@INPROCEEDINGS{pygion,
    author={Slaughter, Elliott and Aiken, Alex},
    booktitle={2019 IEEE/ACM Parallel Applications Workshop, Alternatives To MPI (PAW-ATM)},
    title={Pygion: Flexible, Scalable Task-Based Parallelism with Python},
    year={2019},
    volume={},
    number={},
    pages={58-72},
    doi={10.1109/PAW-ATM49560.2019.00011}
}
@article{flink,
    author = {Carbone, Paris and Katsifodimos, Asterios and Kth, † and Sweden, Sics and Ewen, Stephan and Markl, Volker and Haridi, Seif and Tzoumas, Kostas},
    year = {2015},
    month = {01},
%    pages = {},
    title = {Apache Flink™: Stream and Batch Processing in a Single Engine},
    volume = {38},
    journal = {IEEE Data Engineering Bulletin}
}
@INPROCEEDINGS{kafka,
    author={Shree, Rishika and Choudhury, Tanupriya and Gupta, Subhash Chand and Kumar, Praveen},
    booktitle={2017 2nd International Conference on Telecommunication and Networks (TEL-NET)},
    title={KAFKA: The modern platform for data management and analysis in big data domain},
    year={2017},
    volume={},
    number={},
    pages={1-5},
    %    keywords={Real-time systems;Distributed databases;Tools;File systems;Ecosystems;Servers;Reliability;Apache Kafka;hadoop;stream processing;low latency;flume;high throughput;publish-subscribe;producer;consumer},
    doi={10.1109/TEL-NET.2017.8343593}
}
@inproceedings{timely_dataflow,
    author = {Murray, Derek G. and McSherry, Frank and Isaacs, Rebecca and Isard, Michael and Barham, Paul and Abadi, Mart\'{\i}n},
    title = {Naiad: a timely dataflow system},
    year = {2013},
    isbn = {9781450323888},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
%    url = {https://doi.org/10.1145/2517349.2522738},
    doi = {10.1145/2517349.2522738},
%    abstract = {Naiad is a distributed system for executing data parallel, cyclic dataflow programs. It offers the high throughput of batch processors, the low latency of stream processors, and the ability to perform iterative and incremental computations. Although existing systems offer some of these features, applications that require all three have relied on multiple platforms, at the expense of efficiency, maintainability, and simplicity. Naiad resolves the complexities of combining these features in one framework.A new computational model, timely dataflow, underlies Naiad and captures opportunities for parallelism across a wide class of algorithms. This model enriches dataflow computation with timestamps that represent logical points in the computation and provide the basis for an efficient, lightweight coordination mechanism.We show that many powerful high-level programming models can be built on Naiad's low-level primitives, enabling such diverse tasks as streaming data analysis, iterative machine learning, and interactive graph mining. Naiad outperforms specialized systems in their target application domains, and its unique features enable the development of new high-performance applications.},
    booktitle = {Proceedings of the Twenty-Fourth ACM Symposium on Operating Systems Principles},
    pages = {439–455},
    numpages = {17},
    location = {Farminton, Pennsylvania},
    series = {SOSP '13}
}
@article{sciluigi,
    author = {Lampa, Samuel and Alvarsson, Jonathan and Spjuth, Ola},
    year = {2016},
    month = {11},
    pages = {},
    title = {Towards agile large-scale predictive modelling in drug discovery with flow-based programming design principles},
    volume = {8},
    journal = {Journal of Cheminformatics},
    doi = {10.1186/s13321-016-0179-6}
}
@article{vaex,
    author = {Breddels, Maarten and Veljanoski, Jovan},
    year = {2018},
    month = {01},
    pages = {},
    title = {Vaex: Big Data exploration in the era of Gaia},
    volume = {618},
    journal = {Astronomy \& Astrophysics},
    doi = {10.1051/0004-6361/201732493}
}
@Manual{cudf,
    title = {RAPIDS: Libraries for End to End GPU Data Science},
    author = {RAPIDS Development Team},
    year = {2023},
    url = {https://rapids.ai}
}
@phdthesis{modin,
    Author = {Petersohn, Devin},
    Title = {Dataframe Systems: Theory, Architecture, and Implementation},
    School = {EECS Department, University of California, Berkeley},
    Year = {2021},
    Month = {Aug},
    URL = {http://www2.eecs.berkeley.edu/Pubs/TechRpts/2021/EECS-2021-193.html},
    Number = {UCB/EECS-2021-193},
%    Abstract = {Dataframes are a popular abstraction to represent, prepare, and analyze data. Despite the remarkable success of dataframe libraries in R and Python, dataframes face performance issues even on moderately large datasets. Moreover, there is significant ambiguity regarding dataframe semantics. In this thesis, we discuss the implications of signature dataframe features including flexible schemas, ordering, row/column equivalence, and data/meta-data fluidity, as well as the piecemeal, trial-and-error-based approach to interacting with dataframes. While most modern systems aim to scale dataframe workloads by changing properties of dataframes – or by adding new distributed systems knowledge requirements– we believe it is important to support scalable ops on dataframes without changing their semantics. This thesis takes a ground-up approach towards scaling dataframe systems,starting with a formal data model and algebra, and ending with a reference implementation. This implementation, Modin, has already accumulated a significant amount of community support: over 6,000 GitHub stars and over 1 million installs to date. This interest shows the need for systems that solve modern data science problems without changing semantics. Included in this thesis are several of our insights into how to build systems for data scientists and what data scientists prioritize. We believe these insights were instrumental in unlocking the interest and support from the community in our open source work.}
}
@article{merlin,
    title = {Enabling machine learning-ready HPC ensembles with Merlin},
    journal = {Future Generation Computer Systems},
    volume = {131},
    pages = {255-268},
    year = {2022},
    issn = {0167-739X},
    doi = {https://doi.org/10.1016/j.future.2022.01.024},
%    url = {https://www.sciencedirect.com/science/article/pii/S0167739X22000322},
    author = {J. Luc Peterson and Ben Bay and Joe Koning and Peter Robinson and Jessica Semler and Jeremy White and Rushil Anirudh and Kevin Athey and Peer-Timo Bremer and Francesco {Di Natale} and David Fox and Jim A. Gaffney and Sam A. Jacobs and Bhavya Kailkhura and Bogdan Kustowski and Steven Langer and Brian Spears and Jayaraman Thiagarajan and Brian {Van Essen} and Jae-Seung Yeom},
%    keywords = {Workflow management, Scientific computing, Machine learning, High performance computing, High throughput computing, Distributed computing},
%    abstract = {With the growing complexity of computational and experimental facilities, many scientific researchers are turning to machine learning (ML) techniques to analyze large scale ensemble data. With complexities such as multi-component workflows, heterogeneous machine architectures, parallel file systems, and batch scheduling, care must be taken to facilitate this analysis in a high performance computing (HPC) environment. In this paper, we present Merlin, a workflow framework to enable large ML-friendly ensembles of scientific HPC simulations. By augmenting traditional HPC with distributed compute technologies, Merlin aims to lower the barrier for scientific subject matter experts to incorporate ML into their analysis. As a producer–consumer workflow model, Merlin enables multi-machine, cross-batch job, dynamically allocated yet persistent workflows capable of utilizing surge-compute resources. Key features of Merlin are a flexible HPC-centric interface, low per-task overhead, multi-tiered fault recovery, and a hierarchical sampling algorithm that allows for O(N) task execution and O(NlnN) task queuing to ensembles of millions of tasks. In addition to Merlin’s design, we test the algorithm’s performance in an HPC center and demonstrate the ability to enqueue 40 million simulations in 100 s, with a 30 millisecond per-task overhead that is independent of ensemble size. Finally, we describe some example applications that Merlin has enabled on leadership-class HPC resources, such as the ML-augmented optimization of nuclear fusion experiments and the calibration of infectious disease models to study the progression of and possible mitigation strategies for COVID-19.}
}
@article {fireworks,
    author = {Jain, Anubhav and Ong, Shyue Ping and Chen, Wei and Medasani, Bharat and Qu, Xiaohui and Kocher, Michael and Brafman, Miriam and Petretto, Guido and Rignanese, Gian-Marco and Hautier, Geoffroy and Gunter, Daniel and Persson, Kristin A.},
    title = {FireWorks: a dynamic workflow system designed for high-throughput applications},
    journal = {Concurrency and Computation: Practice and Experience},
    volume = {27},
    number = {17},
    issn = {1532-0634},
%    url = {http://dx.doi.org/10.1002/cpe.3505},
    doi = {10.1002/cpe.3505},
    pages = {5037--5059},
%    keywords = {scientific workflows, high-throughput computing, fault-tolerant computing},
    year = {2015},
    note = {CPE-14-0307.R2},
}
@INPROCEEDINGS{autosubmit,
    author={Manubens-Gil, Domingo and Vegas-Regidor, Javier and Prodhomme, Chloe and Mula-Valls, Oriol and Doblas-Reyes, Francisco J.},
    booktitle={2016 International Conference on High Performance Computing \& Simulation (HPCS)},
    title={Seamless management of ensemble climate prediction experiments on HPC platforms},
    year={2016},
    volume={},
    number={},
    pages={895-900},
%    keywords={Computational modeling;Atmospheric modeling;Monitoring;Earth;Data models;Weather forecasting;Earth System Modelling;Climate;Weather;High Performance Computing;Workflow manager},
    doi={10.1109/HPCSim.2016.7568429}
}
@inproceedings{pydra,
    author = {Jarecka, Dorota and Goncalves, Mathias and Markiewicz, Christopher and Esteban, Oscar and Lo, Nicole and Kaczmarzyk, Jakub and Ghosh, Satrajit},
    year = {2020},
    month = {01},
    pages = {132-139},
    title = {Pydra - a flexible and lightweight dataflow engine for scientific analyses},
    doi = {10.25080/Majora-342d178e-012}
}
@inproceedings{hpx,
    author = {Kaiser, Hartmut and Heller, Thomas and Adelstein-Lelbach, Bryce and Serio, Adrian and Fey, Dietmar},
    title = {HPX: A Task Based Programming Model in a Global Address Space},
    year = {2014},
    isbn = {9781450332477},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
%    url = {https://doi.org/10.1145/2676870.2676883},
    doi = {10.1145/2676870.2676883},
%    abstract = {The significant increase in complexity of Exascale platforms due to energy-constrained, billion-way parallelism, with major changes to processor and memory architecture, requires new energy-efficient and resilient programming techniques that are portable across multiple future generations of machines. We believe that guaranteeing adequate scalability, programmability, performance portability, resilience, and energy efficiency requires a fundamentally new approach, combined with a transition path for existing scientific applications, to fully explore the rewards of todays and tomorrows systems. We present HPX -- a parallel runtime system which extends the C++11/14 standard to facilitate distributed operations, enable fine-grained constraint based parallelism, and support runtime adaptive resource management. This provides a widely accepted API enabling programmability, composability and performance portability of user applications. By employing a global address space, we seamlessly augment the standard to apply to a distributed case. We present HPX's architecture, design decisions, and results selected from a diverse set of application runs showing superior performance, scalability, and efficiency over conventional practice.},
    booktitle = {Proceedings of the 8th International Conference on Partitioned Global Address Space Programming Models},
    articleno = {6},
    numpages = {11},
%    keywords = {Exascale, Global Address Space, High Performance Computing, Parallel Runtime Systems, Programming Models},
    location = {Eugene, OR, USA},
    series = {PGAS '14}
}
@ARTICLE{parsec,
    author={Bosilca, George and Bouteiller, Aurelien and Danalis, Anthony and Faverge, Mathieu and Herault, Thomas and Dongarra, Jack J.},
    journal={Computing in Science \& Engineering},
    title={PaRSEC: Exploiting Heterogeneity to Enhance Scalability},
    year={2013},
    volume={15},
    number={6},
    pages={36-45},
%    keywords={Programming;Computer architecture;Runtime;Computational modeling;Parallel processing;Biological system modeling;Adaptation models;Scalability;Programming;Computer architecture;Runtime;Computational modeling;Parallel processing;Biological system modeling;Adaptation models;Scalability;scientific computing;high-performance computing;HPC;scheduling and task partitioning;distributed programming;programming paradigms},
    doi={10.1109/MCSE.2013.98}
}
@INPROCEEDINGS{legion,
    author={Bauer, Michael and Treichler, Sean and Slaughter, Elliott and Aiken, Alex},
    booktitle={SC '12: Proceedings of the International Conference on High Performance Computing, Networking, Storage and Analysis},
    title={Legion: Expressing locality and independence with logical regions},
    year={2012},
    volume={},
    number={},
    pages={1-11},
%    keywords={Wires;Coherence;Vegetation;Circuit simulation;Out of order;Programming},
    doi={10.1109/SC.2012.71}
}
@article{airfow,
    author = {Kotliar, Michael and Kartashov, Andrey V and Barski, Artem},
    title = "{CWL-Airflow: a lightweight pipeline manager supporting Common Workflow Language}",
    journal = {GigaScience},
    volume = {8},
    number = {7},
    pages = {giz084},
    year = {2019},
    month = {07},
%    abstract = "{Massive growth in the amount of research data and computational analysis has led to increased use of pipeline managers in biomedical computational research. However, each of the \\&gt;100 such managers uses its own way to describe pipelines, leading to difficulty porting workflows to different environments and therefore poor reproducibility of computational studies. For this reason, the Common Workflow Language (CWL) was recently introduced as a specification for platform-independent workflow description, and work began to transition existing pipelines and workflow managers to CWL.Herein, we present CWL-Airflow, a package that adds support for CWL to the Apache Airflow pipeline manager. CWL-Airflow uses CWL version 1.0 specification and can run workflows on stand-alone MacOS/Linux servers, on clusters, or on a variety of cloud platforms. A sample CWL pipeline for processing of chromatin immunoprecipitation sequencing data is provided.CWL-Airflow will provide users with the features of a fully fledged pipeline manager and the ability to execute CWL workflows anywhere Airflow can run—from a laptop to a cluster or cloud environment. CWL-Airflow is available under Apache License, version 2.0 (Apache-2.0), and can be downloaded from https://barski-lab.github.io/cwl-airflow, https://scicrunch.org/resolver/RRID:SCR\_017196.}",
    issn = {2047-217X},
    doi = {10.1093/gigascience/giz084},
%    url = {https://doi.org/10.1093/gigascience/giz084},
%    eprint = {https://academic.oup.com/gigascience/article-pdf/8/7/giz084/28954484/giz084.pdf},
}
@online{dagster,
    title = {Dagster},
    url = {https://dagster.io/},
    urldate = {2024-06-12},
}
@online{prefect,
    title = {Prefect},
    url = {https://www.prefect.io/},
}
@online{argo,
    title = {Argo Workflows},
    url = {https://argoproj.github.io/workflows/},
    urldate = {2024-06-12},
}
@inproceedings{falkon,
    author = {Raicu, Ioan and Zhao, Yong and Dumitrescu, Catalin and Foster, Ian and Wilde, Mike},
    title = {Falkon: a Fast and Light-weight tasK executiON framework},
    year = {2007},
    isbn = {9781595937643},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
%    url = {https://doi.org/10.1145/1362622.1362680},
    doi = {10.1145/1362622.1362680},
%    abstract = {To enable the rapid execution of many tasks on compute clusters, we have developed Falkon, a Fast and Light-weight tasK executiON framework. Falkon integrates (1) multi-level scheduling to separate resource acquisition (via, e.g., requests to batch schedulers) from task dispatch, and (2) a streamlined dispatcher. Falkon's integration of multi-level scheduling and streamlined dispatchers delivers performance not provided by any other system. We describe Falkon architecture and implementation, and present performance results for both microbenchmarks and applications. Microbenchmarks show that Falkon throughput (487 tasks/sec) and scalability (to 54,000 executors and 2,000,000 tasks processed in just 112 minutes) are one to two orders of magnitude better than other systems used in production Grids. Large-scale astronomy and medical applications executed under Falkon by the Swift parallel programming system achieve up to 90\% reduction in end-to-end run time, relative to versions that execute tasks via separate scheduler submissions.},
    booktitle = {Proceedings of the 2007 ACM/IEEE Conference on Supercomputing},
    articleno = {43},
    numpages = {12},
%    keywords = {dynamic resource provisioning, grid computing, parallel programming, scheduling},
    location = {Reno, Nevada},
    series = {SC '07}
}
@inproceedings{ehpc,
    author = {Fox, William and Ghoshal, Devarshi and Souza, Abel and Rodrigo, Gonzalo P. and Ramakrishnan, Lavanya},
    title = {E-HPC: a library for elastic resource management in HPC environments},
    year = {2017},
    isbn = {9781450351294},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
%    url = {https://doi.org/10.1145/3150994.3150996},
    doi = {10.1145/3150994.3150996},
%    abstract = {Next-generation data-intensive scientific workflows need to support streaming and real-time applications with dynamic resource needs on high performance computing (HPC) platforms. The static resource allocation model on current HPC systems that was designed for monolithic MPI applications is insufficient to support the elastic resource needs of current and future workflows. In this paper, we discuss the design, implementation and evaluation of Elastic-HPC (E-HPC), an elastic framework for managing resources for scientific workflows on current HPC systems. E-HPC considers a resource slot for a workflow as an elastic window that might map to different physical resources over the duration of a workflow. Our framework uses checkpoint-restart as the underlying mechanism to migrate workflow execution across the dynamic window of resources. E-HPC provides the foundation necessary to enable dynamic resource allocation of HPC resources that are needed for streaming and real-time workflows. E-HPC has negligible overhead beyond the cost of checkpointing. Additionally, E-HPC results in decreased turnaround time of workflows compared to traditional model of resource allocation for workflows, where resources are allocated per stage of the workflow. Our evaluation shows that E-HPC improves core hour utilization for common workflow resource use patterns and provides an effective framework for elastic expansion of resources for applications with dynamic resource needs.},
    booktitle = {Proceedings of the 12th Workshop on Workflows in Support of Large-Scale Science},
    articleno = {1},
    numpages = {11},
%    keywords = {HPC systems, elastic resource management, scientific workflows},
    location = {Denver, Colorado},
    series = {WORKS '17}
}
@INPROCEEDINGS{l-dag,
    author={You, Xin and Yang, Hailong and Luan, Zhongzhi and Qian, Depei},
    booktitle={2019 IEEE Intl Conf on Dependable, Autonomic and Secure Computing, Intl Conf on Pervasive Intelligence and Computing, Intl Conf on Cloud and Big Data Computing, Intl Conf on Cyber Science and Technology Congress (DASC/PiCom/CBDCom/CyberSciTech)},
    title={L-DAG: Enabling Loopy Workflow in Scientific Application with Automatic DAG Transformation},
    year={2019},
    volume={},
    number={},
    pages={946-953},
%    keywords={Task analysis;Transforms;XML;Engines;Tools;Computational modeling;Atmospheric modeling;Loopy workflow, DAG, Automatic transformation, Scientific application, HTCondor},
    doi={10.1109/DASC/PiCom/CBDCom/CyberSciTech.2019.00174}
}
@software{parallel,
    author       = {Tange, Ole},
    title        = {GNU Parallel 20230722 ('Приго́жин')},
    month        = Jul,
    year         = 2023,
    note         = {{GNU Parallel is a general parallelizer to run
                       multiple serial command line programs in parallel
                       without changing them.}},
    publisher    = {Zenodo},
    doi          = {10.5281/zenodo.8175685},
%    url          = {https://doi.org/10.5281/zenodo.8175685}
}

% Web interfaces for accessing clusters
@INPROCEEDINGS{cumulus,
    author={Harris, Chris and O'Leary, Patrick and Grauer, Michael and Chaudhary, Aashish and Kotfila, Chris and O'Bara, Robert},
    booktitle={2016 6th Workshop on Python for High-Performance and Scientific Computing (PyHPC)},
    title={Dynamic Provisioning and Execution of HPC Workflows Using Python},
    year={2016},
    volume={},
    number={},
    pages={1-8},
%    keywords={Structural beams;Electronic mail;Cloud computing;Dynamic scheduling;Complexity theory;Open source software},
    doi={10.1109/PyHPC.2016.005}
}
@article{openondemand,
    title = {Open OnDemand: A web-based client portal for HPC centers},
    author = {Dave Hudak and Doug Johnson and Alan Chalker and Jeremy Nicklas and Eric Franz and Trey Dockendorf and Brian L. McMichael},
    doi = {10.21105/joss.00622},
%    url = {https://doi.org/10.21105/joss.00622},
    year = {2018},
    publisher = {The Open Journal},
    volume = {3},
    number = {25},
    pages = {622},
    journal = {Journal of Open Source Software}
}
@InProceedings{heappe,
    author="Svatoň, Václav
and Martinovič, Jan
and Křenek, Jan
and Esch, Thomas
and Tomančák, Pavel",
    editor="Barolli, Leonard
and Hussain, Farookh Khadeer
and Ikeda, Makoto",
    title="HPC-as-a-Service via HEAppE Platform",
    booktitle="Complex, Intelligent, and Software Intensive Systems",
    year="2020",
    publisher="Springer International Publishing",
    address="Cham",
    pages="280--293",
    %    abstract="The HPC-as-a-Service concept is to provide users with simple and intuitive access to a supercomputing infrastructure without the need to buy and manage their own physical servers or data centers. This article presents the commonly used services and implementations of this concept and introduces our own in-house application framework called High-End Application Execution Middleware (HEAppE Middleware). HEAppE's universally designed software architecture enables unified access to different HPC systems through simple object-oriented web-based APIs, thus providing HPC capabilities to users without the necessity to manage the running jobs forms the command-line interface of the HPC scheduler directly on the cluster. This article also contains the list of several pilot use-cases from a number of thematic domains where the HEAppE Platform was successfully used. Two of those pilots, focusing on satellite image analysis and bioimage informatics, are presented in more detail.",
    isbn="978-3-030-22354-0"
}
@inproceedings{lexis,
    title={HPC, cloud and big-data convergent architectures: The lexis approach},
    author={Scionti, Alberto and Martinovič, Jan and Terzo, Olivier and Walter, Etienne and Levrier, Marc and Hachinger, Stephan and Magarielli, Donato and Goubier, Thierry and Louise, Stephane and Parodi, Antonio and others},
    booktitle={Complex, Intelligent, and Software Intensive Systems: Proceedings of the 13th International Conference on Complex, Intelligent, and Software Intensive Systems (CISIS-2019)},
    pages={200--212},
    year={2020},
    organization={Springer}
}

% Datasets
@dataset{estee_graphs,
    author       = {Jakub Beránek and
                  Stanislav Böhm and
                  Vojtěch Cima},
    title        = {Task graphs for benchmarking schedulers},
    month        = apr,
    year         = 2019,
    publisher    = {Zenodo},
    version      = {1.0},
    doi          = {10.5281/zenodo.2630385},
}
@dataset{estee_results,
    author       = {Jakub Beránek and
                  Stanislav Böhm and
                  Vojtěch Cima},
    title        = {Task Scheduler Performance Survey Results},
    month        = apr,
    year         = 2019,
    publisher    = {Zenodo},
    version      = {1.0},
    doi          = {10.5281/zenodo.2630589},
}
@dataset{airdataset,
    author = {Kalnay et al.},
    title = {The NCEP/NCAR 40-year reanalysis project},
    howpublished = {Bull. Amer. Meteor. Soc., 77, 437-470},
    year = {1996}
}
@dataset{ps_dataset_1,
    author       = {Wingbermühle, Sebastian and
                  Shugaeva, Tatiana and
                  Lindahl, Erik and
                  Lunghini, Filippo and
                  Biswas, Akash Deep and
                  Talarico, Carmine},
    title        = {{Pose Selector Workflow - Structure Input Files for
    Machine Learning (Set 2)}},
    month        = jun,
    year         = 2024,
    publisher    = {Zenodo},
    version      = {1.0},
    doi          = {10.5281/zenodo.11397486},
%    url          = {https://doi.org/10.5281/zenodo.11397486}
}
@dataset{ps_dataset_2,
    author       = {Wingbermühle, Sebastian and
                  Shugaeva, Tatiana and
                  Lindahl, Erik and
                  Lunghini, Filippo and
                  Biswas, Akash Deep and
                  Talarico, Carmine},
    title        = {{Pose Selector Workflow - Docking Poses, Absolute
    Binding Free Energy Estimates and Structure Input
                   Files for Machine Learning}},
    month        = jun,
    year         = 2024,
    publisher    = {Zenodo},
    version      = {1.0},
    doi          = {10.5281/zenodo.11397017},
%    url          = {https://doi.org/10.5281/zenodo.11397017}
}

% PhD tools
@misc{estee_github,
    author = {Jakub Beránek and Ada Böhm and Vojtěch Cima},
    title = {ESTEE},
    year = {2021},
    publisher = {GitHub},
    journal = {GitHub repository},
    howpublished = {\url{https://github.com/it4innovations/estee}},
    commit = {5ffe5f45a0f2d458df0deab3d6a6124ff960f639}
}
@misc{rsds_github,
    author = {Jakub Beránek and Ada Böhm},
    title = {RSDS},
    year = {2020},
    publisher = {GitHub},
    journal = {GitHub repository},
    howpublished = {\url{https://github.com/it4innovations/rsds}},
    commit = {e53891b621e1fface5b3f521cb3bcfefd9a5796c}
}
@misc{hq_github,
    author = {Jakub Beránek and Ada Böhm and Roman Macháček and Vyomkesh Jha},
    title = {HyperQueue},
    year = {2024},
    publisher = {GitHub},
    journal = {GitHub repository},
    howpublished = {\url{https://github.com/It4innovations/hyperqueue}},
    commit = {4a37594fabe54934d2ea5e2a77b204112e29e359}
}
@online{hq_docs,
    author = {Jakub Beránek and Ada Böhm},
    title = {HyperQueue documentation},
    url = {https://it4innovations.github.io/hyperqueue/stable/},
    urldate = {2024-06-22},
}
@online{hq_pypi,
    author = {Jakub Beránek and Ada Böhm},
    title = {HyperQueue Python package at Python Package Index (PyPi)},
    url = {https://pypi.org/project/hyperqueue/},
    urldate = {2024-06-22},
}

% HyperQueue impact
@misc{aiida-hq,
    title = {Aiida HyperQueue integration},
    url = {https://github.com/aiidateam/aiida-hyperqueue},
    urldate = {2024-01-27},
}
@misc{nextflow-hq,
    title = {Nextflow HyperQueue integration},
    url = {https://docs.csc.fi/support/tutorials/nextflow-hq},
    urldate = {2024-01-27},
}
@online{ert,
    title = {Ensemble based Reservoir Tool},
    url = {https://github.com/equinor/ert},
    urldate = {2024-01-27}
}
@article{atlas,
    author = "Aad, G. and others",
    collaboration = "ATLAS",
    title = "{The ATLAS Experiment at the CERN Large Hadron Collider}",
    doi = "10.1088/1748-0221/3/08/S08003",
    journal = "JINST",
    volume = "3",
    pages = "S08003",
    year = "2008"
}
@article{cern-hq,
    author        = "Svatoš, Michal and Chudoba, Jiří and Vokáč, Petr",
    collaboration = "ATLAS",
    title         = "{ARC-CE+HyperQueue based submission system of ATLAS jobs
                       for the Karolina HPC}",
    year          = "2022",
    url           = "https://cds.cern.ch/record/2837871",
}
@article{atlas-it4i-1,
    author = {Svatoš, Michal and Chudoba, Jiří and Vokáč, Petr},
    year = {2019},
    month = {01},
    pages = {03005},
    title = {ATLAS utilisation of the Czech national HPC center},
    volume = {214},
    journal = {EPJ Web of Conferences},
    doi = {10.1051/epjconf/201921403005}
}
@article{atlas-it4i-2,
    author = {Svatoš, Michal and Chudoba, Jiří and Vokáč, Petr},
    year = {2020},
    month = {01},
    pages = {09010},
    title = {Improvements in utilisation of the Czech national HPC center},
    volume = {245},
    journal = {EPJ Web of Conferences},
    doi = {10.1051/epjconf/202024509010}
}
@online{streamflow-hq,
    title = {StreamFlow HyperQueue integration},
    url = {http://gitlab.linksfoundation.com/across-public/orchestrator/components/hyperqueue-streamflow-plugin},
    urldate = {2024-06-11}
}
%@Article{easydock,
%    author="Minibaeva, Guzel
%and Ivanova, Aleksandra
%and Polishchuk, Pavel",
%    title="EasyDock: customizable and scalable docking tool",
%    journal="Journal of Cheminformatics",
%    year="2023",
%    month="Nov",
%    day="01",
%    volume="15",
%    number="1",
%    pages="102",
%    abstract="Docking of large compound collections becomes an important procedure to discover new chemical entities. Screening of large sets of compounds may also occur in de novo design projects guided by molecular docking. To facilitate these processes, there is a need for automated tools capable of efficiently docking a large number of molecules using multiple computational nodes within a reasonable timeframe. These tools should also allow for easy integration of new docking programs and provide a user-friendly program interface to support the development of further approaches utilizing docking as a foundation. Currently available tools have certain limitations, such as lacking a convenient program interface or lacking support for distributed computations. In response to these limitations, we have developed a module called EasyDock. It can be deployed over a network of computational nodes using the Dask library, without requiring a specific cluster scheduler. Furthermore, we have proposed and implemented a simple model that predicts the runtime of docking experiments and applied it to minimize overall docking time. The current version of EasyDock supports popular docking programs, namely Autodock Vina, gnina, and smina. Additionally, we implemented a supplementary feature to enable docking of boron-containing compounds, which are not inherently supported by Vina and smina, and demonstrated its applicability on a set of 55 PDB protein-ligand complexes.",
%    issn="1758-2946",
%    doi="10.1186/s13321-023-00772-2",
%    url="https://doi.org/10.1186/s13321-023-00772-2"
%}
@misc{umbridge,
    title={Democratizing Uncertainty Quantification},
    author={Linus Seelinger and Anne Reinarz and Mikkel B. Lykkegaard and Amal M. A. Alghamdi and David Aristoff and Wolfgang Bangerth and Jean Bénézech and Matteo Diez and Kurt Frey and John D. Jakeman and Jakob S. Jørgensen and Ki-Tae Kim and Massimiliano Martinelli and Matthew Parno and Riccardo Pellegrini and Noemi Petra and Nicolai A. B. Riis and Katherine Rosenfeld and Andrea Serani and Lorenzo Tamellini and Umberto Villa and Tim J. Dodwell and Robert Scheichl},
    year={2024},
    eprint={2402.13768},
    archivePrefix={arXiv}
}
@INPROCEEDINGS{everest,
    author={Pilato, Christian and Bohm, Stanislav and Brocheton, Fabien and Castrillon, Jeronimo and Cevasco, Riccardo and Cima, Vojtech and Cmar, Radim and Diamantopoulos, Dionysios and Ferrandi, Fabrizio and Martinovic, Jan and Palermo, Gianluca and Paolino, Michele and Parodi, Antonio and Pittaluga, Lorenzo and Raho, Daniel and Regazzoni, Francesco and Slaninova, Katerina and Hagleitner, Christoph},
    booktitle={2021 Design, Automation \& Test in Europe Conference \& Exhibition (DATE)},
    title={EVEREST: A design environment for extreme-scale big data analytics on heterogeneous platforms},
    year={2021},
    volume={},
    number={},
    pages={1320-1325},
%    keywords={Runtime;Decision making;Distributed databases;Big Data;Programming;Hardware;Data mining},
    doi={10.23919/DATE51398.2021.9473940}
}
@inproceedings{across,
    author = {Aldinucci, Marco and Agosta, Giovanni and Andreini, Antonio and Ardagna, Claudio A. and Bartolini, Andrea and Cilardo, Alessandro and Cosenza, Biagio and Danelutto, Marco and Esposito, Roberto and Fornaciari, William and Giorgi, Roberto and Lengani, Davide and Montella, Raffaele and Olivieri, Mauro and Saponara, Sergio and Simoni, Daniele and Torquati, Massimo},
    title = {The Italian research on HPC key technologies across EuroHPC},
    year = {2021},
    isbn = {9781450384049},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    doi = {10.1145/3457388.3458508},
%    booktitle = {Proceedings of the 18th ACM International Conference on Computing Frontiers},
    pages = {178–184},
    numpages = {7},
    location = {Virtual Event, Italy},
    series = {CF '21}
}
@online{exa4mind,
    title = {Extreme Analytics for Mining Data spaces},
    year = {2023},
    url = {https://exa4mind.eu/},
    urldate = {2024-07-12},
}
@online{max,
    title = {MaX (MAterials design at the eXascale) Centre of Excellence Project},
    year = {2023},
    url = {https://www.max-centre.eu/},
    urldate = {2024-06-01},
}
@online{it4i-lumi,
    title = {HyperQueue facilitates better utilization of LUMI’s computational resources},
    url = {https://www.lumi-supercomputer.eu/hyperqueue-facilitates-better-utilization-of-lumis-computational-resources},
    urldate = {2024-01-27},
}
@online{it4i-hq,
    title = {HyperQueue documentation for the IT4Innovations cluster},
    url = {https://docs.it4i.cz/general/hyperqueue},
    urldate = {2024-01-27},
}
@online{cineca,
    title = {HPC Cineca website},
    url = {https://www.hpc.cineca.it},
    urldate = {2024-06-01},
}
@online{puhti-hq,
    title = {High-throughput computing and workflows on the Puhti cluster},
    url = {https://docs.csc.fi/computing/running/throughput/},
    urldate = {2024-07-24},
}
@misc{ps-workflow,
    author = {Sebastian Wingbermühle},
    title = {LIGATE Pose Selector Workflow},
    year = {2024},
    publisher = {GitHub},
    journal = {GitHub repository},
    howpublished = {\url{https://github.com/LigateProject/Pose-Selector-workflow}},
    commit = {5b717af244280b755207df3d0fec4147324e057b}
}
@misc{cadd-workflow,
    author = {Jakub Beránek and Sebastian Wingbermühle and Davide Gadioli},
    title = {LIGATE CADD and Virtual Screening Workflow},
    year = {2024},
    publisher = {GitHub},
    journal = {GitHub repository},
    howpublished = {\url{https://github.com/LigateProject/Computer-Aided-Drug-Design-workflow-HyperQueue}},
    commit = {7051916db3affb2eb8b0e5cf9a932cea24cc0ead}
}
@online{heappe_hq,
    title = {HEAppE HyperQueue integration},
    url = {https://heappe.it4i.cz/docs/4-2-0/pages/hyperqueue.html},
    urldate = {2024-07-29},
}
