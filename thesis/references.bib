@inproceedings{wrf,
    title = {The {Weather} {Reseach} and {Forecast} {Model}: {Software} {Architecture} and {Performance}},
    author = {Michalakes, John and Dudhia, Jimy and Gill, D. and Henderson, Tom and Klemp, J. and Skamarock, W. and Wang, Wei},
    month = jan,
    year = {2004},
}
@inproceedings{cfd,
    title={CFD Vision 2030 Study: A Path to Revolutionary Computational Aerosciences},
    author={Jeffrey P. Slotnick and Abdollah Khodadoust and Juan J. Alonso and David L. Darmofal and William Gropp and Elizabeth A. Lurie and Dimitri J. Mavriplis},
    year={2014}
}
@article{hpcdl,
    author = {Ben-Nun, Tal and Hoefler, Torsten},
    title = {Demystifying Parallel and Distributed Deep Learning: An In-Depth Concurrency Analysis},
    year = {2019},
    issue_date = {July 2020},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {52},
    number = {4},
    issn = {0360-0300},
%    url = {https://doi.org/10.1145/3320060},
    doi = {10.1145/3320060},
%    abstract = {Deep Neural Networks (DNNs) are becoming an important tool in modern computing applications. Accelerating their training is a major challenge and techniques range from distributed algorithms to low-level circuit design. In this survey, we describe the problem from a theoretical perspective, followed by approaches for its parallelization. We present trends in DNN architectures and the resulting implications on parallelization strategies. We then review and model the different types of concurrency in DNNs: from the single operator, through parallelism in network inference and training, to distributed deep learning. We discuss asynchronous stochastic optimization, distributed system architectures, communication schemes, and neural architecture search. Based on those approaches, we extrapolate potential directions for parallelism in deep learning.},
    journal = {ACM Comput. Surv.},
    month = aug,
    articleno = {65},
    numpages = {43},
%    keywords = {parallel algorithms, Deep learning, distributed computing}
}
@InProceedings{bioinformatics,
    author="Per{\'e}z-S{\'a}nchez, Horacio
and Fassihi, Afshin
and Cecilia, Jos{\'e} M.
and Ali, Hesham H.
and Cannataro, Mario",
    editor="Ortu{\~{n}}o, Francisco
and Rojas, Ignacio",
    title="Applications of High Performance Computing in Bioinformatics, Computational Biology and Computational Chemistry",
    booktitle="Bioinformatics and Biomedical Engineering",
    year="2015",
    publisher="Springer International Publishing",
    address="Cham",
    pages="527--541",
%    abstract="In the last 10 years, we are witnessing one of the major revolutions in parallel systems. The consolidation of heterogeneous systems at different levels -from desktop computers to large-scale systems such as supercomputers, clusters or grids, through all kinds of low-power devices- is providing a computational power unimaginable just few years ago, trying to follow the wake of Moore's law. This landscape in the high performance computing arena opens up great opportunities in the simulation of relevant biological systems and for applications in Bioinformatics, Computational Biology and Computational Chemistry. This introductory article shows the last tendencies of this active research field and our perspectives for the forthcoming years.",
    isbn="978-3-319-16480-9"
}
@Inbook{memorywall,
    author="McKee, Sally A.
and Wisniewski, Robert W.",
    editor="Padua, David",
    title="Memory Wall",
    bookTitle="Encyclopedia of Parallel Computing",
    year="2011",
    publisher="Springer US",
    address="Boston, MA",
    pages="1110--1116",
    isbn="978-0-387-09766-4",
    doi="10.1007/978-0-387-09766-4_234",
%    url="https://doi.org/10.1007/978-0-387-09766-4_234"
}
@Inbook{powerwall,
    author="Bose, Pradip",
    editor="Padua, David",
    title="Power Wall",
    bookTitle="Encyclopedia of Parallel Computing",
    year="2011",
    publisher="Springer US",
    address="Boston, MA",
    pages="1593--1608",
    isbn="978-0-387-09766-4",
    doi="10.1007/978-0-387-09766-4_499",
%    url="https://doi.org/10.1007/978-0-387-09766-4_499"
}
@article{xeonphi,
    title={Knights Landing: Second-Generation Intel Xeon Phi Product},
    author={Avinash Sodani and Roger Gramunt and Jes{\'u}s Corbal and Ho-Seop Kim and Krishna Vinod and Sundaram Chinthamani and Steven Hutsell and Rajat Agarwal and Yen-Chen Liu},
    journal={IEEE Micro},
    year={2016},
    volume={36},
    pages={34-46}
}
@inproceedings{mpistudy,
    author = {Laguna, Ignacio and Marshall, Ryan and Mohror, Kathryn and Ruefenacht, Martin and Skjellum, Anthony and Sultana, Nawrin},
    title = {A Large-Scale Study of MPI Usage in Open-Source HPC Applications},
    year = {2019},
    isbn = {9781450362290},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    doi = {10.1145/3295500.3356176},
    booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
    articleno = {31},
    numpages = {14},
    location = {Denver, Colorado},
    series = {SC '19}
}
@inproceedings{dace,
    author = {Ben-Nun, Tal and de Fine Licht, Johannes and Ziogas, Alexandros N. and Schneider, Timo and Hoefler, Torsten},
    title = {Stateful Dataflow Multigraphs: A Data-Centric Model for Performance Portability on Heterogeneous Architectures},
    year = {2019},
    isbn = {9781450362290},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
%    url = {https://doi.org/10.1145/3295500.3356173},
    doi = {10.1145/3295500.3356173},
%    abstract = {The ubiquity of accelerators in high-performance computing has driven programming complexity beyond the skill-set of the average domain scientist. To maintain performance portability in the future, it is imperative to decouple architecture-specific programming paradigms from the underlying scientific computations. We present the Stateful DataFlow multiGraph (SDFG), a data-centric intermediate representation that enables separating program definition from its optimization. By combining fine-grained data dependencies with high-level control-flow, SDFGs are both expressive and amenable to program transformations, such as tiling and double-buffering. These transformations are applied to the SDFG in an interactive process, using extensible pattern matching, graph rewriting, and a graphical user interface. We demonstrate SDFGs on CPUs, GPUs, and FPGAs over various motifs --- from fundamental computational kernels to graph analytics. We show that SDFGs deliver competitive performance, allowing domain scientists to develop applications naturally and port them to approach peak hardware performance without modifying the original scientific code.},
    booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
    articleno = {81},
    numpages = {14},
    location = {Denver, Colorado},
    series = {SC '19}
}
@Article{starpu,
    author = {C{\'e}dric Augonnet and Samuel Thibault and Raymond Namyst and Pierre-Andr{\'e} Wacrenier},
    title = {{StarPU: A Unified Platform for Task Scheduling on Heterogeneous Multicore Architectures}},
    journal = {CCPE - Concurrency and Computation: Practice and Experience, Special Issue: Euro-Par 2009},
    volume = 23,
    issue = 2,
    pages = {187--198},
    year = 2011,
    month = FEB,
    publisher = {John Wiley & Sons, Ltd.},
    doi = {10.1002/cpe.1631},
    url = {http://hal.inria.fr/inria-00550877},
    pdf = {http://hal.inria.fr/inria-00550877/document},
    KEYWORDS = {General Presentations;StarPU}
}
@book{openmp,
    title={Parallel programming in OpenMP},
    author={Chandra, Rohit and Dagum, Leo and Kohr, David and Menon, Ramesh and Maydan, Dror and McDonald, Jeff},
    year={2001},
    publisher={Morgan kaufmann}
}
@ARTICLE{mooreslaw,
    author={Schaller, R.R.},
    journal={IEEE Spectrum},
    title={Moore's law: past, present and future},
    year={1997},
    volume={34},
    number={6},
    pages={52-59},
    doi={10.1109/6.591665}
}
@online{top500gpu,
    author={Feldman, Michael},
    url={https://www.top500.org/news/new-gpu-accelerated-supercomputers-change-the-balance-of-power-on-the-top500/},
    urldate = {2023-04-21},
}
@inproceedings{top500analysis,
    author = {Khan, Awais and Sim, Hyogi and Vazhkudai, Sudharshan S. and Butt, Ali R. and Kim, Youngjae},
    title = {An Analysis of System Balance and Architectural Trends Based on Top500 Supercomputers},
    year = {2021},
    isbn = {9781450388429},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3432261.3432263},
    doi = {10.1145/3432261.3432263},
%    abstract = {Supercomputer design is a complex, multi-dimensional optimization process, wherein several subsystems need to be reconciled to meet a desired figure of merit performance for a portfolio of applications and a budget constraint. However, overall, the HPC community has been gravitating towards ever more Flops, at the expense of many other subsystems. To draw attention to overall system balance, in this paper, we analyze balance ratios and architectural trends in the world’s most powerful supercomputers. Specifically, we have collected the performance characteristics of systems between 1993 and 2019 based on the Top500 lists and then analyzed their architectures from diverse system design perspectives. Notably, our analysis studies the performance balance of the machines, across a variety of subsystems such as compute, memory, I/O, interconnect, intra-node connectivity and power. Our analysis reveals that balance ratios of the various subsystems need to be considered carefully alongside the application workload portfolio to provision the subsystem capacity and bandwidth specifications, which can help achieve optimal performance.},
    booktitle = {The International Conference on High Performance Computing in Asia-Pacific Region},
    pages = {11–22},
    numpages = {12},
    %    keywords = {Architectural Trends and Performance Balance Ratio, High Performance Computing, Top500 Supercomputers},
    location = {Virtual Event, Republic of Korea},
    series = {HPC Asia 2021}
}
@online{dask-client-perf,
    title = {Task Graph Building Performance Issue in Dask},
    url = {https://github.com/dask/distributed/issues/3783},
    urldate = {2022-05-25},
}
@online{dask-user-survey,
    title = {2021 Dask User Survey},
    author = {Genevieve Buckley},
    url = {https://blog.dask.org/2021/09/15/user-survey},
    urldate = {2022-06-09},
}
@online{it4i_node_scheduling_policy,
    title = {IT4Innovations Job Submission Policy},
    url = {https://docs.it4i.cz/general/resource_allocation_and_job_execution/\#job-submission-and-execution},
    urldate = {2023-04-28},
}
@article{graph_partitioning,
    author="Feldmann, Andreas Emil and Foschini, Luca",
    title="Balanced Partitions of Trees and Applications",
    journal="Algorithmica",
    year="2015",
    month="Feb",
    day="01",
    volume="71",
    number="2",
    pages="354--376",
%    abstract="We study the problem of finding the minimum number of edges that, when cut, form a partition of the vertices into k sets of equal size. This is called the k-BALANCED PARTITIONING problem. The problem is known to be inapproximable within any finite factor on general graphs, while little is known about restricted graph classes.",
    issn="1432-0541",
    doi="10.1007/s00453-013-9802-3",
%    url="https://doi.org/10.1007/s00453-013-9802-3"
}

% Technologies
@techreport{mpi,
    author = {Forum, Message P},
    title = {MPI: A Message-Passing Interface Standard},
    year = {1994},
    publisher = {University of Tennessee},
    address = {USA},
%    abstract = {The Message Passing Interface Forum (MPIF), with participation from over 40 organizations, has been meeting since November 1992 to discuss and define a set of library standards for message passing. MPIF is not sanctioned or supported by any official standards organization. The goal of the Message Passing Interface, simply stated, is to develop a widely used standard for writing message-passing programs. As such the interface should establish a practical, portable, efficient and flexible standard for message passing. , This is the final report, Version 1.0, of the Message Passing Interface Forum. This document contains all the technical features proposed for the interface. This copy of the draft was processed by LATEX on April 21, 1994. , Please send comments on MPI to mpi-comments@cs.utk.edu. Your comment will be forwarded to MPIF committee members who will attempt to respond.}
}
@Inbook{pgas,
    author="Almasi, George",
    editor="Padua, David",
    title="PGAS (Partitioned Global Address Space) Languages",
    bookTitle="Encyclopedia of Parallel Computing",
    year="2011",
    publisher="Springer US",
    address="Boston, MA",
    pages="1539--1545",
    isbn="978-0-387-09766-4",
    doi="10.1007/978-0-387-09766-4_210",
%    url="https://doi.org/10.1007/978-0-387-09766-4_210"
}

% Job managers
@InProceedings{slurm,
    author = "Yoo, Andy B.
and Jette, Morris A.
and Grondona, Mark",
    doi = "10.1007/10968987_3",
    editor = "Feitelson, Dror
and Rudolph, Larry
and Schwiegelshohn, Uwe",
    title = "SLURM: Simple Linux Utility for Resource Management",
    booktitle = "Job Scheduling Strategies for Parallel Processing",
    year = "2003",
    publisher = "Springer Berlin Heidelberg",
    address = "Berlin, Heidelberg",
    pages = "44--60",
%    abstract = "A new cluster resource management system called Simple Linux Utility Resource Management (SLURM) is described in this paper. SLURM, initially developed for large Linux clusters at the Lawrence Livermore National Laboratory (LLNL), is a simple cluster manager that can scale to thousands of processors. SLURM is designed to be flexible and fault-tolerant and can be ported to other clusters of different size and architecture with minimal effort. We are certain that SLURM will benefit both users and system architects by providing them with a simple, robust, and highly scalable parallel job execution environment for their cluster system.",
    isbn = "978-3-540-39727-4"
}
@inproceedings{pbs,
    author = {Staples, Garrick},
    title = {TORQUE Resource Manager},
    year = {2006},
    isbn = {0769527000},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    %    url = {https://doi.org/10.1145/1188455.1188464},
    doi = {10.1145/1188455.1188464},
%    abstract = {With TORQUE Resource Manager now reaching over 10,000 downloads per month and use across thousands of leading sites representing commercial, government, and academic organizations, we invite all TORQUE users to meet and discuss TORQUE with the professional developers, community volunteers other members who use and have contributed to the TORQUE project.Here we will discuss the current state of TORQUE including some of the more recent enhancements and capabilities along with the road map for the upcoming year. We will also provide a time for TORQUE users to share experiences, best practices, and new needs.},
    booktitle = {Proceedings of the 2006 ACM/IEEE Conference on Supercomputing},
    pages = {8–es},
    location = {Tampa, Florida},
    series = {SC '06}
}
@online{slurm-schedmd,
    title = {SchedMD Slurm usage statistics},
    url = {https://schedmd.com/},
    urldate = {2022-05-18}
}

% Scheduling
@article{Ullman1975,
    author = {Ullman, J. D.},
    title = {NP-complete Scheduling Problems},
    journal = {J. Comput. Syst. Sci.},
    issue_date = {June, 1975},
    volume = {10},
    number = {3},
    month = jun,
    year = {1975},
    issn = {0022-0000},
    pages = {384--393},
    numpages = {10},
    %    url = {http://dx.doi.org/10.1016/S0022-0000(75)80008-0},
    doi = {10.1016/S0022-0000(75)80008-0},
    acmid = {1740138},
    publisher = {Academic Press, Inc.},
    address = {Orlando, FL, USA},
}
@article{hlfet1974,
    author = {Adam, Thomas L. and Chandy, K. M. and Dickson, J. R.},
    title = {A Comparison of List Schedules for Parallel Processing Systems},
    journal = {Commun. ACM},
    issue_date = {Dec 1974},
    volume = {17},
    number = {12},
    month = dec,
    year = {1974},
    issn = {0001-0782},
    pages = {685--690},
    numpages = {6},
    %    url = {http://doi.acm.org/10.1145/361604.361619},
    doi = {10.1145/361604.361619},
    acmid = {361619},
    publisher = {ACM},
    address = {New York, NY, USA},
}
@inproceedings{kwok1998benchmarking,
    title = {Benchmarking the task graph scheduling algorithms},
    author = {Kwok, Yu-Kwong and Ahmad, Ishfaq},
    booktitle = {ipps},
    pages = {0531},
    year = {1998},
    organization = {IEEE}
}
@article{hagras2003static,
    title = {Static vs. dynamic list-scheduling performance comparison},
    author = {Hagras, Tarek and Jane{\v{c}}ek, J},
    journal = {Acta Polytechnica},
    volume = {43},
    number = {6},
    year = {2003}
}
@article{wang2018list,
    title = {List-Scheduling vs. Cluster-Scheduling},
    author = {Wang, Huijun and Sinnen, Oliver},
    journal = {IEEE Transactions on Parallel and Distributed Systems},
    year = {2018},
    publisher = {IEEE}
}

% Distributed tools
@inproceedings{dask,
    author = {Rocklin, Matthew},
    year = {2015},
    month = {01},
    pages = {126-132},
    title = {Dask: Parallel Computation with Blocked algorithms and Task Scheduling},
    doi = {10.25080/Majora-7b98e3ed-013}
}
@article{snakemake,
    author = {Köster, Johannes and Rahmann, Sven},
    title = "{Snakemake—a scalable bioinformatics workflow engine}",
    journal = {Bioinformatics},
    volume = {28},
    number = {19},
    pages = {2520-2522},
    year = {2012},
    month = {08},
    abstract = "{Summary: Snakemake is a workflow engine that provides a readable Python-based workflow definition language and a powerful execution environment that scales from single-core workstations to compute clusters without modifying the workflow. It is the first system to support the use of automatically inferred multiple named wildcards (or variables) in input and output filenames.Availability:http://snakemake.googlecode.com.Contact:johannes.koester@uni-due.de}",
    issn = {1367-4803},
    doi = {10.1093/bioinformatics/bts480},
    %    url = "https://doi.org/10.1093/bioinformatics/bts480",
    eprint = {https://academic.oup.com/bioinformatics/article-pdf/28/19/2520/819790/bts480.pdf},
}
@article{nextflow,
    author = {Di Tommaso, Paolo and Chatzou, Maria and Floden, Evan W. and Barja, Pablo and Palumbo, Emilio and Notredame, Cedric},
    year = {2017},
    month = {04},
    pages = {316-319},
    title = {Nextflow enables reproducible computational workflows},
    volume = {35},
    journal = {Nature Biotechnology},
    doi = {10.1038/nbt.3820}
}
@article{aiida,
    author="Huber, Sebastiaan P.
and Zoupanos, Spyros
and Uhrin, Martin
and Talirz, Leopold
and Kahle, Leonid
and H{\"a}uselmann, Rico
and Gresch, Dominik
and M{\"u}ller, Tiziano
and Yakutovich, Aliaksandr V.
and Andersen, Casper W.
and Ramirez, Francisco F.
and Adorf, Carl S.
and Gargiulo, Fernando
and Kumbhar, Snehal
and Passaro, Elsa
and Johnston, Conrad
and Merkys, Andrius
and Cepellotti, Andrea
and Mounet, Nicolas
and Marzari, Nicola
and Kozinsky, Boris
and Pizzi, Giovanni",
    title="AiiDA 1.0, a scalable computational infrastructure for automated reproducible workflows and data provenance",
    journal="Scientific Data",
    year="2020",
    month="Sep",
    day="08",
    volume="7",
    number="1",
    pages="300",
    abstract="The ever-growing availability of computing power and the sustained development of advanced computational methods have contributed much to recent scientific progress. These developments present new challenges driven by the sheer amount of calculations and data to manage. Next-generation exascale supercomputers will harden these challenges, such that automated and scalable solutions become crucial. In recent years, we have been developing AiiDA (aiida.net), a robust open-source high-throughput infrastructure addressing the challenges arising from the needs of automated workflow management and data provenance recording. Here, we introduce developments and capabilities required to reach sustained performance, with AiiDA supporting throughputs of tens of thousands processes/hour, while automatically preserving and storing the full data provenance in a relational database making it queryable and traversable, thus enabling high-performance data analytics. AiiDA's workflow language provides advanced automation, error handling features and a flexible plugin model to allow interfacing with external simulation software. The associated plugin registry enables seamless sharing of extensions, empowering a vibrant user community dedicated to making simulations more robust, user-friendly and reproducible.",
    issn="2052-4463",
    doi="10.1038/s41597-020-00638-4",
%    url="https://doi.org/10.1038/s41597-020-00638-4"
}
@article{streamflow,
    abstract = {Workflows are among the most commonly used tools in a variety of execution environments. Many of them target a specific environment; few of them make it possible to execute an entire workflow in different environments, e.g. Kubernetes and batch clusters. We present a novel approach to workflow execution, called StreamFlow, that complements the workflow graph with the declarative description of potentially complex execution environments, and that makes it possible the execution onto multiple sites not sharing a common data space. StreamFlow is then exemplified on a novel bioinformatics pipeline for single cell transcriptomic data analysis workflow.},
    author = {Iacopo Colonnelli and Barbara Cantalupo and Ivan Merelli and Marco Aldinucci},
    doi = {10.1109/TETC.2020.3019202},
    journal = {{IEEE} {T}ransactions on {E}merging {T}opics in {C}omputing},
    number = {4},
    pages = {1723--1737},
    title = {{StreamFlow}: cross-breeding cloud with {HPC}},
    volume = {9},
    year = {2021}
}
@article{gromacs,
    author="Lindahl, Erik
and Hess, Berk
and van der Spoel, David",
    title="GROMACS 3.0: a package for molecular simulation and trajectory analysis",
    journal="Molecular modeling annual",
    year="2001",
    month="Aug",
    day="01",
    volume="7",
    number="8",
    pages="306--317",
%    abstract="GROMACS 3.0 is the latest release of a versatile and very well optimized package for molecular simulation. Much effort has been devoted to achieving extremely high performance on both workstations and parallel computers. The design includes an extraction of virial and periodic boundary conditions from the loops over pairwise interactions, and special software routines to enable rapid calculation of x--1/2. Inner loops are generated automatically in C or Fortran at compile time, with optimizations adapted to each architecture. Assembly loops using SSE and 3DNow! Multimedia instructions are provided for x86 processors, resulting in exceptional performance on inexpensive PC workstations. The interface is simple and easy to use (no scripting language), based on standard command line arguments with self-explanatory functionality and integrated documentation. All binary files are independent of hardware endian and can be read by versions of GROMACS compiled using different floating-point precision. A large collection of flexible tools for trajectory analysis is included, with output in the form of finished Xmgr/Grace graphs. A basic trajectory viewer is included, and several external visualization tools can read the GROMACS trajectory format. Starting with version 3.0, GROMACS is available under the GNU General Public License from http://www.gromacs.org.",
    issn="0948-5023",
    doi="10.1007/s008940100045",
%    url="https://doi.org/10.1007/s008940100045"
}
@inproceedings{hyperloom,
    author = {Cima, Vojtěch and Böhm, Stanislav and Martinovič, Jan and Dvorský, Jiří and Janurová, Kateřina and Aa, Tom Vander and Ashby, Thomas J. and Chupakhin, Vladimir},
    title = {HyperLoom: A Platform for Defining and Executing Scientific Pipelines in Distributed Environments},
    year = {2018},
    isbn = {9781450364447},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3183767.3183768},
    doi = {10.1145/3183767.3183768},
    pages = {1–6},
    numpages = {6},
    %    keywords = {Chemogenomics, Distributed Computing, Task Scheduling, Scientific Pipeline, Big Data, HPC, Machine Learning},
    series = {PARMA-DITAM '18}
}
@inproceedings{ray,
    author = {Moritz, Philipp and Nishihara, Robert and Wang, Stephanie and Tumanov, Alexey and Liaw, Richard and Liang, Eric and Elibol, Melih and Yang, Zongheng and Paul, William and Jordan, Michael I. and Stoica, Ion},
    title = {Ray: A Distributed Framework for Emerging AI Applications},
    year = {2018},
    isbn = {9781931971478},
    publisher = {USENIX Association},
    address = {USA},
    abstract = {The next generation of AI applications will continuously interact with the environment and learn from these interactions. These applications impose new and demanding systems requirements, both in terms of performance and flexibility. In this paper, we consider these requirements and present Ray--a distributed system to address them. Ray implements a unified interface that can express both task-parallel and actor-based computations, supported by a single dynamic execution engine. To meet the performance requirements, Ray employs a distributed scheduler and a distributed and fault-tolerant store to manage the system's control state. In our experiments, we demonstrate scaling beyond 1.8 million tasks per second and better performance than existing specialized systems for several challenging reinforcement learning applications.},
    booktitle = {Proceedings of the 13th USENIX Conference on Operating Systems Design and Implementation},
    pages = {561–577},
    numpages = {17},
    location = {Carlsbad, CA, USA},
    series = {OSDI'18}
}
@inproceedings{parsl,
    author = {Babuji, Yadu and Woodard, Anna and Li, Zhuozhao and Katz, Daniel S. and Clifford, Ben and Kumar, Rohan and Lacinski, Lukasz and Chard, Ryan and Wozniak, Justin M. and Foster, Ian and Wilde, Michael and Chard, Kyle},
    title = {Parsl: Pervasive Parallel Programming in Python},
    year = {2019},
    isbn = {9781450366700},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    %    url = {https://doi.org/10.1145/3307681.3325400},
    doi = {10.1145/3307681.3325400},
    abstract = {High-level programming languages such as Python are increasingly used to provide intuitive interfaces to libraries written in lower-level languages and for assembling applications from various components. This migration towards orchestration rather than implementation, coupled with the growing need for parallel computing (e.g., due to big data and the end of Moore's law), necessitates rethinking how parallelism is expressed in programs. Here, we present Parsl, a parallel scripting library that augments Python with simple, scalable, and flexible constructs for encoding parallelism. These constructs allow Parsl to construct a dynamic dependency graph of components that it can then execute efficiently on one or many processors. Parsl is designed for scalability, with an extensible set of executors tailored to different use cases, such as low-latency, high-throughput, or extreme-scale execution. We show, via experiments on the Blue Waters supercomputer, that Parsl executors can allow Python scripts to execute components with as little as 5 ms of overhead, scale to more than 250000 workers across more than 8000 nodes, and process upward of 1200 tasks per second. Other Parsl features simplify the construction and execution of composite programs by supporting elastic provisioning and scaling of infrastructure, fault-tolerant execution, and integrated wide-area data management. We show that these capabilities satisfy the needs of many-task, interactive, online, and machine learning applications in fields such as biology, cosmology, and materials science.},
    booktitle = {Proceedings of the 28th International Symposium on High-Performance Parallel and Distributed Computing},
    pages = {25–36},
    numpages = {12},
    location = {Phoenix, AZ, USA},
    series = {HPDC '19}
}
@article{pycompss,
    author = {Tejedor, Enric and Becerra, Yolanda and Alomar, Guillem and Queralt, Anna and Badia, Rosa M. and Torres, Jordi and Cortes, Toni and Labarta, Jesús},
    year = {2015},
    month = {08},
    pages = {},
    title = {PyCOMPSs: Parallel computational workflows in Python},
    volume = {31},
    journal = {International Journal of High Performance Computing Applications},
    doi = {10.1177/1094342015594678}
}
