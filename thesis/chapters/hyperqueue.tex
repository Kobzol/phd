The previous two chapters focused primarily on the performance aspects of task graph execution, by
examining various task scheduling algorithms and also bottlenecks that can limit the scalability of
existing task runtimes. Performance is naturally crucial for \gls{hpc}, yet it should
not be the only factor to focus on. \Autoref{ch:sota} discussed several other challenges
affecting \gls{hpc} task graphs, that are related to the second main focus of this
thesis, namely the \emph{ergonomics} of task graph execution.

Execution ergonomics (in the context of task-based programming models) is a broad area that
encompasses several aspects, such as providing an easy way to define the structure of the task
graph, allowing its execution in an effortless manner (both on a local computer and a distributed
cluster), handling fault tolerance without the user's intervention, allowing users to express
complex resource requirements and many others. This thesis primarily focuses on analyzing and
resolving ergonomic challenges that form a barrier for executing task graphs on
\gls{hpc} clusters. As we will see in this chapter, ergonomics and performance do not
have to contradict each other; approaches that increase ergonomics can also help with improving
performance and hardware utilization.

One approach to improve ergonomics could be to make incremental improvements that would mitigate
the mentioned issues in off-the-shelf task runtimes. However, as we will see later in this chapter,
ergonomic areas such as supporting heterogeneous tasks and clusters and resource requirements,
providing fault tolerance and dynamic load balancing and overcoming allocation manager limits are
all intertwined, and they affect each other in non-trivial ways. Therefore, this chapter introduces
a holistic meta-scheduling design that aims to tackle the most important challenges with a unified
solution, in order to enable effortless, efficient and fault-tolerant execution of task graphs on
heterogeneous \gls{hpc} clusters in the presence of allocation managers.

This reference design has been implemented in \hyperqueue{}, an
\gls{hpc}-tailored task runtime that was designed to enable transparent, ergonomic
and efficient execution of task graphs on \gls{hpc} systems. \hyperqueue{}
is a spiritual successor of \rsds{}, and builds upon its server and task scheduler
implementation; however, it does not use the \dask{} interface. It is the
culmination of the research and work presented in this thesis, which was made possible thanks to
the experience gained from \estee{} and \rsds{}, and also from
interacting with many \gls{hpc} workflows and use-cases over the course of several
years.

The work presented in this chapter makes the following contributions:
\begin{enumerate}
	\item We propose a meta-scheduling design that provides a unified way of scheduling and executing tasks
	      in the presence of allocation managers and allows load balancing across different allocations. It
	      disentangles the definition of tasks and the hardware that executes them, making it flexible and
	      easy to use.
	\item We describe an approach that enables a task runtime to automatically submit allocations on behalf
	      of the user in response to computational load, which further simplifies the execution of task
	      graphs on supercomputers.
	\item We provide a task scheduler with complex resource management that supports non-trivial features
	      that are not present in other state-of-the-art tools, such as fractional resources or resource
	      variants.
	\item We evaluate the performance and hardware utilization of the proposed design on several benchmarks
	      and use-cases.
\end{enumerate}

This chapter first introduces the general meta-scheduling design for mapping tasks to allocations.
Then it describes the architecture of \hyperqueue{}, which implements the proposed
design, and presents how its features alleviate the challenges described
in~\Autoref{ch:sota}. Finally, it provides a performance analysis of \hyperqueue{},
evaluates it on several use-cases and compares its features with other state-of-the-art
meta-scheduling task runtimes.

We have described the design and key ideas of \hyperqueue{} in
\emph{HyperQueue: efficient and ergonomic task graphs on HPC clusters}~\cite{hyperqueue}. The architecture of \hyperqueue{}
descriptions presented in this chapter was adapted from this publication.

\workshare{I have collaborated on this work with Ada BÃ¶hm; we have both contributed to it equally. I am the
sole author of the design and implementation of the automatic allocator
component, and I have also significantly contributed to the design and implementation of most remaining parts
of \hyperqueue{}. I have designed and performed all the experiments described in this
chapter. While Ada and I are the primary contributors to \hyperqueue{}, it should be noted that several other people have contributed to it, as its development is a team effort. Source code contribution statistics for
\hyperqueue{} can be found on GitHub\footnoteurl{https://github.com/it4innovations/hyperqueue/graphs/contributors}.}

\section{Interaction with allocation managers}
The primary challenge that affects the simplicity of task graph execution on
\gls{hpc} clusters is the presence of an allocation manager, as it forms a barrier;
on such clusters, it is not possible to compute anything at all without interacting with
allocations. In order for task graph execution to become truly ergonomic, there is no point in
tackling the rest of the (ergonomic) challenges until there is a straightforward way of executing
task graphs through an allocation manager.

\Autoref{challenge:allocation-manager} has described several approaches that can be used to map tasks to
allocations, which is necessary to execute task graphs, as well as their various shortcomings. Due
to the limits imposed by allocation managers, it might be necessary to partition task graphs into
multiple subgraphs in order to fit them within an allocation, which can be challenging.
Furthermore, it can result in non-optimal usage of hardware resources, because tasks from
(sub)graphs submitted in separate allocations will only be load balanced within their own
allocation, and not across different allocations.

It is important to understand what is uniquely challenging about the interaction with allocations.
The fact that the computation needs to go through a queue is not an issue by itself, as we are
dealing with batch processing anyway. Therefore, some form of a delay between submitting the task
graph and receiving the results is expected, even if we did not have to submit allocations at all.

The main issue of allocations is that they strictly tie together two separate aspects;
\emph{what} the user wants to compute (the computation performed once the allocation
starts) and \emph{where} should the computation take place (specific hardware resources
and computational nodes reserved for the allocation). As was already described previously, both of
these things have to be specified together in an allocation. This is a very inflexible design, for
several reasons:

\begin{itemize}
	\item The user needs to consider both aspects at the same time. Ultimately, the main thing that the user
	      cares about is what they want to compute. With allocations, they also need to think about specific
	      hardware resources that should be allocated for computing their task graph, which can be
	      challenging.
	\item Hardware resources are allocated for the whole duration of the allocation. This can result in
	      inefficient resource usage, especially for heterogeneous task graphs that consist of many types of
	      tasks that use different resources. In situations where not all resources can be used at the same
	      time, some of the resources can unnecessarily remain idle. For example, consider a task graph
	      executed inside an allocation that provides two computational nodes. Tasks will be load balanced
	      among these two nodes, but towards the end of the computation, there might be tasks that will take
	      a long time to finish. This might lead to a situation where only one of the nodes will be computing
	      the long tail of the remaining tasks, while the second node will be idle. Due to the fact that the
	      allocation cannot release the second node while the computation is still ongoing, the user
	      pointlessly pays the cost of both nodes until the task graph finishes computing, even though the
	      second node is not doing any useful work.

	\item The amount of used resources has to be decided up front, new hardware resources cannot be added or
	      removed from existing allocations. This can cause inefficiencies, as described above, but it also
	      limits load balancing. If balancing only happens within (and not across) allocations, users cannot
	      easily provide new hardware resources during the computation, if they realize that it is
	      progressing too slowly.
	\item Granularity of the allocated resources might not match the granularity of tasks. Unless the
	      allocation manager supports very fine-grained allocations (e.g.\ on the level of individual
	      \gls{cpu} cores, rather than whole computational nodes), there can be a large gap
	      between the resources required by a task graph and the resources provided to an individual
	      allocation. This can again lead to resources sitting idle.
	\item Binding computation with specific hardware resources up front complicates handling task failures.
	      When the user submits thousands of tasks on a specific set of hardware resources, and some of these
	      tasks fail, the user might need to create a new allocation to recompute the failed tasks. This
	      requires once again figuring out a new set of hardware resources that should be requested, as the
	      original resources will probably not be a good fit for just a subset of the original computation. A
	      fault-tolerant design would ideally allow recomputing tasks on any compatible hardware resource
	      transparently, which is incompatible with forcing computations and resources to be defined
	      together. Furthermore, fault tolerance should be handled automatically, without the user's
	      intervention.
\end{itemize}

It is interesting to note that some of these challenges uniquely affect task graphs, and in general
programming models that are different from the traditional ways of defining \gls{hpc}
computations. Consider a distributed application implemented using \gls{mpi}, which
has historically been the most common way of defining computations on supercomputers.
\gls{mpi} applications typically assume that they will run on a set of computational
nodes for a relatively long duration (hours or even days). This set of nodes (corresponding to
\gls{mpi} processes) usually does not change during the computation; it is not
trivial to add new processes, and if some of the processes crash, it typically leads to the whole
computation being aborted, as fault tolerance is not a default property of \gls{mpi}
applications~\cite{fault_tolerant_mpi}.

These properties of \gls{mpi} applications are similar to the mentioned properties of
allocations; therefore, they fit together well, and using allocations to execute them is relatively
straightforward. In fact, it is clear that the allocation model itself was designed with
\gls{mpi}-like use-cases in mind. Therefore, it is not very surprising that the
concept of allocations is not a good match for programming models that are very different from
\gls{mpi}, such as task-based workflows.

%Note that several of the problems associated with using allocations are directly relevant to
%various challenges from~\Autoref{ch:sota}. Factors such as heterogeneous resource
%requirements, fault tolerance or efficient hardware usage are affected by the usage of allocations.
%Because of that, resolving the problems with allocations should also go a long way towards
%alleviating other ergonomic challenges of task graph execution.

\section{Meta-scheduling design}
This section describes a design for executing task graphs that aims to alleviate the mentioned
challenges by treating task-based programming models as first-class citizens, but still remaining
compatible with allocation managers to enable straightforward execution on \gls{hpc}
clusters. First, we will see a general overview of the design, then we will discuss its properties,
and the following sections will describe its concrete implementation within the
\hyperqueue{} task runtime.

We have seen that the most problematic aspect of allocations is the requirement to define
computations and hardware resources together. The key idea of the proposed design is thus to
completely disentangle the definition of what the user wants to compute (tasks) from the hardware
resources where the computation should take place (computational nodes, \gls{cpu}
cores, etc.). By separating these two concerns, we enable users to focus on what they care about
the most (their task graphs), instead of having to think about mapping tasks to allocations and
graph partitioning.

But how can we disentangle these two concepts when allocations require us to specify both? The
``trick'' lies in leveraging meta-scheduling on top of allocation managers, which allows us to
change what kind of computation is submitted in allocations and thus greatly simplify the
submission process. The proposed approach is built upon the following three principles.

\begin{description}[wide=0pt]
	\item[Task runtime runs outside of allocations] Moving the task runtime outside of allocations and running it at some persistent location in the
		cluster (e.g.\ on a login node) enables users to submit tasks to it in a straightforward way and
		crucially also to define the task graph independently of allocations. This removes the need to
		decide which tasks should be computed on which hardware resources and in which allocations up
		front. It can also improve hardware utilization, because it gives the task runtime the ability to
		load balance tasks across all active allocations.

		This is where the term \emph{meta-scheduling} comes in; the essence of the idea is to use a task
		runtime as a sort of high-level scheduler on top of an allocation manager. Instead of forcing users
		to think in terms of allocations, they can submit task graphs in the same way they would on a
		system that is not supervised by allocation managers, and let the task runtime automatically decide
		in which allocation a task should be executed.

	\item[Allocations are uniform] Even if the task runtime runs outside an allocation, we still need to submit \emph{some}
		allocations to provide hardware resources that will actually execute tasks. However, instead of
		defining specific tasks that should be computed within an allocation, we can submit allocations
		that execute a generic computational provider (worker), which will connect to the task runtime
		instance running outside the allocation, and then execute tasks that are dynamically assigned to it
		by the runtime.

		With this approach, allocations become trivial and completely uniform, because they all have the
		same structure; each allocation simply starts a (set of) computational provider(s). This makes it
		much easier for users to submit allocations. Instead of thinking about how to partition their task
		graphs, users simply decide how many computational resources they want to spend at any given
		moment, and start the corresponding number of allocations. Furthermore, this also allows the task
		runtime to submit allocations completely automatically based on the current computational load.
	\item[Pair tasks with workers using abstract resources] The principles described above separate the two core pillars of task graph execution; the
		definition of the task graph and the provisioning of hardware resources. However, we actually do
		need to have \emph{some} way of tying these two aspects together, because tasks might
		have various constraints on the environment in which they can be executed. We saw that submitting
		tasks directly as allocations, which explicitly ties tasks to a set of hardware resources, had many
		disadvantages. What we can do instead is to use a resource system that will allow us to pair tasks
		with workers in a more general and abstract way. Each task can describe a set of abstract resources
		that have to be available so that the task can be executed, and each worker will in turn provide a
		set of abstract resources that it provides. The task runtime will then dynamically assign tasks to
		workers by matching these resources together, while making sure that worker resources are not being
		oversubscribed.

		As an example, a task can specify that it requires four \gls{cpu} cores and two
		\glspl{gpu}. If we provide the task runtime with a worker that has 128 cores and 8
		\glspl{gpu}, it could schedule up to four instances of this task on this worker at the
		same time.

		This approach allows defining complex heterogeneous tasks while allowing users to provide workers
		independently of tasks, and without having to bind tasks to specific allocations.
\end{description}

The proposed design enables straightforward execution of large heterogeneous task graphs with
complex dependencies, because it sidesteps the allocation manager by moving the task runtime
outside of allocations. This avoids the limits associated with executing tasks in allocations
directly, such as limited support for dependencies, limited scale of task graphs that can be
executed, and non-optimal load balancing and hardware utilization.

This scheme can also alleviate additional challenges mentioned in~\Autoref{ch:sota}. For
example, it benefits fault tolerance, as tasks are no longer tied to specified allocations, because
they are stored in a persistent task runtime instance. When a task fails, the runtime can simply
recompute it, even in a different allocation than where it was originally started, in a way that is
fully transparent to the user. Furthermore, heterogeneous task graphs can be executed easily,
without having to consider allocation limits and the granularity of allocated hardware resources,
due to the usage of the abstract resource system. Even if the allocation manager always provides at
least a whole node in each allocation, multiple granular tasks (even from completely independent
task graphs) that require e.g.\ only a single core can be load balanced onto that same node. And
lastly, task scheduling becomes more flexible, because the task runtime can dynamically leverage
resources provided by multiple allocations at once. The last two mentioned aspects can help improve
hardware utilization in particular.

Of course, this approach is not a panacea, and it also introduces certain trade-offs. For example,
running the task runtime on a login node might be problematic if the login nodes are severely
computationally constrained or if they are not even able to connect to compute nodes of the
\gls{hpc} cluster through the network. Performance and security aspects of this
deployment method also need to be considered. In theory, the runtime could also be executed
elsewhere, for example in a cloud partition of a cluster, or even in an ephemeral way inside
allocations themselves (in that case it would still communicate with workers running in different
allocations). However, this would complicate the deployment process and it might require additional
resiliency features to be implemented in the task runtime.

To evaluate the proposed approach works in practice, what its performance and usage implications
are and how it compares to existing meta-schedulers, we have implemented this design in a task
runtime called \hyperqueue{}, which will be described in the rest of this chapter.

\section{\hyperqueue{}}
\hyperqueue{} (\hq{}) is a distributed task runtime that aims to make task graph execution a
first-class concept on supercomputers. It does that by implementing the meta-scheduling design
described in the previous section, and providing functionality optimized for
\gls{hpc} use-cases. In terms of architecture and implementation, it is an evolution
of the \rsds{} task runtime described in~\Autoref{ch:rsds}; however, instead
of using the \dask{} components and \glspl{api}, it implements its own
task-based programming model. \hyperqueue{} is developed in the Rust programming language
and it is provided as an \mbox{MIT-licensed} open-source tool~\cite{hq_github}.

This section describes the high-level design of \hyperqueue{}, its programming model and
the most important features. It also presents several use-cases where it has been successfully
leveraged, and evaluates its performance and other aspects in various experiments.

Note that for simplicity, Slurm will be used as a default representative of an allocation manager
throughout this whole chapter, but it could be replaced by \gls{pbs} or any other
commonly used allocation manager without loss of generality.

\subsection{Architecture}
\label{hq:architecture}
\hyperqueue{} uses a fairly standard distributed architecture. It consists of two main
components, a central management service called a \emph{server} and a component that
serves as a computational provider which executes tasks, called a \emph{worker}. Users
then interact with the server using a \emph{client} interface.~\Autoref{fig:hq-architecture}
displays a high-level view of the \hq{} architecture for a typical deployment on
an \gls{hpc} cluster with an allocation manager, where the server runs on a login
node and meta-schedules tasks to workers that run on computational nodes in various allocations.
The individual components are described in more detail below.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.9\textwidth]{imgs/hq/architecture}
	\caption{Architecture of \hyperqueue{}.}
	\label{fig:hq-architecture}
\end{figure}

\subsubsection*{Server}
The \emph{server} is the most important part of \hyperqueue{}. Its main goal is
to manage both the lifecycle of task graphs and of workers that provide computational resources. It
keeps track of all tasks and workers, and allows users to query their state and also send commands
to it through a client interface. The server contains a work-stealing task scheduler that is based
on the \rsds{} scheduler implementation described in~\Autoref{sec:rsds-description}. The
scheduler assigns individual tasks to workers based on task resource requirements and current
computational load of workers, with the goal of maximizing hardware utilization of the connected
workers.

It is designed to be executed as a long-running background process, which should ideally be
executed in a persistent location not managed by allocation managers (i.e.\ outside ephemeral
allocations). Typically, it is executed on a login node of a cluster, but it can also run
elsewhere, for example in a cloud partition. It should keep running until all tasks that the user
wants to execute are finished, although it can also restore its state from disk, so the computation
of a task graph can be interrupted and resumed at a later time, if needed.

The fact that the server runs outside allocations allows it to load balance tasks across workers
running in multiple allocations concurrently, and work around the various limits of allocation
managers. This adheres to the first principle of the meta-scheduling design described in the
previous section.

The server itself does not execute any tasks, and it consumes a relatively modest amount of
resources, and therefore it is not computationally demanding.~\Autoref{sec:hq-exp-server-cpu-consumption} evaluates how
many \gls{cpu} resources the server consumes in extreme scenarios.

Apart from basic responsibilities related to worker management and task scheduling, the server also
provides additional functionality. For example, it can submit allocations fully automatically on
behalf of the user. This feature will be described in detail in~\Autoref{hq:automatic-allocation}.

\subsubsection*{Worker}
The \emph{worker} component is a computational provider that executes tasks assigned to
it by the server. A single worker typically manages all hardware resources (\gls{cpu}
cores, \gls{gpu} or \gls{fpga} accelerators, memory, etc.) of a given
computer, usually a single computational node of a supercomputer.

The worker is able to automatically detect all relevant hardware resources that are available on
the node where it is started and advertise them to the server. Therefore, in a typical case, users
can simply start the worker on a given node with a single uniform command, and from that moment on
the resources of that node will be used for task execution. This adheres to the second principle of
the proposed meta-scheduling design, where computational providers should be generic (not tied to
any specific task or a set of tasks), and it should be possible to start them using a uniform
command, to make their deployment trivial.

Each worker also participates in task and resource scheduling. The server makes high-level
scheduling decisions, such as which tasks will be executed on a given worker, while the worker then
performs micro decisions, such as in which order it will execute tasks assigned to it, or which
specific resources will be assigned to a given task. Resource management will be described in more
detail in~\Autoref{sec:hq-resource-management}.

\hyperqueue{} workers were designed with fault tolerance in mind. On an
\gls{hpc}
cluster, workers will be typically executed inside allocations that only run for a limited amount
of time, which implies that workers can disappear at any time during the computation of a task
graph. Both the server and workers thus operate with the assumption that workers can be potentially
short-lived; if a worker disconnects while it is executing a task, the task will be transparently
reassigned to a different worker without user intervention. Worker management is thus dynamic and
flexible; new workers can also be added at any time, after which they will immediately start
contributing to the execution of the submitted task graphs. This property enables users of
\hq{} to arbitrarily scale the computation of a task graph both up and down, even
while it is being executed, by dynamically adding or removing workers as needed.

While workers can be deployed manually by users, \hq{} can also automatically
submit allocations to an allocation manager that start workers on computational nodes, based on
current computational needs of submitted task graphs, as was already mentioned. This system, called
\emph{automatic allocation}, will be described in more detail later in~\Autoref{hq:automatic-allocation}.

\subsubsection*{Client interfaces}
Users can use two main interfaces for deploying the \hyperqueue{} server and workers,
submitting task graphs and managing and monitoring the computation. They can either use a
\gls{cli}, which has been designed to feel familiar to users of Slurm in order to
make it easier to migrate existing \gls{hpc} workflows to \hyperqueue{}, or
a Python \gls{api}, which is useful for more complex use-cases that are challenging
to express using the command line. These interfaces are relatively large, and their specific shape
is not crucial for this thesis; the complete set of the supported \gls{cli} commands
and their parameters and the Python \gls{api} is described in the
\hyperqueue{} documentation~\cite{hq_docs}. In the rest of the text, we will
show various examples of these interfaces (primarily of several \gls{cli} commands),
to provide an idea of how ergonomic it is for users to interact with \hyperqueue{}.

The \gls{cli} has been designed with the ``Simple things should be simple. Complex
things should be possible.''\footnote{Originally coined by Alan Kay.} approach in mind. It allows submitting various
kinds of workflows in an easy way, but also allows users to configure their execution extensively.
This is in contrast to some other task runtimes, which require using Python code (e.g.\
\dask{}) or workflow files (e.g.\ \snakemake{}) for executing even the
simplest of workflows.~\Autoref{lst:hq-cli-commands} shows three basic commands that leverage the
\texttt{hq} binary to deploy an instance of a \hyperqueue{} server and a
worker, and then execute a single task through it. More examples will be shown later throughout
this chapter.

For simple existing workflows defined using Bash scripts, it is enough to replace uses of the
\texttt{sbatch} command with \texttt{hq submit} to let \hyperqueue{} manage the execution
instead of Slurm.

\begin{listing}[h]
	\begin{minted}[fontsize=\small, tabsize=4]{bash}
# Start a HQ server
$ hq server start &

# Start a HQ worker
$ hq worker start &

# Submit a HQ job (task graph) with a single task
$ hq submit ./my-program
	\end{minted}
	\caption{Examples of \hyperqueue{} \gls{cli} commands}
	\label{lst:hq-cli-commands}
\end{listing}

\subsubsection*{Deployment}
Even though it is often overlooked, deployment of software on supercomputers can be challenging, as
was described in~\Autoref{challenge:deployment}. Since \hyperqueue{} aims to provide seamless
support for \gls{hpc} use-cases, it was also designed to be trivial to deploy. It is
distributed as a single, relatively small (approximately \SI{15}{\mebi\byte}), statically
linked executable called \texttt{hq}, which implements all the provided functionality
(server, worker and the whole \gls{cli}). It does not depend on any external services
or dynamic libraries, apart from the ubiquitous \texttt{C} standard library, and it
runs fully in userspace and does not require any elevated privileges. It also does not require any
installation nor configuration steps.

The \hyperqueue{} Python \gls{api} is then distributed as a Python package
that is available on the standard Python Package Index~\cite{hq_pypi}. This package
contains all the \hq{} logic inline; it is thus self-contained and it does not
need the \texttt{hq} binary to function.

Both clients and workers communicate with the server using the standard \gls{tcpip}
protocol, which is ubiquitously available on most clusters. By default, all communication is
encrypted, so that other users of the cluster cannot observe the data sent between the
\hyperqueue{} components. This is performed out of an abundance of caution, because the
server typically runs on a login node, which is shared by multiple users, rather than on
computational nodes, which are usually isolated between users by the allocation manager. The
performance overhead of encryption will be evaluated in~\Autoref{sec:hq-exp-encryption-overhead}.

In order to connect the individual components using \gls{tcpip}, normally it would be
required for users to specify network hostnames and ports. While this process is relatively simple,
it still presents a minor ergonomic hurdle, because users would need to reconfigure their worker
deployment scripts and client commands every time the server would start on a new
\gls{tcpip} address. To make this easier, \hyperqueue{} removes the need to
specify hostnames and ports by default, by exploiting a useful property of \gls{hpc}
clusters, which commonly use a shared networked filesystem. When a server is created, it creates a
configuration file in the user's home directory, which contains all information necessary for
exchanging handshakes and connecting the components. When workers and clients discover the presence
of this file (which can be accessed due to the filesystem being shared), they are able to connect
to the server without the user having to specify any addresses. However, it is still possible to
deploy multiple concurrent instances of \hyperqueue{} under the same account; users just
need to provide separate directories for storing the configuration file.

Thanks to these properties, it is trivial to deploy and use \hyperqueue{} on
supercomputers, even without the involvement of the administrators of the target cluster. The
deployment benefits extend also beyond \gls{hpc} clusters. Users can also trivially
deploy \hyperqueue{} on their own personal computers. This enables them to prototype
their \gls{hpc} workflows locally, which can accelerate their development process and
is another step towards improving the ergonomics of using task graphs on supercomputers. This is in
contrast with using allocation managers for submitting tasks directly, which makes local
experimentation very difficult, as allocation managers are typically not straightforward to deploy
on personal computers.

\subsection{Programming model}
\label{sec:hq-programming-model}
The task-based programming model used by \hyperqueue{} builds upon the task model defined
in~\Autoref{ch:taskgraphs}. It adds some additional \gls{hpc}-focused features on top
of it, such as fine-grained resource management and support for multi-node tasks.

The core element of the programming model is a \textbf{Task}, which has the following
properties:
\begin{itemize}
	\item It is a unit of computation. A task can be executed by \hq{}, and its execution
	      either succeeds or fails. In the current implementation, each task represents either the execution
	      of a binary executable, or the invocation of a single Python function, although the specific
	      details of the task execution are opaque to \hyperqueue{}.
	\item It is a unit of scheduling. The \hq{} scheduler assigns each task to a worker (or
	      to a set of workers in the case of multi-node tasks), and can move tasks between workers using
	      work-stealing if their load is unbalanced.
	\item It forms a failure boundary. When a worker crashes while executing a set of tasks,
	      \hq{} can schedule them again on a different worker and restart their execution
	      from scratch.
	\item It is a unit of dependency management. Each task may depend on a set of other tasks, and in this
	      way they may be composed in a computational \gls{dag} of tasks.
	\item It is a unit of resource management. Each task has a set of attached resource requirements, which
	      describe constraints on a computational environment where the task is allowed to be executed. The
	      \hq{} scheduler then matches these requirements with resources provided by
	      workers.
\end{itemize}

Each task has various properties, such as a path to the executed binary (or a Python function), a
set of environment variables, working directory, and others. Each task also has a set of attached
resource requirements, which will be described in more detail in~\Autoref{sec:hq-resource-management}.

Tasks are composed into task graphs, which are called \textbf{Jobs} in the
\hq{} terminology. Each task belongs to exactly one job, and jobs are completely
independent, so there can be no dependencies between tasks belonging to different jobs. Jobs are
units of monitoring and management, as they allow users to group many tasks together, observe their
status or perform operations on all (or some) tasks of a job at once. There are two kinds of jobs,
\emph{graph jobs} and \emph{task arrays}. Graph jobs correspond to arbitrary
\glspl{dag}, where tasks may depend on one another. These can be only created using the
Python \gls{api} or a \gls{toml} workflow file, since it would be very
difficult for users to express dependencies using a command-line interface.

Task arrays are a special case of graph jobs, which use a compressed representation of a
potentially large task graph that has a regular shape. For example, a common use-case is to compute
a task graph where each task runs the same computation, but on a different input. This can be
achieved with a task array, which stores only a single instance of the task definition (what should
be computed), but creates many copies of this task, each with a different input. This both reduces
the memory usage of the server, since the task graph representation is stored in this compressed
way, and also allows users to easily execute large task graphs using the \gls{cli}.

Each task in the task array gets assigned a different \emph{task ID}, which uniquely
identifies a specific task, which allows it to perform a computation on a different input. For
example, if a user creates a task array with a thousand tasks, by default, the first will have ID
$1$, the second one will have ID $2$, etc. The ID is passed to
the task through an environment variable. Apart from assigning unique inputs to tasks through task
IDs, \hq{} also supports passing a unique binary blob to each task (again through
an environment variable) that can contain arbitrary data.~\Autoref{lst:hq-cli-task-arrays} shows an example
of how users may submit task arrays through the \gls{cli}.

\begin{listing}[h]
	\begin{minted}[fontsize=\small, tabsize=4]{bash}
# Task array with 10 tasks, each task gets a different ID
$ hq submit --array=1-10 ./my-script.sh

# Each task will receive a single line from the given file
$ hq submit --each-line=inputs.txt ./my-script.sh

# Each task will receive a single item from a JSON array stored in the given file
$ hq submit --from-json=items.json ./my-script.sh
	\end{minted}
	\caption{Creating task arrays using the \hyperqueue{} \gls{cli}}
	\label{lst:hq-cli-task-arrays}
\end{listing}

\subsubsection*{Task and job lifecycle}
The server manages the state lifecycle of tasks and jobs and also reports them to the user, so that
they can observe what is happening with their computation. At any given time, each task can be in a
single \emph{state}:

\begin{description}
	\item[Waiting] After the user creates (\emph{submits}) a task, it starts in the \emph{waiting}
		state, where it waits until it is scheduled to a worker that is able to fulfill its resource
		requirements.
	\item[Running] After a task is scheduled and starts executing on a worker, it moves on to the
		\emph{running} state. If the worker that is executing the task stops or crashes, the task
		will move back to the \emph{waiting} state, so that it can be rescheduled to a different
		worker. This is labeled as a \emph{task restart}.
	\item[Finished] When a task finishes successfully (a binary program exits with the exit code $0$
		or a Python function returns without throwing an exception), it moves to the \emph{finished}
		state.
	\item[Failed] When a task fails (a binary program exits with a non-zero exit code or a Python function throws an
		exception), it moves to the \emph{failed} state.
	\item[Canceled] When a waiting or a running task is canceled by the user, it moves to the \emph{canceled}
		state, and it is no longer considered for execution. Users can cancel tasks and jobs that are no
		longer relevant and that should not continue executing.
\end{description}

\Autoref{fig:hq-task-state-diagram} displays a state diagram of the various task states and
transitions that cause state changes. The \emph{finished}, \emph{failed} and
\emph{canceled} states are terminal; once a task reaches that state, it cannot change its
state anymore. Each job also has its associated state, which is derived from the state of its
tasks.

\begin{figure}[h]
	\centering
	\begin{tikzpicture}
		\tikzstyle{every node}=[font=\footnotesize]
		\tikzstyle{every state}=[fill={rgb:black,1;white,8},minimum size=1.5cm,font=\footnotesize]

		\node[state,initial,initial text=Submitted]   	(waiting)  							{Waiting};
		\node[state] 							   		(running) 	[right=2.5 of waiting]	{Running};
		\node[state,accepting]           		   		(finished) 	[right=2 of running]	{Finished};
		\node[state,accepting]                     		(failed) 	[above=1 of finished] 	{Failed};
		\node[state,accepting]                     		(canceled)	[below=1 of finished]   {Canceled};

		\path[->]
		(waiting)   edge [bend left]	node [above] 			{Scheduled} 		(running)
		(waiting)   edge [bend right]	node [below left]		{Canceled}			(canceled)
		(running)	edge    			node [above] 			{Finished}			(finished)
		(running) 	edge    			node [above left] 		{Failed} 			(failed)
		(running)	edge    			node [left,xshift=-5]	{Canceled} 			(canceled)
		(running)	edge [bend left]    node [below,xshift=8]	{Worker crashed}	(waiting);
	\end{tikzpicture}
	\caption{State diagram of \hyperqueue{} tasks}
	\label{fig:hq-task-state-diagram}
\end{figure}

\subsubsection*{Iterative computation}
Iterative computation, such as running a simulation until it converges or training a
machine-learning model, can be expressed using the \hyperqueue{} Python
\gls{api}. It provides facilities for executing a task graph, waiting until it
finishes computing and then submitting a new task graph based on the computed results. While this
is enough for describing basic scenarios that require dynamic iteration, it is not fully ergonomic,
as it only provides dynamic submission of new computation on the granularity level of jobs (task
graphs), not individual tasks.

A more general solution would be to allow adding new tasks to already submitted
\hyperqueue{} jobs, which would enable tasks themselves to submit new tasks into their
own task graph. This would make it easier to express arbitrarily dynamic task graph structures,
where the shape of the graph is not fully known before their computation starts. We plan to
implement support for this feature as a future extension.

\subsection{Task input and output}
By default, each \hyperqueue{} task stores its \emph{standard output} (\emph{stdout}) and
\emph{standard error output} (\emph{stderr}) streams to the filesystem. Users can override this to avoid storing these
output streams, or to let \hq{} remove them if they are empty after the task has
been finished, to reduce disk clutter. It is also possible to use various placeholders for
configuring the paths of these streams. For example, the string \texttt{\%\{TASK\_ID\}} in a stream
path will get resolved to the task ID of the task that produces the stream. This can be used to
easily separate the output of different tasks of task arrays, for example.

Since task graphs can be large, generating two files per task, especially on shared network
filesystems that are commonly used on supercomputers, could cause performance bottlenecks or run
into limitations caused by user disk quotas. To alleviate this problem, \hq{}
supports an output management system called \emph{output streaming}. If it is enabled for a given
job, then \emph{stdout} and \emph{stderr} of tasks is streamed over the network
from workers to the server, instead of being stored into separate files. The server then stores all
this data into a single file, and then offers several commands for manipulating and filtering task
output data from this file. This helps to reduce the pressure put on the
filesystem.~\Autoref{sec:hq-exp-output-streaming} will evaluate how this system helps with potential
\gls{io} issues.~\Autoref{lst:hq-cli-log} shows several \hq{}
commands that can be used to interact with output streaming.

\begin{listing}[h]
	\begin{minted}[fontsize=\small, tabsize=4]{bash}
# Submit a job with output streaming enabled
$ hq submit --log=log.bin --wait -- ./my-program

# Print stdout of tasks 1 to 10 from the log
$ hq log log.bin cat --task 1-10 stdout

# Export the log into a JSON format
$ hq log log.bin export
	\end{minted}
	\caption{\hyperqueue{} \gls{cli} commands for working with output streaming}
	\label{lst:hq-cli-log}
\end{listing}

Note that \hyperqueue{} currently does not support data transfers between tasks, as it is
primarily focused on executing tasks that execute black-box binaries which communicate through
files on a file-system. Arbitrary binary data can be attached as an input for any task (through its
\emph{standard input} stream), but task outputs are only used for specifying dependencies; no
task outputs can actually be transferred between the workers. This is only relevant for the Python
\gls{api}, because tasks submitted using the \gls{cli} cannot express
data transfers. The \hq{} server is based on the \rsds{} server
implementation, which supports data transfers between workers, so the transfer functionality itself
is actually implemented in \hq{}. However, data transfers still need to be
incorporated into the \hyperqueue{} programming model, and a user-facing (Python)
interface for consuming outputs of tasks needs to be designed. It should be noted that this is
merely a limitation of the current implementation, which we plan to lift in the future.

\subsection{Resource management and scheduling}
\label{sec:hq-resource-management}
As has already been mentioned, the \hyperqueue{} scheduler is derived from the
\rsds{} work-stealing scheduler that was described in~\Autoref{sec:rsds-description}. It
assigns tasks to workers eagerly, to optimize the fast path where there are enough tasks to utilize
the available workers. When some workers are under-utilized, the server uses work-stealing to steal
tasks between workers to balance the computational load. Scheduling is performed on the level of
tasks, regardless of which job they belong to. In fact, the scheduler does not even know anything
about jobs.

What makes the scheduler different from \rsds{} is its support for complex resource
management. It takes into account various resource requirements requested by tasks, and also the
state of currently available resources on each worker, and uses this information to drive its
scheduling decisions. The most important responsibility of this resource management is to uphold
the invariant that each task will be executed on a worker that is able to provide the corresponding
amount of each resource requested by the task at the time of the task's execution. When a task is
being scheduled, the scheduler looks for a worker that can satisfy the resource requirements of the
task, based on the amount of resources provided by the worker in general, and also the amount of
resources that the worker currently has available. When a task is scheduled onto a worker, the
scheduler \emph{allocates} a specific subset of the worker's resources to that task, which
will not be available for other tasks until the scheduled tasks finished executing.

\hyperqueue{} resources are \emph{generic}, as users can define their own
arbitrary resources for both tasks and workers, although \hq{} also recognizes
several known resource names and provides specialized support for them. Resource management in
\hyperqueue{} consists of two main elements, resource requirements specified by tasks and
resource pools provided by workers.

\subsubsection*{Worker resource pools}
Worker resources are configured when a worker is started through so-called \emph{resource pools},
named sets of resources. The values of the most common resource pools, like the
\gls{cpu} cores and \gls{numa} sockets, the amount of
\gls{ram} and the presence of \gls{gpu} accelerators, are detected
automatically. On top of that, users can specify an arbitrary set of additional pools. There are
two kinds of resource pools:

\begin{description}
	\item [Indexed pool] represents a set of resources where each individual resource is
	      non-fungible and has a unique identity (represented either by an integer or a string). An example
	      of such a resource pool could be a set of \gls{cpu} cores or \gls{gpu}
	      devices. For example, if a worker has two NVIDIA \glspl{gpu}, that worker could provide
	      an indexed pool containing two resources (e.g.\ with indices $0$ and
	      $1$), each representing one individual \gls{gpu} device. These two
	      resources are not interchangeable; the scheduler decides which specific resource will be assigned
	      to a task, and it tracks the individual resources based on their identity. If a task
	      $t$ is currently using \gls{gpu} $0$, that
	      resource will not be assigned to any other task until $t$ has finished executing.

	      Individual resources of an indexed pool can be further subdivided into \emph{groups},
	      which mark some form of relationship between specific resources. This can be used e.g.\ to
	      represent \gls{numa} domains and \gls{cpu} sockets; a worker might provide
	      a single indexed pools of \glspl{cpu}, with a separate group for each
	      \gls{numa} socket present on the worker's node. Tasks can then specify if they want to
	      prioritize drawing resources that belong to the same group. This will be described later.
	\item [Sum pool] is designed for resources that are fungible. Such resources are typically
	      numerous, and it would not be practical to track each individual resource separately. A sum pool
	      thus does not define its individual resources through indices, but only defines a single number
	      that specifies the amount of resources available in the pool. A typical representative of a sum
	      pool is \gls{ram} available on a worker. Modern \gls{hpc} nodes can
	      contain hundreds of GiBs of memory, or even more, and it is not feasible to treat each byte as a
	      separate resource. At the same time, tasks usually do not need such fine-grained tracking; if they
	      care about memory requirements, they just specify how much memory they need.
\end{description}

\Autoref{lst:hq-cli-worker-resources} shows a few examples of how users can configure resources
when starting workers through the \gls{cli}.

\begin{listing}[h]
	\begin{minted}[fontsize=\small, tabsize=4]{bash}
# Start a worker with automatically detected resources
$ hq worker start

# Start a worker that manages 16 CPU cores
$ hq worker start --cpus 16

# Start a worker with additional resources
# Adds an indexed pool of four GPUs and a sum pool of 1024 resources (bytes)
$ hq worker start \
	--resource 'gpus/nvidia=[0,1,2,3]' \
	--resource 'memory=sum(1024)'
	\end{minted}
	\caption{Configuring worker resources using the \hyperqueue{} \gls{cli}}
	\label{lst:hq-cli-worker-resources}
\end{listing}

\subsubsection*{Task resource requirements}
Resource pools define the resources that are available on workers. The scheduler then uses
\emph{resource requirements} attached to individual tasks to decide on which workers a given task can be
executed, and which specific resources it will consume while the task is running. Task resource
requirements are defined for each task separately. By default, each task requires a single
\gls{cpu} core, but users can specify a different \gls{cpu}
requirement, and also add additional arbitrary requirements.

A resource requirement consists of a name of the resource that the task needs, and an amount
specifying how many such resources are needed. For example, a task might specify that it requires
two \glspl{gpu} for its execution. The resource requirement always specifies the amount
of required resources, regardless of whether the given resource is taken from an indexed or a sum
pool. Therefore, tasks cannot ask for specific instances of resources based on their identity; the
scheduler decides which specific resources will be assigned to individual tasks, which makes
scheduling more flexible. For example, assume that a worker has $128$ cores (each
with their own identity), and there is a task that requires two cores. It would not be very
practical if the task had to specify that it specifically requires cores with ID
$0$ and $1$ or $5$ and
$8$, etc. Instead, it just specifies the number of \gls{cpu} cores
it needs, and lets the scheduler decide which specific cores it will assign to the task, based on
what cores will be available on the given worker at the time the task is about to start executing.

In addition to the basic system described above, resource requirements can also leverage a few
additional parameters that allow tasks to describe more complex use-cases related to resource
management.

A \emph{group allocation strategy} can be used to affect which specific resources from an indexed pool will
be used for the task. It is not relevant for indexed pools with a single group nor for sum pools. A
typical example where indexed pool groups are useful are \gls{numa} nodes, where tasks
might want to be executed on cores that belong to the same \gls{numa} node (group). There are three
different strategies that can be selected:
\begin{description}
	\item [Compact] is the default strategy. The scheduler tries to allocate all requested
	      resources from as few groups as possible. However, if there are not enough resources currently
	      available for that on a given worker, it will also try to draw resources from a larger number of
	      groups, to satisfy the resource requirement.
	\item [Strict compact] is a stricter version of the \emph{compact} strategy. With this
	      strategy, the scheduler first figures out the minimum number of groups required to satisfy the
	      resource requirement, and it will then not schedule the task until it can provide all requested
	      resources from the smallest number of groups possible.
	\item [Scatter] is the opposite of the \emph{compact} mode. It tries to allocate resources
	      for the task from as many groups as possible.
\end{description}

Resource requirements can also be specified without an upper bound, i.e.\ a task can specify that
it wants to consume all available resources of the specified pool. If a worker has at least a
single resource of the given pool available, and the scheduler assigns a task with this kind of
resource requirement to it, that task will be allocated all the worker resources that are available
when the task starts to execute.

In some cases, requesting whole resources can still be too coarse-grained for some tasks. For
example, a task might execute software that is able to leverage \gls{gpu}
acceleration, but it cannot utilize a whole \gls{gpu} device by itself. If such task
has requested a whole \gls{gpu}, part of that accelerator's hardware might sit idle
while the task is executed. To avoid this situation, \hyperqueue{} allows tasks to define
\emph{fractional resources}, which allow multiple tasks to share a single resource at the same time.
For example, the mentioned task could ask for only $0.5$ of a
\gls{gpu}, in which case the scheduler would allocate the same
\gls{gpu} device to up to two tasks at once. The scheduler uses fixed-point
arithmetic to manage fractional resources, in order to avoid common issues associated with
floating-point arithmetic, rounding errors.

An even more complex resource management use-case supported by \hyperqueue{} is the
ability to specify \emph{resource variants}. Normally, each resource requirement specified by a task
is additive; all the requested resources have to be available in order for the task to start
executing. However, \hq{} also allows tasks to define a list of resource
requirement sets (called resource variants), and lets the scheduler use the first one that can be
satisfied by a worker at the time of scheduling (the order of the list elements decides the
priority of the individual requirement sets). Resource variants allow tasks to define multiple
situations under which it makes sense to execute the task, and let the scheduler decide which
variant to select, based on dynamic information about the current worker resource. As an example,
many \gls{hpc} software tools can run either on a \gls{cpu} or on a
\gls{gpu} device. A task using such software might specify e.g.\ that it needs either
a single \gls{gpu} and a single \gls{cpu}, or that it needs
$16$ \gls{cpu} cores (in the case that a \gls{gpu} is
not available). In this case, the scheduler could decide to execute the task purely using
\gls{cpu} resources, if there are no \gls{gpu} resources currently
available. Without support for resource variants, the scheduler would need to wait for a
\gls{gpu} to become available, even if there were free \gls{cpu}
resources that could be used in the meantime.

There is also one additional special resource that is relevant for scheduling, called a
\emph{time request}. If a task specifies a time request (a duration) $t$, it
tells the scheduler that it needs at least $t$ time for its execution. This is
important for avoiding needless task restarts, due to the way \hq{} workers are
commonly deployed in allocations. Workers started in allocations cannot run for a longer time than
the wall-time of the allocation. Since this duration is known (each allocation must have a
wall-time), \hq{} knows the remaining lifetime of each such worker at any given
time. This information is used for scheduling; if a worker has remaining lifetime
$l$, the scheduler will not assign it tasks with a time request that is longer
than $l$, because it is highly probable that such tasks could not finish
executing before that worker shuts down.

Once a task is scheduled to a worker, and the scheduler decides which resources will be allocated
to it, it passes this information to the task through environment variables. For example, if a task
asks for two \gls{cpu} cores, and the scheduler allocated the cores
$10$ and $14$ of a specific worker to it,
\hq{} will pass \texttt{HQ\_RESOURCE\_VALUES\_cpus=10,14} to the task.

It is important to note that \hyperqueue{} treats resources as opaque and virtual; they
are used only for scheduling. \hq{} does not enforce that e.g.\ if a task is
provided the cores $0$ and $1$, it will be only executed on
these specific cores. This responsibility is left to the task itself, which can use the provided
environment variable to decide using which resources it should actually be executed. For example, a
task could pin its executed program on cores allocated to it. In the case of
\gls{cpu} cores, \hq{} can even pin the allocated cores for the
task automatically, if the task is configured for pinning.

\begin{listing}[h]
	\begin{minted}[fontsize=\small, tabsize=4]{bash}
# Submit a task requiring 4 CPU cores
$ hq submit --cpus=4 ./my-program

# Submit 1000 tasks, each requiring 2 CPU cores
$ hq submit --cpus=2 --array=1-1000 ./my-program

# Submit a task requiring 2 CPU cores, enable CPU pinning through OpenMP
$ hq submit --cpus=2 --pin=omp ./my-program

# Submit a task that requires 2 FPGA devices and needs at least 10 minutes to execute
$ hq submit --resource 'fpga=2' --time-request 10m ./my-program
	\end{minted}
	\caption{Configuring task resource requirements using the \hyperqueue{} \gls{cli}}
	\label{lst:hq-cli-task-resources}
\end{listing}

\subsubsection*{Multi-node tasks}
\hyperqueue{} also supports a special kind of resource that is especially useful for
tasks that want to execute \gls{hpc} computations which are designed to run on
multiple nodes (e.g. \gls{mpi} programs). A task may specify that it wants to be
executed on multiple workers at the same time. In this case, \hq{} assumes
exclusive ownership of all the resources of each worker (node), which typically matches the
requirement of \gls{mpi}-like applications. A multi-node task requiring
$n$ nodes can only be scheduled when $n$ workers are idle
(not executing any tasks) at the same time. When a multi-node task starts to execute,
\hq{} passes it a \emph{nodefile} which contains the hostnames of all
workers allocated to the task. This nodefile can then be used by common \gls{mpi}
program launchers to start an \gls{mpi} program.

Users that execute multi-node tasks running \gls{mpi} applications might want to make
sure that all workers (nodes) contributing in this computation are started in the same allocation,
for performance reasons (e.g.\ they might configure allocations to use nodes that reside close
together in the networking topology of the cluster). By default, \hyperqueue{} tries to
adhere to this constraint. It introduces an abstract concept of \emph{worker groups}, and always
schedules a multi-node task to workers that belong to the same group. At the same time, it
automatically configures the group of a worker based on the allocation inside which it is executed
(if any). This ensures that multi-node tasks are by default executed only on workers that reside
within the same allocation. However, it is possible to override the group of a worker, and e.g.\
assign the same group to all workers. In that case, workers from all active allocations will be
used to participate in multi-node tasks.

\subsection{Fault tolerance}
The execution of a task in \hyperqueue{} can end in a failure for two main reasons. The
worker that executes the task might crash, be stopped by the user, or an allocation inside which it
is executed can run out of its wall-time. In that case, the task will be rescheduled by the server
to a different worker that upholds its resource requirements. This is made possible due to the fact
that tasks are not tied to any specific computational resource or allocation. When such a task
restart happens, it is important to let the executed program know that it is not being executed for
the first time, so that it can react to it (for example clean up files that were created during a
previous execution of the same task or restore execution from a previously created checkpoint).
This is achieved using an \emph{instance ID}. It is a non-negative number attached to each
task that is increased every time the task is restarted. This number is then passed to the executed
task, so that it can decide whether it should react to the restart in any specific way.

In rare cases, the crash of the worker might be caused by the task itself, e.g.\ if it allocates
too much memory and causes an ``\gls{oom}'' error. If this happens repeatedly, such
task could hamper the execution of the whole task graph, since it would be repeatedly crashing
workers. For that reason, it is possible to configure the maximum number of worker crashes per task
(it is configured to a low number by default). If a task exceeds this number (i.e.\ it is present
during too many worker crashes), it will be automatically canceled.

The second reason why a task might fail is that its computation simply does not succeed, for
example due to the executed process returning a non-zero exit code. In that case, the task is
marked as failed, and any tasks that (transitively) depend on it are canceled.
\hyperqueue{} allows a simple way of querying tasks by their state and resubmitting only
failed and/or canceled tasks in a straightforward way, which is possible due to the job state being
kept outside of ephemeral allocations.~\Autoref{lst:hq-cli-fault-tolerance} demonstrates how users can use the
\hq{} \gls{cli} to resubmit failed tasks from a previously
submitted job. It also shows that several \hq{} commands are designed to be
composable, adhering to the Unix philosophy of creating commands that do a single thing well and
that can be easily combined together.

\begin{listing}[h]
	\begin{minted}[fontsize=\small, tabsize=4]{bash}
# Submit task array with 1000 tasks, wait until it completes
$ hq submit --array=1-1000 --wait ./my-script.sh

# Find task IDs that have failed in the last job
$ FAILED_TASKS=$(hq job task-ids last --filter=failed)

# Resubmit only the failed tasks
$ hq submit --array=${FAILED_TASKS} ./my-computation
	\end{minted}
	\caption{Handling task failure using the \hyperqueue{} \gls{cli}}
	\label{lst:hq-cli-fault-tolerance}
\end{listing}

Too many tasks failing can signal the presence of some catastrophic condition, e.g.\ the user
forgot to configure their program properly, load some required runtime dependencies or used the
wrong filesystem path for inputs. If this happens for a very large job, it could lead to a lot of
wasted computation, where tasks will start to execute, but they will probably fail very soon after
that. \hyperqueue{} thus allows users to configure a parameter that specifies the maximum
number of task failures per job. If the configured number of failures is exceeded,
\hq{} will preemptively cancel the whole job to avoid wasting computation.

A special case of a failure is when the server itself crashes or it is stopped by the user, or when
a worker loses connection to the server due to networking issues. In this case, the worker cannot
reasonably continue executing long-term, without having a communication channel with the server.
However, if it stores tasks that are already being executed, it could be wasteful to stop their
computation just because the connection to the server has been lost. The worker can thus be
configured in a way that it will finish executing such running tasks before shutting itself down.
However, the completion of these tasks will not be reported back to the server, so users will need
to handle their output on disk manually.

It is possible to configure the server to store a log containing all necessary information about
its task database to the filesystem. The server can thus be restored in case of a crash or a
failure to resume previously unfinished computation of tasks.

\subsection{Automatic submission of allocations}
\label{hq:automatic-allocation}
The meta-scheduling design employed by \hyperqueue{} resolves most of the challenges
associated with using allocation managers for executing tasks, since it fully automates the mapping
of tasks to allocations. It also makes creating allocations simple, so that users can scale the
computational resources used for computing their task graphs at their will. However, creating
allocations manually can still be relatively demanding for users, who might want to employ a more
automated approach for scaling computational resources. Since \hyperqueue{} knows the
state of all tasks and workers, and it is commonly executed on login nodes that have access to
allocation managers, it is natural to provide it with the ability to automatically submit new
allocations on behalf of the user, based on current computational load. This system has been
implemented in \hq{} under the name \emph{Automatic allocator} (shortened as \autoalloc{}). The automatic
allocator was created with the following design goals:

\begin{description}[wide=0pt]
	\item[Allow computational resources to scale up] At any given moment, if there are tasks that are waiting to be executed, and there are no free
		computational resources, \autoalloc{} should attempt to add more resources by creating
		new allocations. At the same time, it should respect backpressure from the allocation manager to
		avoid overloading it.
	\item[Allow computational resources to scale down] Keeping allocations running on a supercomputer can be quite costly. \Autoalloc{} should
		thus make sure to shutdown allocations that are not performing useful work because they do not have
		anything to compute anymore, in order to avoid wasting resources.
	\item[Be flexible] Allocation managers typically provide various configuration knobs that can affect the behavior of
		allocations. While it will not ever be possible to support all possible implementations of
		allocation managers out of the box, \autoalloc{} should provide extension points that
		enable users to configure the submitted allocations to their liking.
\end{description}

\Autoalloc{} has been implemented as a background service within the
\hyperqueue{} server. To use it, users first need to create at least one
\emph{allocation queue}. Such a queue describes a template for creating new allocations, which will
be submitted by \hq{} when there is a demand for more computational resources.
Each queue contains several properties that can be configured by users:

\begin{description}
	\item[Allocation manager] \Autoalloc{} needs to communicate with a given allocation manager
		to be able to submit allocations into it. Users thus need to tell \hq{} which
		allocation manager it should communicate with. Currently, two managers are supported, Slurm and
		\gls{pbs}.
	\item[Time limit] This determines the wall-time of each allocation submitted by \autoalloc{}. Knowing the
		maximum duration of each allocation helps it decide when it makes sense to create allocations. For
		example, if the only tasks waiting to be computed have a \emph{time request} that is longer than
		the time limit of a given allocation queue, it does not make sense to create allocations in this
		queue, because workers started in these allocations could not execute such tasks anyway.
	\item[Backlog] This parameter specifies the maximum number of allocations that can be queued in the allocation
		manager at any given time. It is designed to ensure that the automatic allocator does not overload
		the allocation manager.
	\item[Worker count per allocation] Determines how many nodes (workers) should be requested for each allocation. Unless users want to
		execute multi-node tasks, it is usually convenient to use the default value
		($1$), because typically, the fewer nodes are requested in an allocation, the
		sooner it will be started by the allocation manager.
	\item[Max worker count] This parameter can be used to limit the maximum number of running workers across all allocations
		started by this queue.
	\item[Idle timeout] Time after which an idle worker (a worker that does not have any tasks to compute) will stop. The
		idle timeout mechanism helps to avoid resource waste, by stopping workers that do not have anything
		to do anymore. By default, this timeout is set to a few minutes.
	\item[Worker resources] Users can override the worker resources assigned to each worker started by the given allocation
		queue. This can be used if the user knows that the allocations will be started in a specific
		partition of the cluster that contains resources that cannot be automatically detected by
		\hyperqueue{} (e.g. \gls{fpga} devices).
	\item[Custom allocation parameters] Users can also specify arbitrary command-line parameters that will be passed to the submit command
		sent to the corresponding allocation manager, like the computational project that should be used,
		or the Slurm partition where the allocation should be created. To make debugging easier,
		\hyperqueue{} will submit a test allocation by default immediately after the allocation
		queue is created, to test that the allocation parameters can be handled by the manager. If this
		test allocation is successfully created, it is then immediately canceled to avoid wasting
		computational resources.
\end{description}

\Autoref{lst:hq-cli-autoalloc} shows several examples of commands for creating allocation queues and
observing allocation state using the \hyperqueue{} \gls{cli}.

\begin{listing}[h]
	\begin{minted}[fontsize=\small, tabsize=4]{bash}
# Create an allocation queue for Slurm
$ hq alloc add slurm --time-limit 1h -- -Aproject-1 -pcpu_partition

# Create an allocation queue for PBS with GPU resources
$ hq alloc add pbs --resource "gpus/nvidia=range(1-2)" ...

# Display information about (in)active allocations from the given queue
$ hq alloc info 1
	\end{minted}
	\caption{Handling task failure using the \hyperqueue{} \gls{cli}}
	\label{lst:hq-cli-autoalloc}
\end{listing}

\Autoalloc{} is a reactive system. It observes events from the server (such as new jobs
being submitted or new workers being connected) and manages allocations in response to these
events. If it encounters a situation where there are tasks that are in the waiting state, and no
workers can execute them (either because there are no workers or all of them are fully occupied),
it will start submitting allocations to the allocation manager. It will submit allocations up until
the configured \emph{backlog} value, or until the allocation manager applies backpressure
(whichever comes sooner). Note that \autoalloc{} does not limit the total number of
allocations that are ``in-flight'' (queued or running) by default, just the queued ones. This
allows it to potentially create a large number of allocations and thus make use of all available resources of
the cluster, if the allocation manager allows it. Users can use other configuration parameters
(such as the total number of workers) to limit the amount of submitted resources.
\Autoalloc{} also contains an internal rate limiter that makes sure that it does not
submit allocations too often and that it pauses the allocation process if the allocations start
failing too often.

\hyperqueue{} communicates with \gls{pbs} or Slurm using standard
commands, such as \texttt{qsub} or \texttt{sbatch}, as they do not provide a complete
machine-readable \gls{api}. This means that in order to use automatic allocation, the server has to
be executed on a node that has access to these commands and can communicate with the manager. If
this is not possible, a proxy service could be used to forward communication between the node
with the server and a node where the manager is deployed. \hyperqueue{} implements communication
with each manager in a custom way. While there have been some attempts to create a unified
interface~\cite{psij,workflow-alloc-manager-comm} for submitting allocations to different
allocation managers, they have not seen wide adoption so far. Furthermore, adopting these
approaches would require \hq{} to introduce a dependency on Python or an external \gls{http}
service, which would make it more difficult to deploy.

It is crucial for \autoalloc{} to maintain an updated state of its created allocations,
so that it can report their state to the user, and also so that it knows if it can submit new
allocations. Originally, it was using a polling approach to determine the latest state, where it
was repeatedly querying the allocation manager about the state of allocations. However, this turned
out to put a lot of pressure on the allocation managers, which were not very optimized for this
use-case, especially if the polling frequency was very high (every few seconds). Some allocation
managers are even configured in a way where they cache the state of allocations when they are
queried too often; this reduces the chance of them being overloaded, but it also means that
\hyperqueue{} would not get the latest data, which is not ideal. However, we can observe
that it is not actually required to poll the allocation manager. The only allocation events that
are important for the allocator are the start and an end of an allocation. And this information can
be gained through a proxy; the connections are disconnections of \hq{} workers.
When a worker is started in an allocation, it will connect to the server, and tell it its
allocation ID\@. This lets the automatic allocator learn about new allocations. On the other hand,
when such a worker disconnects, it signifies that its containing allocation has also been
completed. Using this approach, the allocator is able to maintain the latest state of allocations
without explicitly asking the allocation manager at all.

\Autoalloc{} attempts to take certain task properties into account when creating
allocations. For example, if a task has a \emph{time request} of two hours, but a configured
allocation queue only has a time limit of one hour, the allocator will not submit allocations using
this queue to provide resources for such a task, because workers started in these allocations could
not compute such a task anyway. It should be noted that in general, it is very difficult to guess
when and how many allocations should be created, because the allocator does not know in advance
which tasks will be submitted in the future, nor for how long the currently submitted tasks will
run. It also cannot precisely predict how long its submitted allocations will stay in the
allocation manager queue. As a future extension, the allocator could be extended with a prediction
of allocation queuing times~\cite{allocation-duration-prediction} or with a prediction of task execution
times~\cite{task-duration-prediction}.

%\Autoref{sec:hq-exp-autoalloc} will evaluate how well the automatic allocation system is able to
%scale in response to computational demand and how it limits the waste of computational resources.

\section{\hyperqueue{} use-cases}
This section describes several use-cases where \hyperqueue{} has been successfully used
to execute task graphs on \gls{hpc} systems. Apart from these selected case studies,
it has also been used in other projects and scenarios; these will be enumerated at the end of this
chapter.

The presented use-cases originate from the LIGATE (LIgand Generator and portable drug discovery
platform AT Exascale)~\cite{ligate} project, whose goal was to integrate state-of-the-art
\gls{cadd} tools into a unified platform designed for executing drug design
experiments on petascale and exascale \gls{hpc} clusters. This project provided
funding for the development of \hyperqueue{}, and its use-cases also served as one of the
main driving forces behind the design of this task runtime. The usage of \hyperqueue{}
within the LIGATE project is briefly described in~\cite{ligate}.

\subsection{Virtual screening and free-energy calculations}
\label{sec:hq-usecase-ligen}
One of the \gls{md} frameworks that were used in the LIGATE project was
LiGen~\cite{ligen,ligen_exscalate}, a suite of utilities that can be used for performing virtual
screening. Virtual screening is a computational method for identifying molecules that can bind to
drug candidates, which is used for computer-driven drug discovery.

%\begin{figure}[h]
%	\centering
%	\includegraphics[width=0.9\textwidth]{imgs/hq/architecture}
%	\caption{Architecture of \hyperqueue{}.}
%	\label{fig:hq-architecture}
%\end{figure}
%
%\subsubsection*{Server}
%The \emph{server} is the most important part of \hyperqueue{}. Its main goal is
%to manage both the lifecycle of task graphs and of workers that provide computational resources. It
%keeps track of all tasks and workers, and allows users to query their state and also send commands
%to it through a client interface. The server contains a work-stealing task scheduler that is based
%on the \rsds{} scheduler implementation described in~\Autoref{sec:rsds-description}. The
%scheduler assigns individual tasks to workers based on task resource requirements and current
%computational load of workers, with the goal of maximizing hardware utilization of the connected
%workers.
%
%It is designed to be executed as a long-running background process, which should ideally be
%executed in a persistent location not managed by allocation managers (i.e.\ outside ephemeral
%allocations). Typically, it is executed on a login node of a cluster, but it can also run
%elsewhere, for example in a cloud partition. It should keep running until all tasks that the user
%wants to execute are finished, although it can also restore its state from disk, so the computation
%of a task graph can be interrupted and resumed at a later time, if needed.
%
%The fact that the server runs outside allocations allows it to load balance tasks across workers
%running in multiple allocations concurrently, and work around the various limits of allocation
%managers. This adheres to the first principle of the meta-scheduling design described in the
%previous section.
%
%The server itself does not execute any tasks, and it consumes a relatively modest amount of
%resources, and therefore it is not computationally demanding.~\Autoref{sec:hq-exp-server-cpu-consumption} evaluates how
%many \gls{cpu} resources the server consumes in extreme scenarios.
%
%Apart from basic responsibilities related to worker management and task scheduling, the server also
%provides additional functionality. For example, it can submit allocations fully automatically on
%behalf of the user. This feature will be described in detail in~\Autoref{hq:automatic-allocation}.
%
%\subsubsection*{Worker}
%The \emph{worker} component is a computational provider that executes tasks assigned to
%it by the server. A single worker typically manages all hardware resources (\gls{cpu}
%cores, \gls{gpu} or \gls{fpga} accelerators, memory, etc.) of a given
%computer, usually a single computational node of a supercomputer.
%
%The worker is able to automatically detect all relevant hardware resources that are available on
%the node where it is started and advertise them to the server. Therefore, in a typical case, users
%can simply start the worker on a given node with a single uniform command, and from that moment on
%the resources of that node will be used for task execution. This adheres to the second principle of
%the proposed meta-scheduling design, where computational providers should be generic (not tied to
%any specific task or a set of tasks), and it should be possible to start them using a uniform
%command, to make their deployment trivial.
%
%Each worker also participates in task and resource scheduling. The server makes high-level
%scheduling decisions, such as which tasks will be executed on a given worker, while the worker then
%performs micro decisions, such as in which order it will execute tasks assigned to it, or which
%specific resources will be assigned to a given task. Resource management will be described in more
%detail in~\Autoref{sec:hq-resource-management}.
%
%\hyperqueue{} workers were designed with fault tolerance in mind. On an
%\gls{hpc}
%cluster, workers will be typically executed inside allocations that only run for a limited amount
%of time, which implies that workers can disappear at any time during the computation of a task
%graph. Both the server and workers thus operate with the assumption that workers can be potentially
%short-lived; if a worker disconnects while it is executing a task, the task will be transparently
%reassigned to a different worker without user intervention. Worker management is thus dynamic and
%flexible; new workers can also be added at any time, after which they will immediately start
%contributing to the execution of the submitted task graphs. This property enables users of
%\hq{} to arbitrarily scale the computation of a task graph both up and down, even
%while it is being executed, by dynamically adding or removing workers as needed.
%
%While workers can be deployed manually by users, \hq{} can also automatically
%submit allocations to an allocation manager that start workers on computational nodes, based on
%current computational needs of submitted task graphs, as was already mentioned. This system, called
%\emph{automatic allocation}, will be described in more detail later in~\Autoref{hq:automatic-allocation}.
%
%\subsubsection*{Client interfaces}
%Users can use two main interfaces for deploying the \hyperqueue{} server and workers,
%submitting task graphs and managing and monitoring the computation. They can either use a
%\gls{cli}, which has been designed to feel familiar to users of Slurm in order to
%make it easier to migrate existing \gls{hpc} workflows to \hyperqueue{}, or
%a Python \gls{api}, which is useful for more complex use-cases that are challenging
%to express using the command line. These interfaces are relatively large, and their specific shape
%is not crucial for this thesis; the complete set of the supported \gls{cli} commands
%and their parameters and the Python \gls{api} is described in the
%\hyperqueue{} documentation~\cite{hq_docs}. In the rest of the text, we will
%show various examples of these interfaces (primarily of several \gls{cli} commands),
%to provide an idea of how ergonomic it is for users to interact with \hyperqueue{}.
%
%The \gls{cli} has been designed with the ``Simple things should be simple. Complex
%things should be possible.''\footnote{Originally coined by Alan Kay.} approach in mind. It allows submitting various
%kinds of workflows in an easy way, but also allows users to configure their execution extensively.
%This is in contrast to some other task runtimes, which require using Python code (e.g.\
%\dask{}) or workflow files (e.g.\ \snakemake{}) for executing even the
%simplest of workflows.~\Autoref{lst:hq-cli-commands} shows three basic commands that leverage the
%\texttt{hq} binary to deploy an instance of a \hyperqueue{} server and a
%worker, and then execute a single task through it. More examples will be shown later throughout
%this chapter.
%
%For simple existing workflows defined using Bash scripts, it is enough to replace uses of the
%\texttt{sbatch} command with \texttt{hq submit} to let \hyperqueue{} manage the execution
%instead of Slurm.
%
%\begin{listing}[h]
%	\begin{minted}[fontsize=\small, tabsize=4]{bash}
%# Start a HQ server
%$ hq server start &
%
%# Start a HQ worker
%$ hq worker start &
%
%# Submit a HQ job (task graph) with a single task
%$ hq submit ./my-program
%	\end{minted}
%	\caption{Examples of \hyperqueue{} \gls{cli} commands}
%	\label{lst:hq-cli-commands}
%\end{listing}
%
%\subsubsection*{Deployment}
%Even though it is often overlooked, deployment of software on supercomputers can be challenging, as
%was described in~\Autoref{challenge:deployment}. Since \hyperqueue{} aims to provide seamless
%support for \gls{hpc} use-cases, it was also designed to be trivial to deploy. It is
%distributed as a single, relatively small (approximately \SI{15}{\mebi\byte}), statically
%linked executable called \texttt{hq}, which implements all the provided functionality
%(server, worker and the whole \gls{cli}). It does not depend on any external services
%or dynamic libraries, apart from the ubiquitous \texttt{C} standard library, and it
%runs fully in userspace and does not require any elevated privileges. It also does not require any
%installation nor configuration steps.
%
%The \hyperqueue{} Python \gls{api} is then distributed as a Python package
%that is available on the standard Python Package Index~\cite{hq_pypi}. This package
%contains all the \hq{} logic inline; it is thus self-contained and it does not
%need the \texttt{hq} binary to function.
%
%Both clients and workers communicate with the server using the standard \gls{tcpip}
%protocol, which is ubiquitously available on most clusters. By default, all communication is
%encrypted, so that other users of the cluster cannot observe the data sent between the
%\hyperqueue{} components. This is performed out of an abundance of caution, because the
%server typically runs on a login node, which is shared by multiple users, rather than on
%computational nodes, which are usually isolated between users by the allocation manager. The
%performance overhead of encryption will be evaluated in~\Autoref{sec:hq-exp-encryption-overhead}.
%
%In order to connect the individual components using \gls{tcpip}, normally it would be
%required for users to specify network hostnames and ports. While this process is relatively simple,
%it still presents a minor ergonomic hurdle, because users would need to reconfigure their worker
%deployment scripts and client commands every time the server would start on a new
%\gls{tcpip} address. To make this easier, \hyperqueue{} removes the need to
%specify hostnames and ports by default, by exploiting a useful property of \gls{hpc}
%clusters, which commonly use a shared networked filesystem. When a server is created, it creates a
%configuration file in the user's home directory, which contains all information necessary for
%exchanging handshakes and connecting the components. When workers and clients discover the presence
%of this file (which can be accessed due to the filesystem being shared), they are able to connect
%to the server without the user having to specify any addresses. However, it is still possible to
%deploy multiple concurrent instances of \hyperqueue{} under the same account; users just
%need to provide separate directories for storing the configuration file.
%
%Thanks to these properties, it is trivial to deploy and use \hyperqueue{} on
%supercomputers, even without the involvement of the administrators of the target cluster. The
%deployment benefits extend also beyond \gls{hpc} clusters. Users can also trivially
%deploy \hyperqueue{} on their own personal computers. This enables them to prototype
%their \gls{hpc} workflows locally, which can accelerate their development process and
%is another step towards improving the ergonomics of using task graphs on supercomputers. This is in
%contrast with using allocation managers for submitting tasks directly, which makes local
%experimentation very difficult, as allocation managers are typically not straightforward to deploy
%on personal computers.
%
%\subsection{Programming model}
%\label{sec:hq-programming-model}
%The task-based programming model used by \hyperqueue{} builds upon the task model defined
%in~\Autoref{ch:taskgraphs}. It adds some additional \gls{hpc}-focused features on top
%of it, such as fine-grained resource management and support for multi-node tasks.
%
%The core element of the programming model is a \textbf{Task}, which has the following
%properties:
%\begin{itemize}
%	\item It is a unit of computation. A task can be executed by \hq{}, and its execution
%	      either succeeds or fails. In the current implementation, each task represents either the execution
%	      of a binary executable, or the invocation of a single Python function, although the specific
%	      details of the task execution are opaque to \hyperqueue{}.
%	\item It is a unit of scheduling. The \hq{} scheduler assigns each task to a worker (or
%	      to a set of workers in the case of multi-node tasks), and can move tasks between workers using
%	      work-stealing if their load is unbalanced.
%	\item It forms a failure boundary. When a worker crashes while executing a set of tasks,
%	      \hq{} can schedule them again on a different worker and restart their execution
%	      from scratch.
%	\item It is a unit of dependency management. Each task may depend on a set of other tasks, and in this
%	      way they may be composed in a computational \gls{dag} of tasks.
%	\item It is a unit of resource management. Each task has a set of attached resource requirements, which
%	      describe constraints on a computational environment where the task is allowed to be executed. The
%	      \hq{} scheduler then matches these requirements with resources provided by
%	      workers.
%\end{itemize}
%
%Each task has various properties, such as a path to the executed binary (or a Python function), a
%set of environment variables, working directory, and others. Each task also has a set of attached
%resource requirements, which will be described in more detail in~\Autoref{sec:hq-resource-management}.
%
%Tasks are composed into task graphs, which are called \textbf{Jobs} in the
%\hq{} terminology. Each task belongs to exactly one job, and jobs are completely
%independent, so there can be no dependencies between tasks belonging to different jobs. Jobs are
%units of monitoring and management, as they allow users to group many tasks together, observe their
%status or perform operations on all (or some) tasks of a job at once. There are two kinds of jobs,
%\emph{graph jobs} and \emph{task arrays}. Graph jobs correspond to arbitrary
%\glspl{dag}, where tasks may depend on one another. These can be only created using the
%Python \gls{api} or a \gls{toml} workflow file, since it would be very
%difficult for users to express dependencies using a command-line interface.
%
%Task arrays are a special case of graph jobs, which use a compressed representation of a
%potentially large task graph that has a regular shape. For example, a common use-case is to compute
%a task graph where each task runs the same computation, but on a different input. This can be
%achieved with a task array, which stores only a single instance of the task definition (what should
%be computed), but creates many copies of this task, each with a different input. This both reduces
%the memory usage of the server, since the task graph representation is stored in this compressed
%way, and also allows users to easily execute large task graphs using the \gls{cli}.
%
%Each task in the task array gets assigned a different \emph{task ID}, which uniquely
%identifies a specific task, which allows it to perform a computation on a different input. For
%example, if a user creates a task array with a thousand tasks, by default, the first will have ID
%$1$, the second one will have ID $2$, etc. The ID is passed to
%the task through an environment variable. Apart from assigning unique inputs to tasks through task
%IDs, \hq{} also supports passing a unique binary blob to each task (again through
%an environment variable) that can contain arbitrary data.~\Autoref{lst:hq-cli-task-arrays} shows an example
%of how users may submit task arrays through the \gls{cli}.
%
%\begin{listing}[h]
%	\begin{minted}[fontsize=\small, tabsize=4]{bash}
%# Task array with 10 tasks, each task gets a different ID
%$ hq submit --array=1-10 ./my-script.sh
%
%# Each task will receive a single line from the given file
%$ hq submit --each-line=inputs.txt ./my-script.sh
%
%# Each task will receive a single item from a JSON array stored in the given file
%$ hq submit --from-json=items.json ./my-script.sh
%	\end{minted}
%	\caption{Creating task arrays using the \hyperqueue{} \gls{cli}}
%	\label{lst:hq-cli-task-arrays}
%\end{listing}
%
%\subsubsection*{Task and job lifecycle}
%The server manages the state lifecycle of tasks and jobs and also reports them to the user, so that
%they can observe what is happening with their computation. At any given time, each task can be in a
%single \emph{state}:
%
%\begin{description}
%	\item[Waiting] After the user creates (\emph{submits}) a task, it starts in the \emph{waiting}
%		state, where it waits until it is scheduled to a worker that is able to fulfill its resource
%		requirements.
%	\item[Running] After a task is scheduled and starts executing on a worker, it moves on to the
%		\emph{running} state. If the worker that is executing the task stops or crashes, the task
%		will move back to the \emph{waiting} state, so that it can be rescheduled to a different
%		worker. This is labeled as a \emph{task restart}.
%	\item[Finished] When a task finishes successfully (a binary program exits with the exit code $0$
%		or a Python function returns without throwing an exception), it moves to the \emph{finished}
%		state.
%	\item[Failed] When a task fails (a binary program exits with a non-zero exit code or a Python function throws an
%		exception), it moves to the \emph{failed} state.
%	\item[Canceled] When a waiting or a running task is canceled by the user, it moves to the \emph{canceled}
%		state, and it is no longer considered for execution. Users can cancel tasks and jobs that are no
%		longer relevant and that should not continue executing.
%\end{description}
%
%\Autoref{fig:hq-task-state-diagram} displays a state diagram of the various task states and
%transitions that cause state changes. The \emph{finished}, \emph{failed} and
%\emph{canceled} states are terminal; once a task reaches that state, it cannot change its
%state anymore. Each job also has its associated state, which is derived from the state of its
%tasks.
%
%\begin{figure}[h]
%	\centering
%	\begin{tikzpicture}
%		\tikzstyle{every node}=[font=\footnotesize]
%		\tikzstyle{every state}=[fill={rgb:black,1;white,8},minimum size=1.5cm,font=\footnotesize]
%
%		\node[state,initial,initial text=Submitted]   	(waiting)  							{Waiting};
%		\node[state] 							   		(running) 	[right=2.5 of waiting]	{Running};
%		\node[state,accepting]           		   		(finished) 	[right=2 of running]	{Finished};
%		\node[state,accepting]                     		(failed) 	[above=1 of finished] 	{Failed};
%		\node[state,accepting]                     		(canceled)	[below=1 of finished]   {Canceled};
%
%		\path[->]
%		(waiting)   edge [bend left]	node [above] 			{Scheduled} 		(running)
%		(waiting)   edge [bend right]	node [below left]		{Canceled}			(canceled)
%		(running)	edge    			node [above] 			{Finished}			(finished)
%		(running) 	edge    			node [above left] 		{Failed} 			(failed)
%		(running)	edge    			node [left,xshift=-5]	{Canceled} 			(canceled)
%		(running)	edge [bend left]    node [below,xshift=8]	{Worker crashed}	(waiting);
%	\end{tikzpicture}
%	\caption{State diagram of \hyperqueue{} tasks}
%	\label{fig:hq-task-state-diagram}
%\end{figure}
%
%\subsubsection*{Iterative computation}
%Iterative computation, such as running a simulation until it converges or training a
%machine-learning model, can be expressed using the \hyperqueue{} Python
%\gls{api}. It provides facilities for executing a task graph, waiting until it
%finishes computing and then submitting a new task graph based on the computed results. While this
%is enough for describing basic scenarios that require dynamic iteration, it is not fully ergonomic,
%as it only provides dynamic submission of new computation on the granularity level of jobs (task
%graphs), not individual tasks.
%
%A more general solution would be to allow adding new tasks to already submitted
%\hyperqueue{} jobs, which would enable tasks themselves to submit new tasks into their
%own task graph. This would make it easier to express arbitrarily dynamic task graph structures,
%where the shape of the graph is not fully known before their computation starts. We plan to
%implement support for this feature as a future extension.
%
%\subsection{Task input and output}
%By default, each \hyperqueue{} task stores its \emph{standard output} (\emph{stdout}) and
%\emph{standard error output} (\emph{stderr}) streams to the filesystem. Users can override this to avoid storing these
%output streams, or to let \hq{} remove them if they are empty after the task has
%been finished, to reduce disk clutter. It is also possible to use various placeholders for
%configuring the paths of these streams. For example, the string \texttt{\%\{TASK\_ID\}} in a stream
%path will get resolved to the task ID of the task that produces the stream. This can be used to
%easily separate the output of different tasks of task arrays, for example.
%
%Since task graphs can be large, generating two files per task, especially on shared network
%filesystems that are commonly used on supercomputers, could cause performance bottlenecks or run
%into limitations caused by user disk quotas. To alleviate this problem, \hq{}
%supports an output management system called \emph{output streaming}. If it is enabled for a given
%job, then \emph{stdout} and \emph{stderr} of tasks is streamed over the network
%from workers to the server, instead of being stored into separate files. The server then stores all
%this data into a single file, and then offers several commands for manipulating and filtering task
%output data from this file. This helps to reduce the pressure put on the
%filesystem.~\Autoref{sec:hq-exp-output-streaming} will evaluate how this system helps with potential
%\gls{io} issues.~\Autoref{lst:hq-cli-log} shows several \hq{}
%commands that can be used to interact with output streaming.
%
%\begin{listing}[h]
%	\begin{minted}[fontsize=\small, tabsize=4]{bash}
%# Submit a job with output streaming enabled
%$ hq submit --log=log.bin --wait -- ./my-program
%
%# Print stdout of tasks 1 to 10 from the log
%$ hq log log.bin cat --task 1-10 stdout
%
%# Export the log into a JSON format
%$ hq log log.bin export
%	\end{minted}
%	\caption{\hyperqueue{} \gls{cli} commands for working with output streaming}
%	\label{lst:hq-cli-log}
%\end{listing}
%
%Note that \hyperqueue{} currently does not support data transfers between tasks, as it is
%primarily focused on executing tasks that execute black-box binaries which communicate through
%files on a file-system. Arbitrary binary data can be attached as an input for any task (through its
%\emph{standard input} stream), but task outputs are only used for specifying dependencies; no
%task outputs can actually be transferred between the workers. This is only relevant for the Python
%\gls{api}, because tasks submitted using the \gls{cli} cannot express
%data transfers. The \hq{} server is based on the \rsds{} server
%implementation, which supports data transfers between workers, so the transfer functionality itself
%is actually implemented in \hq{}. However, data transfers still need to be
%incorporated into the \hyperqueue{} programming model, and a user-facing (Python)
%interface for consuming outputs of tasks needs to be designed. It should be noted that this is
%merely a limitation of the current implementation, which we plan to lift in the future.
%
%\subsection{Resource management and scheduling}
%\label{sec:hq-resource-management}
%As has already been mentioned, the \hyperqueue{} scheduler is derived from the
%\rsds{} work-stealing scheduler that was described in~\Autoref{sec:rsds-description}. It
%assigns tasks to workers eagerly, to optimize the fast path where there are enough tasks to utilize
%the available workers. When some workers are under-utilized, the server uses work-stealing to steal
%tasks between workers to balance the computational load. Scheduling is performed on the level of
%tasks, regardless of which job they belong to. In fact, the scheduler does not even know anything
%about jobs.
%
%What makes the scheduler different from \rsds{} is its support for complex resource
%management. It takes into account various resource requirements requested by tasks, and also the
%state of currently available resources on each worker, and uses this information to drive its
%scheduling decisions. The most important responsibility of this resource management is to uphold
%the invariant that each task will be executed on a worker that is able to provide the corresponding
%amount of each resource requested by the task at the time of the task's execution. When a task is
%being scheduled, the scheduler looks for a worker that can satisfy the resource requirements of the
%task, based on the amount of resources provided by the worker in general, and also the amount of
%resources that the worker currently has available. When a task is scheduled onto a worker, the
%scheduler \emph{allocates} a specific subset of the worker's resources to that task, which
%will not be available for other tasks until the scheduled tasks finished executing.
%
%\hyperqueue{} resources are \emph{generic}, as users can define their own
%arbitrary resources for both tasks and workers, although \hq{} also recognizes
%several known resource names and provides specialized support for them. Resource management in
%\hyperqueue{} consists of two main elements, resource requirements specified by tasks and
%resource pools provided by workers.
%
%\subsubsection*{Worker resource pools}
%Worker resources are configured when a worker is started through so-called \emph{resource pools},
%named sets of resources. The values of the most common resource pools, like the
%\gls{cpu} cores and \gls{numa} sockets, the amount of
%\gls{ram} and the presence of \gls{gpu} accelerators, are detected
%automatically. On top of that, users can specify an arbitrary set of additional pools. There are
%two kinds of resource pools:
%
%\begin{description}
%	\item [Indexed pool] represents a set of resources where each individual resource is
%	      non-fungible and has a unique identity (represented either by an integer or a string). An example
%	      of such a resource pool could be a set of \gls{cpu} cores or \gls{gpu}
%	      devices. For example, if a worker has two NVIDIA \glspl{gpu}, that worker could provide
%	      an indexed pool containing two resources (e.g.\ with indices $0$ and
%	      $1$), each representing one individual \gls{gpu} device. These two
%	      resources are not interchangeable; the scheduler decides which specific resource will be assigned
%	      to a task, and it tracks the individual resources based on their identity. If a task
%	      $t$ is currently using \gls{gpu} $0$, that
%	      resource will not be assigned to any other task until $t$ has finished executing.
%
%	      Individual resources of an indexed pool can be further subdivided into \emph{groups},
%	      which mark some form of relationship between specific resources. This can be used e.g.\ to
%	      represent \gls{numa} domains and \gls{cpu} sockets; a worker might provide
%	      a single indexed pools of \glspl{cpu}, with a separate group for each
%	      \gls{numa} socket present on the worker's node. Tasks can then specify if they want to
%	      prioritize drawing resources that belong to the same group. This will be described later.
%	\item [Sum pool] is designed for resources that are fungible. Such resources are typically
%	      numerous, and it would not be practical to track each individual resource separately. A sum pool
%	      thus does not define its individual resources through indices, but only defines a single number
%	      that specifies the amount of resources available in the pool. A typical representative of a sum
%	      pool is \gls{ram} available on a worker. Modern \gls{hpc} nodes can
%	      contain hundreds of GiBs of memory, or even more, and it is not feasible to treat each byte as a
%	      separate resource. At the same time, tasks usually do not need such fine-grained tracking; if they
%	      care about memory requirements, they just specify how much memory they need.
%\end{description}
%
%\Autoref{lst:hq-cli-worker-resources} shows a few examples of how users can configure resources
%when starting workers through the \gls{cli}.
%
%\begin{listing}[h]
%	\begin{minted}[fontsize=\small, tabsize=4]{bash}
%# Start a worker with automatically detected resources
%$ hq worker start
%
%# Start a worker that manages 16 CPU cores
%$ hq worker start --cpus 16
%
%# Start a worker with additional resources
%# Adds an indexed pool of four GPUs and a sum pool of 1024 resources (bytes)
%$ hq worker start \
%	--resource 'gpus/nvidia=[0,1,2,3]' \
%	--resource 'memory=sum(1024)'
%	\end{minted}
%	\caption{Configuring worker resources using the \hyperqueue{} \gls{cli}}
%	\label{lst:hq-cli-worker-resources}
%\end{listing}
%
%\subsubsection*{Task resource requirements}
%Resource pools define the resources that are available on workers. The scheduler then uses
%\emph{resource requirements} attached to individual tasks to decide on which workers a given task can be
%executed, and which specific resources it will consume while the task is running. Task resource
%requirements are defined for each task separately. By default, each task requires a single
%\gls{cpu} core, but users can specify a different \gls{cpu}
%requirement, and also add additional arbitrary requirements.
%
%A resource requirement consists of a name of the resource that the task needs, and an amount
%specifying how many such resources are needed. For example, a task might specify that it requires
%two \glspl{gpu} for its execution. The resource requirement always specifies the amount
%of required resources, regardless of whether the given resource is taken from an indexed or a sum
%pool. Therefore, tasks cannot ask for specific instances of resources based on their identity; the
%scheduler decides which specific resources will be assigned to individual tasks, which makes
%scheduling more flexible. For example, assume that a worker has $128$ cores (each
%with their own identity), and there is a task that requires two cores. It would not be very
%practical if the task had to specify that it specifically requires cores with ID
%$0$ and $1$ or $5$ and
%$8$, etc. Instead, it just specifies the number of \gls{cpu} cores
%it needs, and lets the scheduler decide which specific cores it will assign to the task, based on
%what cores will be available on the given worker at the time the task is about to start executing.
%
%In addition to the basic system described above, resource requirements can also leverage a few
%additional parameters that allow tasks to describe more complex use-cases related to resource
%management.
%
%A \emph{group allocation strategy} can be used to affect which specific resources from an indexed pool will
%be used for the task. It is not relevant for indexed pools with a single group nor for sum pools. A
%typical example where indexed pool groups are useful are \gls{numa} nodes, where tasks
%might want to be executed on cores that belong to the same \gls{numa} node (group). There are three
%different strategies that can be selected:
%\begin{description}
%	\item [Compact] is the default strategy. The scheduler tries to allocate all requested
%	      resources from as few groups as possible. However, if there are not enough resources currently
%	      available for that on a given worker, it will also try to draw resources from a larger number of
%	      groups, to satisfy the resource requirement.
%	\item [Strict compact] is a stricter version of the \emph{compact} strategy. With this
%	      strategy, the scheduler first figures out the minimum number of groups required to satisfy the
%	      resource requirement, and it will then not schedule the task until it can provide all requested
%	      resources from the smallest number of groups possible.
%	\item [Scatter] is the opposite of the \emph{compact} mode. It tries to allocate resources
%	      for the task from as many groups as possible.
%\end{description}
%
%Resource requirements can also be specified without an upper bound, i.e.\ a task can specify that
%it wants to consume all available resources of the specified pool. If a worker has at least a
%single resource of the given pool available, and the scheduler assigns a task with this kind of
%resource requirement to it, that task will be allocated all the worker resources that are available
%when the task starts to execute.
%
%In some cases, requesting whole resources can still be too coarse-grained for some tasks. For
%example, a task might execute software that is able to leverage \gls{gpu}
%acceleration, but it cannot utilize a whole \gls{gpu} device by itself. If such task
%has requested a whole \gls{gpu}, part of that accelerator's hardware might sit idle
%while the task is executed. To avoid this situation, \hyperqueue{} allows tasks to define
%\emph{fractional resources}, which allow multiple tasks to share a single resource at the same time.
%For example, the mentioned task could ask for only $0.5$ of a
%\gls{gpu}, in which case the scheduler would allocate the same
%\gls{gpu} device to up to two tasks at once. The scheduler uses fixed-point
%arithmetic to manage fractional resources, in order to avoid common issues associated with
%floating-point arithmetic, rounding errors.
%
%An even more complex resource management use-case supported by \hyperqueue{} is the
%ability to specify \emph{resource variants}. Normally, each resource requirement specified by a task
%is additive; all the requested resources have to be available in order for the task to start
%executing. However, \hq{} also allows tasks to define a list of resource
%requirement sets (called resource variants), and lets the scheduler use the first one that can be
%satisfied by a worker at the time of scheduling (the order of the list elements decides the
%priority of the individual requirement sets). Resource variants allow tasks to define multiple
%situations under which it makes sense to execute the task, and let the scheduler decide which
%variant to select, based on dynamic information about the current worker resource. As an example,
%many \gls{hpc} software tools can run either on a \gls{cpu} or on a
%\gls{gpu} device. A task using such software might specify e.g.\ that it needs either
%a single \gls{gpu} and a single \gls{cpu}, or that it needs
%$16$ \gls{cpu} cores (in the case that a \gls{gpu} is
%not available). In this case, the scheduler could decide to execute the task purely using
%\gls{cpu} resources, if there are no \gls{gpu} resources currently
%available. Without support for resource variants, the scheduler would need to wait for a
%\gls{gpu} to become available, even if there were free \gls{cpu}
%resources that could be used in the meantime.
%
%There is also one additional special resource that is relevant for scheduling, called a
%\emph{time request}. If a task specifies a time request (a duration) $t$, it
%tells the scheduler that it needs at least $t$ time for its execution. This is
%important for avoiding needless task restarts, due to the way \hq{} workers are
%commonly deployed in allocations. Workers started in allocations cannot run for a longer time than
%the wall-time of the allocation. Since this duration is known (each allocation must have a
%wall-time), \hq{} knows the remaining lifetime of each such worker at any given
%time. This information is used for scheduling; if a worker has remaining lifetime
%$l$, the scheduler will not assign it tasks with a time request that is longer
%than $l$, because it is highly probable that such tasks could not finish
%executing before that worker shuts down.
%
%Once a task is scheduled to a worker, and the scheduler decides which resources will be allocated
%to it, it passes this information to the task through environment variables. For example, if a task
%asks for two \gls{cpu} cores, and the scheduler allocated the cores
%$10$ and $14$ of a specific worker to it,
%\hq{} will pass \texttt{HQ\_RESOURCE\_VALUES\_cpus=10,14} to the task.
%
%It is important to note that \hyperqueue{} treats resources as opaque and virtual; they
%are used only for scheduling. \hq{} does not enforce that e.g.\ if a task is
%provided the cores $0$ and $1$, it will be only executed on
%these specific cores. This responsibility is left to the task itself, which can use the provided
%environment variable to decide using which resources it should actually be executed. For example, a
%task could pin its executed program on cores allocated to it. In the case of
%\gls{cpu} cores, \hq{} can even pin the allocated cores for the
%task automatically, if the task is configured for pinning.
%
%\begin{listing}[h]
%	\begin{minted}[fontsize=\small, tabsize=4]{bash}
%# Submit a task requiring 4 CPU cores
%$ hq submit --cpus=4 ./my-program
%
%# Submit 1000 tasks, each requiring 2 CPU cores
%$ hq submit --cpus=2 --array=1-1000 ./my-program
%
%# Submit a task requiring 2 CPU cores, enable CPU pinning through OpenMP
%$ hq submit --cpus=2 --pin=omp ./my-program
%
%# Submit a task that requires 2 FPGA devices and needs at least 10 minutes to execute
%$ hq submit --resource 'fpga=2' --time-request 10m ./my-program
%	\end{minted}
%	\caption{Configuring task resource requirements using the \hyperqueue{} \gls{cli}}
%	\label{lst:hq-cli-task-resources}
%\end{listing}
%
%\subsubsection*{Multi-node tasks}
%\hyperqueue{} also supports a special kind of resource that is especially useful for
%tasks that want to execute \gls{hpc} computations which are designed to run on
%multiple nodes (e.g. \gls{mpi} programs). A task may specify that it wants to be
%executed on multiple workers at the same time. In this case, \hq{} assumes
%exclusive ownership of all the resources of each worker (node), which typically matches the
%requirement of \gls{mpi}-like applications. A multi-node task requiring
%$n$ nodes can only be scheduled when $n$ workers are idle
%(not executing any tasks) at the same time. When a multi-node task starts to execute,
%\hq{} passes it a \emph{nodefile} which contains the hostnames of all
%workers allocated to the task. This nodefile can then be used by common \gls{mpi}
%program launchers to start an \gls{mpi} program.
%
%Users that execute multi-node tasks running \gls{mpi} applications might want to make
%sure that all workers (nodes) contributing in this computation are started in the same allocation,
%for performance reasons (e.g.\ they might configure allocations to use nodes that reside close
%together in the networking topology of the cluster). By default, \hyperqueue{} tries to
%adhere to this constraint. It introduces an abstract concept of \emph{worker groups}, and always
%schedules a multi-node task to workers that belong to the same group. At the same time, it
%automatically configures the group of a worker based on the allocation inside which it is executed
%(if any). This ensures that multi-node tasks are by default executed only on workers that reside
%within the same allocation. However, it is possible to override the group of a worker, and e.g.\
%assign the same group to all workers. In that case, workers from all active allocations will be
%used to participate in multi-node tasks.
%
%\subsection{Fault tolerance}
%The execution of a task in \hyperqueue{} can end in a failure for two main reasons. The
%worker that executes the task might crash, be stopped by the user, or an allocation inside which it
%is executed can run out of its wall-time. In that case, the task will be rescheduled by the server
%to a different worker that upholds its resource requirements. This is made possible due to the fact
%that tasks are not tied to any specific computational resource or allocation. When such a task
%restart happens, it is important to let the executed program know that it is not being executed for
%the first time, so that it can react to it (for example clean up files that were created during a
%previous execution of the same task or restore execution from a previously created checkpoint).
%This is achieved using an \emph{instance ID}. It is a non-negative number attached to each
%task that is increased every time the task is restarted. This number is then passed to the executed
%task, so that it can decide whether it should react to the restart in any specific way.
%
%In rare cases, the crash of the worker might be caused by the task itself, e.g.\ if it allocates
%too much memory and causes an ``\gls{oom}'' error. If this happens repeatedly, such
%task could hamper the execution of the whole task graph, since it would be repeatedly crashing
%workers. For that reason, it is possible to configure the maximum number of worker crashes per task
%(it is configured to a low number by default). If a task exceeds this number (i.e.\ it is present
%during too many worker crashes), it will be automatically canceled.
%
%The second reason why a task might fail is that its computation simply does not succeed, for
%example due to the executed process returning a non-zero exit code. In that case, the task is
%marked as failed, and any tasks that (transitively) depend on it are canceled.
%\hyperqueue{} allows a simple way of querying tasks by their state and resubmitting only
%failed and/or canceled tasks in a straightforward way, which is possible due to the job state being
%kept outside of ephemeral allocations.~\Autoref{lst:hq-cli-fault-tolerance} demonstrates how users can use the
%\hq{} \gls{cli} to resubmit failed tasks from a previously
%submitted job. It also shows that several \hq{} commands are designed to be
%composable, adhering to the Unix philosophy of creating commands that do a single thing well and
%that can be easily combined together.
%
%\begin{listing}[h]
%	\begin{minted}[fontsize=\small, tabsize=4]{bash}
%# Submit task array with 1000 tasks, wait until it completes
%$ hq submit --array=1-1000 --wait ./my-script.sh
%
%# Find task IDs that have failed in the last job
%$ FAILED_TASKS=$(hq job task-ids last --filter=failed)
%
%# Resubmit only the failed tasks
%$ hq submit --array=${FAILED_TASKS} ./my-computation
%	\end{minted}
%	\caption{Handling task failure using the \hyperqueue{} \gls{cli}}
%	\label{lst:hq-cli-fault-tolerance}
%\end{listing}
%
%Too many tasks failing can signal the presence of some catastrophic condition, e.g.\ the user
%forgot to configure their program properly, load some required runtime dependencies or used the
%wrong filesystem path for inputs. If this happens for a very large job, it could lead to a lot of
%wasted computation, where tasks will start to execute, but they will probably fail very soon after
%that. \hyperqueue{} thus allows users to configure a parameter that specifies the maximum
%number of task failures per job. If the configured number of failures is exceeded,
%\hq{} will preemptively cancel the whole job to avoid wasting computation.
%
%A special case of a failure is when the server itself crashes or it is stopped by the user, or when
%a worker loses connection to the server due to networking issues. In this case, the worker cannot
%reasonably continue executing long-term, without having a communication channel with the server.
%However, if it stores tasks that are already being executed, it could be wasteful to stop their
%computation just because the connection to the server has been lost. The worker can thus be
%configured in a way that it will finish executing such running tasks before shutting itself down.
%However, the completion of these tasks will not be reported back to the server, so users will need
%to handle their output on disk manually.
%
%It is possible to configure the server to store a log containing all necessary information about
%its task database to the filesystem. The server can thus be restored in case of a crash or a
%failure to resume previously unfinished computation of tasks.
%
%\subsection{Automatic submission of allocations}
%\label{hq:automatic-allocation}
%The meta-scheduling design employed by \hyperqueue{} resolves most of the challenges
%associated with using allocation managers for executing tasks, since it fully automates the mapping
%of tasks to allocations. It also makes creating allocations simple, so that users can scale the
%computational resources used for computing their task graphs at their will. However, creating
%allocations manually can still be relatively demanding for users, who might want to employ a more
%automated approach for scaling computational resources. Since \hyperqueue{} knows the
%state of all tasks and workers, and it is commonly executed on login nodes that have access to
%allocation managers, it is natural to provide it with the ability to automatically submit new
%allocations on behalf of the user, based on current computational load. This system has been
%implemented in \hq{} under the name \emph{Automatic allocator} (shortened as \autoalloc{}). The automatic
%allocator was created with the following design goals:
%
%\begin{description}[wide=0pt]
%	\item[Allow computational resources to scale up] At any given moment, if there are tasks that are waiting to be executed, and there are no free
%		computational resources, \autoalloc{} should attempt to add more resources by creating
%		new allocations. At the same time, it should respect backpressure from the allocation manager to
%		avoid overloading it.
%	\item[Allow computational resources to scale down] Keeping allocations running on a supercomputer can be quite costly. \Autoalloc{} should
%		thus make sure to shutdown allocations that are not performing useful work because they do not have
%		anything to compute anymore, in order to avoid wasting resources.
%	\item[Be flexible] Allocation managers typically provide various configuration knobs that can affect the behavior of
%		allocations. While it will not ever be possible to support all possible implementations of
%		allocation managers out of the box, \autoalloc{} should provide extension points that
%		enable users to configure the submitted allocations to their liking.
%\end{description}
%
%\Autoalloc{} has been implemented as a background service within the
%\hyperqueue{} server. To use it, users first need to create at least one
%\emph{allocation queue}. Such a queue describes a template for creating new allocations, which will
%be submitted by \hq{} when there is a demand for more computational resources.
%Each queue contains several properties that can be configured by users:
%
%\begin{description}
%	\item[Allocation manager] \Autoalloc{} needs to communicate with a given allocation manager
%		to be able to submit allocations into it. Users thus need to tell \hq{} which
%		allocation manager it should communicate with. Currently, two managers are supported, Slurm and
%		\gls{pbs}.
%	\item[Time limit] This determines the wall-time of each allocation submitted by \autoalloc{}. Knowing the
%		maximum duration of each allocation helps it decide when it makes sense to create allocations. For
%		example, if the only tasks waiting to be computed have a \emph{time request} that is longer than
%		the time limit of a given allocation queue, it does not make sense to create allocations in this
%		queue, because workers started in these allocations could not execute such tasks anyway.
%	\item[Backlog] This parameter specifies the maximum number of allocations that can be queued in the allocation
%		manager at any given time. It is designed to ensure that the automatic allocator does not overload
%		the allocation manager.
%	\item[Worker count per allocation] Determines how many nodes (workers) should be requested for each allocation. Unless users want to
%		execute multi-node tasks, it is usually convenient to use the default value
%		($1$), because typically, the fewer nodes are requested in an allocation, the
%		sooner it will be started by the allocation manager.
%	\item[Max worker count] This parameter can be used to limit the maximum number of running workers across all allocations
%		started by this queue.
%	\item[Idle timeout] Time after which an idle worker (a worker that does not have any tasks to compute) will stop. The
%		idle timeout mechanism helps to avoid resource waste, by stopping workers that do not have anything
%		to do anymore. By default, this timeout is set to a few minutes.
%	\item[Worker resources] Users can override the worker resources assigned to each worker started by the given allocation
%		queue. This can be used if the user knows that the allocations will be started in a specific
%		partition of the cluster that contains resources that cannot be automatically detected by
%		\hyperqueue{} (e.g. \gls{fpga} devices).
%	\item[Custom allocation parameters] Users can also specify arbitrary command-line parameters that will be passed to the submit command
%		sent to the corresponding allocation manager, like the computational project that should be used,
%		or the Slurm partition where the allocation should be created. To make debugging easier,
%		\hyperqueue{} will submit a test allocation by default immediately after the allocation
%		queue is created, to test that the allocation parameters can be handled by the manager. If this
%		test allocation is successfully created, it is then immediately canceled to avoid wasting
%		computational resources.
%\end{description}
%
%\Autoref{lst:hq-cli-autoalloc} shows several examples of commands for creating allocation queues and
%observing allocation state using the \hyperqueue{} \gls{cli}.
%
%\begin{listing}[h]
%	\begin{minted}[fontsize=\small, tabsize=4]{bash}
%# Create an allocation queue for Slurm
%$ hq alloc add slurm --time-limit 1h -- -Aproject-1 -pcpu_partition
%
%# Create an allocation queue for PBS with GPU resources
%$ hq alloc add pbs --resource "gpus/nvidia=range(1-2)" ...
%
%# Display information about (in)active allocations from the given queue
%$ hq alloc info 1
%	\end{minted}
%	\caption{Handling task failure using the \hyperqueue{} \gls{cli}}
%	\label{lst:hq-cli-autoalloc}
%\end{listing}
%
%\Autoalloc{} is a reactive system. It observes events from the server (such as new jobs
%being submitted or new workers being connected) and manages allocations in response to these
%events. If it encounters a situation where there are tasks that are in the waiting state, and no
%workers can execute them (either because there are no workers or all of them are fully occupied),
%it will start submitting allocations to the allocation manager. It will submit allocations up until
%the configured \emph{backlog} value, or until the allocation manager applies backpressure
%(whichever comes sooner). Note that \autoalloc{} does not limit the total number of
%allocations that are ``in-flight'' (queued or running) by default, just the queued ones. This
%allows it to potentially create a large number of allocations and thus make use of all available resources of
%the cluster, if the allocation manager allows it. Users can use other configuration parameters
%(such as the total number of workers) to limit the amount of submitted resources.
%\Autoalloc{} also contains an internal rate limiter that makes sure that it does not
%submit allocations too often and that it pauses the allocation process if the allocations start
%failing too often.
%
%\hyperqueue{} communicates with \gls{pbs} or Slurm using standard
%commands, such as \texttt{qsub} or \texttt{sbatch}, as they do not provide a complete
%machine-readable \gls{api}. This means that in order to use automatic allocation, the server has to
%be executed on a node that has access to these commands and can communicate with the manager. If
%this is not possible, a proxy service could be used to forward communication between the node
%with the server and a node where the manager is deployed. \hyperqueue{} implements communication
%with each manager in a custom way. While there have been some attempts to create a unified
%interface~\cite{psij,workflow-alloc-manager-comm} for submitting allocations to different
%allocation managers, they have not seen wide adoption so far. Furthermore, adopting these
%approaches would require \hq{} to introduce a dependency on Python or an external \gls{http}
%service, which would make it more difficult to deploy.
%
%It is crucial for \autoalloc{} to maintain an updated state of its created allocations,
%so that it can report their state to the user, and also so that it knows if it can submit new
%allocations. Originally, it was using a polling approach to determine the latest state, where it
%was repeatedly querying the allocation manager about the state of allocations. However, this turned
%out to put a lot of pressure on the allocation managers, which were not very optimized for this
%use-case, especially if the polling frequency was very high (every few seconds). Some allocation
%managers are even configured in a way where they cache the state of allocations when they are
%queried too often; this reduces the chance of them being overloaded, but it also means that
%\hyperqueue{} would not get the latest data, which is not ideal. However, we can observe
%that it is not actually required to poll the allocation manager. The only allocation events that
%are important for the allocator are the start and an end of an allocation. And this information can
%be gained through a proxy; the connections are disconnections of \hq{} workers.
%When a worker is started in an allocation, it will connect to the server, and tell it its
%allocation ID\@. This lets the automatic allocator learn about new allocations. On the other hand,
%when such a worker disconnects, it signifies that its containing allocation has also been
%completed. Using this approach, the allocator is able to maintain the latest state of allocations
%without explicitly asking the allocation manager at all.
%
%\Autoalloc{} attempts to take certain task properties into account when creating
%allocations. For example, if a task has a \emph{time request} of two hours, but a configured
%allocation queue only has a time limit of one hour, the allocator will not submit allocations using
%this queue to provide resources for such a task, because workers started in these allocations could
%not compute such a task anyway. It should be noted that in general, it is very difficult to guess
%when and how many allocations should be created, because the allocator does not know in advance
%which tasks will be submitted in the future, nor for how long the currently submitted tasks will
%run. It also cannot precisely predict how long its submitted allocations will stay in the
%allocation manager queue. As a future extension, the allocator could be extended with a prediction
%of allocation queuing times~\cite{allocation-duration-prediction} or with a prediction of task execution
%times~\cite{task-duration-prediction}.
%
%%\Autoref{sec:hq-exp-autoalloc} will evaluate how well the automatic allocation system is able to
%%scale in response to computational demand and how it limits the waste of computational resources.
%
%\section{\hyperqueue{} use-cases}
%This section describes several use-cases where \hyperqueue{} has been successfully used
%to execute task graphs on \gls{hpc} systems. Apart from these selected case studies,
%it has also been used in other projects and scenarios; these will be enumerated at the end of this
%chapter.
%
%The presented use-cases originate from the LIGATE (LIgand Generator and portable drug discovery
%platform AT Exascale)~\cite{ligate} project, whose goal was to integrate state-of-the-art
%\gls{cadd} tools into a unified platform designed for executing drug design
%experiments on petascale and exascale \gls{hpc} clusters. This project provided
%funding for the development of \hyperqueue{}, and its use-cases also served as one of the
%main driving forces behind the design of this task runtime. The usage of \hyperqueue{}
%within the LIGATE project is briefly described in~\cite{ligate}.
%
%\subsection{Virtual screening and free-energy calculations}
%\label{sec:hq-usecase-ligen}
%One of the \gls{md} frameworks that were used in the LIGATE project was
%LiGen~\cite{ligen,ligen_exscalate}, a suite of utilities that can be used for performing virtual
%screening. Virtual screening is a computational method for identifying molecules that can bind to
%drug candidates, which is used for computer-driven drug discovery.
%
%%\begin{figure}[h]
%%	\centering
%%	\includegraphics[width=\textwidth]{imgs/hq/ligen-workflow}
%%	\caption{LiGen high-throughput virtual screening}
%%	\label{fig:ligen-virtual-screening}
%%\end{figure}
%
%%TODO: diagram
%%\begin{figure}[h]
%%	\centering
%%%	\resizebox{!}{35mm}{
%%		\begin{tikzpicture}
%%			\tikzset{%
%%				data/.style={rectangle, draw, rounded corners, minimum size=4mm},
%%			}
%%			\tikz{
%%				(0, 0) node[data] {Ligands (SMI)};
%%			}
%%		\end{tikzpicture}
%%%	}
%%	\caption{Simple task graph with six tasks and six data objects}
%%	\label{fig:task-graph-example}
%%\end{figure}
%
%LiGen was used to implement a high-throughput virtual screening pipeline, whose goal was to assign
%scores to a large number of input ligands, and then perform molecular docking for the most
%promising ligand candidates. The primary input for the workflow is a set of ligands in the
%SMILES~\cite{smiles} format, where each line represents a single ligand. LiGen then
%expands each ligand into a 3D representation stored in a Mol2~\cite{mol2} format, and
%performs virtual screening to assign a score to it. The resulting ligand-score pairs are then
%stored in a \gls{csv} file. After that, the most promising $N$
%candidates are selected, based on their scores, and molecular docking is then performed on them.
%
%The virtual screening workflow has been implemented using the \hyperqueue{} Python
%\gls{api}, due to its support for task dependencies, and also because it was natural
%to implement several utility tasks within the workflow in Python, rather than e.g.\ in Bash. The
%workflow consists of both very short running tasks (such as ligand expansion) and also more
%computationally demanding tasks (such as scoring and docking). The scoring and docking stages can
%also run on \gls{gpu} accelerators, which provide much higher throughput than the
%\gls{cpu} implementation. These constraints were encoded using
%\hq{} resource requirements, which allowed selecting the proper hardware
%resources for each kind of task. \Autoref{sec:hq-exp-ligen} describes an experiment that measures the
%achieved hardware utilization when running this workflow with \hyperqueue{}.
%
%This workflow was executed on four computational sites; the Karolina~\cite{karolina},
%LUMI~\cite{lumi}, LEONARDO~\cite{leonardo} and E4~\cite{e4}
%\gls{hpc} clusters. The only change required to port the workflow to a new cluster
%was to update the Slurm credentials used for submitting allocations. This demonstrates the
%simplicity of porting \hyperqueue{} workflows between different supercomputers.
%
%The output of the scoring and docking stages of the virtual screening workflow then serve as input
%for a more complex \gls{cadd} workflow that is designed for performing free-energy
%calculations that estimate the \gls{rbfe} of ligands and protein-hybrid ligand
%complexes using \gls{awh} computation. The \gls{cadd} workflow uses the
%GROMACS \gls{md} framework, along with tens of other bioinformatics tools and
%packages, and is structurally quite complex, as it consists of tens of different Bash and Python
%scripts that depend on the outputs of one another. This workflow has been also implemented using
%the \hyperqueue{} Python \gls{api}, to encode the dependencies between
%tasks and their resource requirements. The final \gls{awh} step, which is the most
%computationally intensive part of the workflow, uses iterative computation.
%%TODO: reword/implement
%
%The source code of the LiGen virtual screening and the \gls{cadd} workflows is
%publicly available~\cite{cadd-workflow}.
%
%\subsection{Pose selection}
%The \emph{Pose selection} workflow was one of the supporting workflows of the LIGATE project. Its
%purpose was to generate a large training dataset that would then be used to train machine-learning
%models designed to improve the accuracy of protein-ligand complex pose scoring and docking
%performed by the LiGen virtual screening tool suite.
%
%A single task of the workflow estimates the stability of an individual pose of a protein-ligand
%complex (a drug candidate). It leverages the GROMACS~\cite{gromacs}
%\gls{md} framework for simulating a short (\SI{100}{\pico\second}) trajectory for
%each pose, from which a final \gls{abfe} value is calculated. Because this computation
%is relatively lightweight (it only lasts for a few minutes), it was possible to run it on a large
%dataset of inputs. It was executed for more than four thousand complexes from the
%PDBbind~\cite{pdbbind} v2020 database; for each complex, up to $256$
%different poses were evaluated, and for each pose, $8$ simulation replicas were
%executed to reduce statistical error (since the computation is randomized). This resulted in more
%than $6.5$ million GROMACS simulations that had to be performed in total.
%
%The workflow itself is not structurally complex; each task is independent and there are no
%dependencies between tasks. However, due to the sheer amount of simulations that needed to be
%performed, it would be quite challenging to execute such a large number of tasks directly using
%Slurm allocations. \hyperqueue{} was thus used to define the computational task graph and
%manage the executions of all the simulations. Each task executed all eight replicas (independent
%simulations) for a given complex pose, either on $8$ \glspl{gpu} or
%$16$ \gls{cpu} cores in parallel. \hyperqueue{} thus had
%to schedule and manage approximately $800$ thousand tasks in total. The automatic
%allocator took care of automatically submitting Slurm allocations, users of the workflow thus did
%not need to concern themselves with creating them manually.
%
%Another benefit of using \hyperqueue{} for this workflow was its built-in support for
%fault-tolerance. With such a large number of tasks, some of them necessarily failed, either because
%of a Slurm allocation ending in the middle of a computation, or because of an instability of the
%used \gls{hpc} cluster. Thanks to \hyperqueue{}, such failed tasks were
%transparently rescheduled to different computational nodes, without any user intervention. This was
%possible due to the fact that the \hq{} server was executed on a login node, and
%thus kept the state of the submitted tasks in a persistent database that was not affected by
%shutdowns or failures of Slurm allocations.
%
%This workflow has been executed over the course of four months, and has consumed a non-trivial
%amount of both \gls{cpu} and \gls{gpu} computational time. Concretely,
%its execution has used $240$ thousand \gls{gpu} hours on
%\emph{LUMI-G}
%(the \gls{gpu} partition of the LUMI~\cite{lumi} cluster) and $2$ million \gls{cpu} core hours on
%the MeluXina~\cite{meluxina} cluster. This demonstrates that \hyperqueue{} is
%able to scale to large use-cases and enables robust execution using a significant amount of
%computational resources. To our knowledge, the execution of this workflow was among the largest
%\gls{md} campaigns that were ever computed on \gls{hpc}
%resources\footnote{This claim can also be found in LIGATE Deliverable D7.1, which has not been published yet at the time of writing of this thesis.}.
%
%This use-case has benefited from several interconnected \hyperqueue{} features. Due to
%its low overhead, it was possible to execute and schedule a very large number of tasks. These tasks
%were automatically meta-scheduled to many different allocations, respecting their
%(\gls{cpu} and \gls{gpu}) resource requirements. Thanks to the
%automatic allocator, all required Slurm allocations were submitted automatically in response to
%current computational load. And \hyperqueue{} also automatically re-executed any tasks
%that have failed to successfully finish. Both the source code~\cite{ps-workflow} and the
%datasets~\cite{ps_dataset_1,ps_dataset_2} with the computed results of the Pose Selector workflow are
%publicly available.
%
%\subsection{Hyperparameter search}
%\hyperqueue{} was used in the EXA4MIND~\cite{exa4mind} project to parallelize and
%distribute independent instances of a hyperparameter search workflow on the \gls{gpu}
%partition of the Karolina cluster~\cite{karolina}. Hyperparameter search is a technique
%where a machine-learning model is trained multiple times with separate values for various
%hyperparameters (such as learning rate or batch size). The goal is to find a configuration of
%hyperparameters that produces a model with the highest possible prediction performance.
%
%Parallelizing a hyperparameter search using \hyperqueue{} is very simple, even using the
%\gls{cli}. The individual configurations can be stored either into a
%\gls{json} array (where each configuration is a \gls{json} object) or
%into a text file (where each configuration is a separate line), from which \hq{}
%can then automatically build the workflow.~\Autoref{lst:hq-exa4mind-hyperparameter-search} shows an example of how easy is
%to use \hq{} in this case. The user simply starts the server, configures
%automatic allocation, so that it starts allocations on their behalf, and then submits a job that is
%autogenerated from a \gls{json} file containing the hyperparameter configurations. The
%executed script does not know anything about \hyperqueue{}, apart from reading its
%assigned configuration from the \texttt{HQ\_ENTRY} environment variable.
%
%\begin{listing}[h]
%	\begin{minted}[fontsize=\small, tabsize=4]{bash}
%# Start the HyperQueue server
%$ hq server start &
%
%# Configure automatic allocation
%$ hq alloc add slurm --time-limit 48h -- --partition=qgpu --gpus=1 --account=...
%
%# Submit the computation. Each training requires a single GPU and 16 CPU cores
%$ hq submit --cpus=16 --resource="gpus/nvidia=1" --from-json configs.json train.sh
%	\end{minted}
%	\caption{Hyperparameter search using \hyperqueue{}}
%	\label{lst:hq-exa4mind-hyperparameter-search}
%\end{listing}
%
%\subsection{Efficient load balancing of opportunistic computations}
%The CERN ATLAS~\cite{atlas} experiment generates vast amounts of data from its Large
%Hadron Collider, which are then further postprocessed using various simulations. Some of these
%simulations are executed on Czech supercomputing resources through the
%\gls{arcce}~\cite{atlas-it4i-1} submission system. \gls{arcce}
%periodically connects over the network to IT4Innovations \gls{hpc} clusters (such as
%Karolina) and submits allocations that perform the desired simulations. Its goal is to
%opportunistically leverage free cluster resources that are available when no other allocations with
%a higher priority can run at the same time.
%
%Originally, \gls{arcce} was submitting simulation computations directly as
%\gls{pbs} allocations\footnote{Note that the IT4Innovations clusters originally used \gls{pbs} as their allocation
%manager; they have switched to Slurm in 2023.}. While this approach worked, it also ran
%into some of the challenges presented in~\Autoref{challenge:allocation-manager}; notably the submission system was
%hitting the limits of the maximum allowed number of allocations that could be submitted by a single
%user. Furthermore, it encountered problems caused by communicating with the allocation manager too
%frequently~\cite{atlas-it4i-2}, which we have also noticed during the implementation of the
%\hyperqueue{} automatic allocator, described in~\Autoref{hq:automatic-allocation}. In order to
%resolve issues with allocation manager communication and to improve hardware utilization, the users
%of \gls{arcce} have switched to using \hyperqueue{}. This has provided two
%primary benefits.
%
%The first benefit is that computation submission becomes much easier for \gls{arcce}.
%It simply submits its computations into a \hyperqueue{} running on the login node of
%Karolina, without having to deal with Slurm allocation count or communication frequency limits. It
%also configures the automatic allocator to submit allocations into several Slurm queues, and the
%rest is handled fully automatically by \hyperqueue{}.
%
%The second advantage of using \hyperqueue{} is improved utilization of nodes. Previously,
%the computations were performed on the Salomon supercomputer~\cite{salomon}, which had
%$24$ \gls{cpu} cores per node. This was a reasonable amount of
%cores for computing a single simulation per each node. However, once Salomon has been
%decommissioned, \gls{arcce} started submitting computations to
%Karolina~\cite{karolina}, which has $128$ cores per node. This resulted in
%a decrease of \gls{cpu} utilization, because each simulation spends a non-trivial
%amount of time performing either serial computation or \gls{io}, during which only a
%few cores were utilized. And with so many cores available on the node, this resulted in lower
%hardware utilization, as many cores remained idle. A simple method of improving utilization is to
%split the computation into smaller parts that each use a smaller number of cores. However, on the
%Karolina \gls{cpu} partition it is not possible to ask for less than a whole node
%using Slurm directly. This is where \hyperqueue{} came in; once \gls{arcce}
%switched to it, the computations were configured for $32$ cores, and
%\hyperqueue{} then took care of packing four such computations to each allocated Karolina
%node. This change resulted in an improvement of the average \gls{cpu} utilization of
%the submitted computations on Karolina from $70\%$ to $90\%$, which
%saves tens of thousands of computational node hours each year~\cite{cern-hq}.
%
%This use-case shows that \hyperqueue{} can also be used programmatically by automated
%tools, rather than manually by cluster users. It provides a machine-readable
%\gls{json} output mode for its \gls{cli} commands that facilitates this
%kind of usage.
%
%\section{Evaluation}
%\label{hq:evaluation}
%This section evaluates the performance and scalability of \hyperqueue{} and its ability
%to maximize hardware utilization of workers on a series of experiments. All experiments were
%performed on the \gls{cpu} partition of the Karolina
%supercomputer~\cite{karolina} located at the IT4Innovations supercomputing
%center~\cite{it4i}. Each non-accelerated computational node of Karolina has two AMD
%EPYC\texttrademark{} 7H12 2.6 GHz 64 core \glspl{cpu}, for a total of 128 cores
%per node (hyper-threading is disabled), and \SI{256}{\gibi\byte} of DDR4
%\gls{ram}. It runs on the RockyOS 8 operating system with Linux kernel
%$4.18.0$ and \texttt{glibc} (GNU \texttt{C} standard library implementation) $2.28$. All experiments were
%performed with \hyperqueue{} version $0.18$, compiled with Rust
%$1.79.0$.
%
%In selected experiments, \hq{} was benchmarked in a so-called
%\emph{zero worker} mode, which is very similar to the zero worker mode that has been used to
%benchmark the overhead of \rsds{}, described in~\Autoref{sec:dask-overhead-per-task}. In this
%mode, workers do not actually execute any tasks; when a task is about to be executed on a worker,
%it instead finishes immediately. This mode can be used to benchmark the inner overhead of
%\hyperqueue{}, without it being affected by external factors, such as task execution.
%Experiments performed with this mode will be marked explicitly.
%
%Unless otherwise noted, each benchmark was executed and measured three times. In all experiments,
%the server and the workers of \hyperqueue{} (and \dask{}) were executed on separate nodes. Many
%experiments use the term \emph{makespan}, which describes the duration from submitting a
%task graph (a \hyperqueue{} \emph{job}) until all tasks of that task graph
%are finished computing. We will also use the term \emph{simple workflow}, which is a task graph
%without any dependencies, where each task has a resource requirement of a single
%\gls{cpu} core and runs for a fixed duration by executing the \texttt{sleep}
%UNIX program.
%
%\subsection{Total overhead}
%\label{sec:hq-exp-total-overhead}
%In this experiment, we evaluate the total \emph{overhead} of \hyperqueue{}. Under
%an ideal scenario, there exists a minimal makespan in which a given task graph can be executed on a
%fixed set of computational resources, assuming an infinitely fast communication network and no
%delays between executing individual tasks. We define overhead as any additional time on top of this
%ideal makespan, which is induced by the task runtime (in this case \hyperqueue{}). This
%overhead is caused by network communication, scheduling and various forms of bookkeeping that is
%performed by task runtimes.
%
%The overhead will of course depend on the task graph that is executed, and the computational
%resources (workers) used for its execution. Nevertheless, it is interesting to examine the minimum
%overhead introduced by the task runtime on a trivial task graph, in particular to gain insight into
%the smallest duration of tasks that can be used without the overhead becoming too limiting.
%
%We have benchmarked the \emph{simple workflow} in two sizes ($10$ and
%$50$ thousand tasks), with three different worker counts ($1$,
%$2$ and $4$ workers) and four different task durations
%(\SI{1}{\milli\second}, \SI{10}{\milli\second}, \SI{100}{\milli\second} and
%\SI{250}{\milli\second}). The \emph{ideal duration} for each task graph was estimated by taking
%the total time to execute all the tasks serially (task count times task duration), and divided by
%the total number of cores available (worker count times $128$). This estimates a
%perfect scenario without any overhead, where all tasks are perfectly parallelized across all
%available \gls{cpu} cores.
%
%\begin{figure}[h]
%	\centering
%	\includegraphics[width=\textwidth]{imgs/hq/charts/total-overhead-vs-ideal}
%	\caption{Total overhead of \hyperqueue{} (ratio vs theoretically ideal makespan)}
%	\label{fig:hq-overhead-vs-ideal}
%\end{figure}
%
%The results of this experiment can be observed in~\Autoref{fig:hq-overhead-vs-ideal}. Each chart column shows
%a different task duration; the rows separate different task counts. Within each chart, the
%horizontal axis represents the number of used workers, while the vertical axis shows the duration
%needed to execute the task graph in \hyperqueue{}, normalized to the theoretical ideal
%duration. The ideal duration is marked with a red horizontal dotted line. For example, the value
%$1.05$ specifies that the \hyperqueue{} execution was
%$5\%$ slower than the theoretically ideal duration.
%
%We can see that for tasks that last for \SI{100}{\milli\second} or more, the overhead of
%\hyperqueue{} is within $5\%$ of the ideal makespan, and with the larger
%task graph that contains $50$ thousand tasks, the overhead stays within
%$3\%$. The overhead increases slightly with an added number of workers, which is
%expected; the scheduler needs to perform more work and the server also communicates with more
%workers over the network.
%
%The situation becomes more interesting in the case where the task duration is only
%\SI{1}{\milli\second}. Here the overhead of \hyperqueue{} seemingly becomes very large.
%We have examined this situation in detail and found out that the issue is caused mostly by slow
%command execution performance on Karolina. Our calculation of the theoretically ideal makespan
%assumes that the executed command will last for exactly \SI{1}{\milli\second}, but from our
%experiments, even running a completely empty process on Karolina takes \emph{at least}
%\SI{500}{\micro\second}, and executing a program that is supposed to sleep for
%\SI{1}{\milli\second} can take several milliseconds. Because the overhead of actually executing
%the command is so large, it skews the total overhead of \hyperqueue{}, as it is not able
%to reach that theoretically ideal makespan, since the execution of commands takes more time than
%expected. Note that this would be an issue for any task runtime that executes a command in each
%task, and there is not that much that \hyperqueue{} could do to avoid this overhead.
%
%\begin{figure}[h]
%	\centering
%	\includegraphics[width=\textwidth]{imgs/hq/charts/total-overhead-vs-manual}
%	\caption{Total overhead of \hyperqueue{} (ratio vs manual process execution)}
%	\label{fig:hq-overhead-vs-manual}
%\end{figure}
%
%To provide a fairer evaluation of the overhead of \hyperqueue{} for short tasks, we have
%also evaluated it against a different baseline, which is not based on a theoretical calculation,
%but on the actual performance of command execution on Karolina. We have implemented a simple
%program that executes the same tasks (\texttt{sleep} processes) as
%\hyperqueue{}, in parallel on all $128$ available cores. This provides a
%more reasonable baseline that takes into account the overhead of command execution on Karolina. The
%results of this experiments can be seen in~\Autoref{fig:hq-overhead-vs-manual}. The baseline is marked with a
%horizontal red line; it represents the fastest measured time to execute the given number of
%processes. We have performed this experiment only with a single worker, because our baseline
%program did not implement multi-node distribution. Based on these results, we can see that for the
%\emph{simple workflow}, the actual overhead of \hyperqueue{} (when compared to manually
%running the commands without a task runtime) stays within
%$10\%$.
%
%\subsection{Task throughput}
%\label{sec:hq-exp-task-throughput}
%The previous experiment has evaluated the total overhead of \hyperqueue{} while taking
%into account the time to execute tasks. To further examine the inner overhead of scheduling and
%network communication, in this experiment we evaluate the possible task throughput (number of tasks
%processed per second) using the \emph{zero worker} mode. In this mode, \hq{}
%performs all operations that it does normally (managing tasks and workers, scheduling, sending
%network packets), but it does not actually execute the tasks; the corresponding worker immediately
%marks each task as completed instead. This allows us to examine the overhead of
%\hyperqueue{} without it being affected by process spawning, which can have severe
%overhead, as was demonstrated in the previous experiment. We have benchmarked the
%\emph{simple workflow} containing from $10$ thousand to $1$
%million tasks, with varying worker counts (up to $16$ workers and thus
%$2048$ \gls{cpu} cores).
%
%\begin{figure}[h]
%	\centering
%	\includegraphics[width=0.8\textwidth]{imgs/hq/charts/task-per-s}
%	\caption{Tasks processed per second with \emph{zero worker} mode}
%	\label{fig:hq-tasks-per-s}
%\end{figure}
%
%The results can be observed in~\Autoref{fig:hq-tasks-per-s}. The horizontal axis displays the number of
%tasks in the task graph, while the vertical axis displays the achieved throughput measured in tasks
%processed per second. In theory, the throughput should be increasing with more added workers. For
%the smallest task graph with $10$ thousand tasks, the throughput increases up to
%four workers ($512$ cores), then it decreases; the task graph is too small and the
%constant overhead factors start dominating over the available parallelism. For task graphs with up
%to $100$ thousand tasks, the throughput increases up to four workers, where it
%saturates. For even larger task graphs, the throughput once again starts decreasing with more than
%four workers; the overhead of network communication and scheduling starts to dominate.
%
%The absolute numbers of the achieved throughput demonstrate that \hyperqueue{} has very
%little baseline overhead, and can in theory execute hundreds of thousands of tasks per second. The
%tasks would have to be shorter than approximately \SI{5}{\micro\second} to fully saturate the
%throughput achievable by \hyperqueue{}. Such a task duration is uncommon for scientific
%workflows (indeed, on Karolina even starting a new process takes a hundred times longer); it is
%more common for task-parallel programming models that have been described
%in~\Autoref{sec:task-granularity}, which are operating on a very different level of granularity than what
%\hyperqueue{} was designed for.
%
%\subsection{Strong scaling}
%\label{sec:hq-exp-scalability}
%This experiment evaluates the ability of \hyperqueue{} to scale to many computational
%resources, by executing the same task graph with increasing worker counts (up to
%$64$ workers and thus $8192$ cores in total). Note that each
%benchmark configuration in this experiment was measured only once because of the large amount of
%computational resources required to execute tens of worker nodes.
%
%We have designed two separate scenarios for this experiment. In the first one, we have selected a
%fixed target makespan, so that the \emph{simple workflow} would be executed in approximately\
%$5$ minutes ($300$ seconds) on a single worker, and benchmarked
%three task graphs with increasing task counts, to examine how the scalability of
%\hyperqueue{} changes based on the number of tasks in the task graph when the total
%computational workload stays the same. The task duration of each task was scaled accordingly for
%each graph, so that the total makespan on a single worker would be $5$ minutes.
%The benchmarked worker counts were $1$, $2$,
%$4$, $8$, $16$, $32$ and
%$64$.
%
%\begin{figure}[h]
%	\centering
%	\includegraphics[width=\textwidth]{imgs/hq/charts/scalability-fixed-makespan}
%	\caption{Strong scalability of \hyperqueue{} with a fixed target makespan (\SI{300}{\second})}
%	\label{fig:hq-scalability-fixed-makespan}
%\end{figure}
%
%The results of the benchmark with a fixed makespan can be observed in~\Autoref{fig:hq-scalability-fixed-makespan}. The
%horizontal axis displays the number of workers used for executing the task graph. In the top row,
%the vertical axis shows the measured makespan; in the bottom row, the vertical axis shows speedup
%against the measured makespan on a single worker. We can see that for all three measured task
%durations, \hyperqueue{} was able to scale reasonably up to $64$
%workers, and it was able to compute the whole task graph in approximately $5$
%seconds. In the best case, with tasks that took \SI{2.5}{\second} to execute, it provided a
%$59.2$x speedup with $64$ workers. In the worst case, with tasks
%that lasted \SI{100}{\milli\second}, it provided a $54.4$x speedup with
%$64$ workers. With task duration \SI{0.1}{\second} and $64$ workers, \hyperqueue{} was able to
%dispatch almost $70$ thousand tasks per second.
%
%In the second scenario, we fixed the task duration of each task to $1$ second,
%and then varied the task count. This allowed us to examine how the scalability of
%\hyperqueue{} changes with an increasing workload.
%
%\begin{figure}[h]
%	\centering
%	\includegraphics[width=\textwidth]{imgs/hq/charts/scalability-fixed-task-duration}
%	\caption{Strong scalability of \hyperqueue{} with a fixed task duration (\SI{1}{\second})}
%	\label{fig:hq-scalability-fixed-task-duration}
%\end{figure}
%
%We can observe the results in~\Autoref{fig:hq-scalability-fixed-task-duration}. We can see that the scalability improves
%with the number of tasks; with $100$ thousand tasks, \hyperqueue{} is
%able to provide a $59$x speedup with $64$ workers. On the other
%hand, with only $10000$ tasks, the speedup in this case reaches only
%$38.8$x, because there are not enough tasks to saturate all workers. With
%$64$ workers that each have $128$ cores, the total number of
%cores managed by \hq{} is $8192$. Therefore, in this small task
%graph, each worker barely gets a single task to compute.
%
%In general, the results indicate that \hyperqueue{} introduces very little overhead, and
%is able to scale to a large amount of computational resources (tens of nodes and thousands of
%cores) and also to large amounts of tasks (hundreds of thousands) without an issue.
%
%\subsection{Performance comparison of \dask{} and \hq{}}
%\label{sec:hq-exp-dask}
%This experiment evaluates basic performance differences between the \dask{} task
%runtime and \hyperqueue{}. We have evaluated \emph{dask/distributed}
%$2024.7.0$ running under CPython $3.10.8$, with a disabled monitoring
%dashboard for improved performance. All measurements in this experiment were performed only once,
%to reduce the required computational resources.
%
%We have evaluated the scalability of both runtimes on the \emph{simple workflow} with a target
%makespan set to \SI{30}{\second} on a single worker, with an increasing number of workers
%and a varying number of tasks. In this scenario, \dask{} was benchmarked in three
%separate worker configurations, with $1$ process per node and
%$128$ threads per process ($1p/128t$), with $8$
%processes per node and $16$ threads per process ($8p/16t$) and
%with $128$ processes per node and $1$ thread per process
%($128p/1t$). The reason for choosing different process/thread combinations has been
%extensively explained in~\Autoref{dask:evaluation}; we chose two extremes ($1$
%process and $128$ processes) and then a compromise with $8$
%processes per node, according to recommendations in the \dask{}
%documentation~\cite{dask-thread-recommendation}.
%
%\begin{figure}[h]
%	\centering
%	\includegraphics[width=0.8\textwidth]{imgs/hq/charts/dask-vs-hq-sleep}
%	\caption{Scalability of \hyperqueue{} vs \dask{} with a fixed target makespan
%	(\SI{30}{\second})}
%	\label{fig:hq-dask-sleep}
%\end{figure}
%
%The results of this experiment are displayed in~\Autoref{fig:hq-dask-sleep}. The individual charts
%display separate task graphs with varying task counts (in each case, the task graph should have a
%makespan of $30$ seconds on a single node). The horizontal axis shows the amount
%of used nodes and the vertical axis displays the makespan. Note that we are using the term
%\emph{node count} instead of \emph{worker count} here, because each
%\dask{} process spawned per node is technically a separate worker. We can see that
%the in the case of \dask{}, the performance depends a lot on the number of tasks in
%the task graph, and also on the task duration. With only $1000$ tasks and task
%duration in the range of seconds, \dask{} is able to scale relatively well in all
%three configurations, although it is still slightly slower than \hyperqueue{}. However,
%with a larger number of tasks (tens of thousands), and tasks that last only hundreds or tens of
%milliseconds, the performance of \dask{} decreases very quickly.
%
%We can also see differences between the three \dask{} configurations, particularly
%in the benchmark with $50000$ tasks. With a single node, \dask{} is
%unable to keep up with the number of tasks, and it has a twice longer makespan than when multiple
%worker processes are used. On the other hand, once the number of nodes is increased, the situation
%reverses, and the configuration with a single worker per node becomes the fastest one, because
%\dask{} starts being overwhelmed by the number of connected workers. This suggests
%that some amount of manual tuning based on the number of used nodes and workers might be required
%to achieve optimal performance of \dask{} workflows. Conversely,
%\hyperqueue{} was able to keep a consistent performance profile in this experiment,
%without being affected by the number of tasks or the task duration. And since each
%\hq{} worker always manages all resources of a single node, it does not require
%any manual worker tuning.
%
%Note that this benchmark actually presents a sort of a best-case scenario for
%\dask{}. It executes its tasks as Python functions; therefore, it does not need to
%start a new Linux process per task, unlike \hyperqueue{}. Furthermore, since each task
%simply sleeps for a specified duration, it releases the \gls{gil} and therefore does
%not block other Python threads of the worker, which can thus perform other work concurrently.
%
%\begin{figure}[h]
%	\centering
%	\includegraphics[width=0.8\textwidth]{imgs/hq/charts/dask-vs-hq-empty}
%	\caption{Scalability of \hyperqueue{} vs \dask{} with an empty task}
%	\label{fig:hq-dask-empty}
%\end{figure}
%
%We have further examined the baseline overhead of \dask{} by executing an empty
%task (an empty Python function), which emulates the \emph{zero worker} mode of
%\hyperqueue{}, as it shows how the task runtime scales with the fastest possible task
%execution. We have compared this empty task execution with running the fastest possible task in
%\hyperqueue{} (by spawning a process that immediately exits) and also with the \emph{zero worker} mode of \hyperqueue{}.
%
%The results of this experiment can be seen in~\Autoref{fig:hq-dask-empty}. The horizontal axis shows
%the node count, and the vertical axis displays the makespan. Here we can observe a large difference
%between the inner overhead of \hyperqueue{} and \dask{}, which struggles
%to keep up with large amounts of very short tasks. This is of course an artificial scenario because
%the tasks are empty; however, it does demonstrate that \dask{}'s can have
%performance issues with large or very granular task graphs. It is also clear that the overhead of
%\dask{} increases quickly with multiple added workers, which is further exacerbated
%when multiple workers per node are used.
%
%\subsection{Worker \gls{cpu} utilization}
%\label{sec:hq-cpu-utilization}
%One of the primary metrics that task graph users care about is hardware utilization -- how well can
%a task graph (executed with a given task runtime) make use of available resources? This experiment
%examines the average \gls{cpu} utilization of worker nodes while executing a task
%graph with \hyperqueue{}, both for tasks with a uniform duration and also for tasks with
%a skewed distribution of durations. We have benchmarked the \emph{simple workflow} with
%$10$ thousand tasks, where each task fully utilizes a full
%\gls{cpu} core by executing the \texttt{stress} UNIX utility (instead of just
%sleeping). This task graph was executed on $4$ workers, whose
%\gls{cpu} utilization was periodically measured over the course of the computation.
%
%\begin{figure}[h]
%	\centering
%	\includegraphics[width=\textwidth]{imgs/hq/charts/scalability-stress-utilization}
%
%	In the bottom row, 75\% of tasks last \SI{5}{\second}, 25\% of tasks last
%	\SI{30}{\second} \caption{\gls{cpu} utilization of \hyperqueue{} worker nodes} \label{fig:hq-cpu-utilization}
%\end{figure}
%
%The results are shown in~\Autoref{fig:hq-cpu-utilization}. The horizontal axis shows the progress of the
%computation in seconds, the vertical axis displays the average \gls{cpu} utilization
%over all $128$ cores for each separate worker. The red horizontal line marks the
%total average utilization of each worker over the course of the whole computation. The top row
%displays a situation where all tasks in the task graph have a uniform duration of
%$5$ seconds. In this case, \hyperqueue{} workers keep their average
%utilization above $90\%$. Note that it takes some amount of time for each
%\texttt{stress} invocation to fully reach full core utilization, and since all tasks have
%the same duration, the tasks perform this ramp-up process in a mostly synchronized fashion. This
%causes the ``teeth'' in the utilization chart. Therefore, it is not realistic to reach full
%$100\%$ node utilization with this task graph.
%
%The second row displays a situation where the workload is skewed. Most ($75\%$) of
%the tasks still have a duration of \SI{5}{\second}; the remainder simulates a ``long tail''
%of slower tasks that take \SI{30}{\second} to execute. This is a more difficult situation
%for achieving high utilization; because some tasks are longer, they might get stuck executing at a
%time when other resources can no longer be utilized. We can observe this towards the end of the
%workflow execution, where approximately $30$ seconds before the end, the
%utilization starts to drop. However, even for this skewed task graph, the average
%\gls{cpu} utilization was kept at almost $90\%$.
%
%\subsection{Server \gls{cpu} consumption}
%\label{sec:hq-exp-server-cpu-consumption}
%The \hyperqueue{} server component is primarily designed to be executed on login nodes
%when deployed on supercomputers. This could pose a problem if it consumed too many system
%resources, because login nodes are shared by multiple users, and they are not designed for
%computationally intensive tasks. Some clusters even forcefully limit the total amount of
%\gls{cpu} time that can be consumed by processes running on login
%nodes~\cite{leonardo_time_limit}, and terminate the process if it exceeds the maximum allowed time.
%
%To evaluate how many resources the server consumes, we have performed an experiment where the
%server had to schedule a large number of tasks in a short time period (which acts as a sort of a
%stress test), and we measured its total \gls{cpu} time consumption across all cores.
%The server was running on a login node, and it was managing up to $12$ workers
%($1536$ cores) and up to $200$ thousand tasks. Each worker was
%deployed on a computational node, so that the server had to communicate with the workers over the
%network. The benchmark was executed with a fixed makespan; the duration of each task was scaled so
%that the whole task graph would finish computing in one minute. We measured the total amount of
%\gls{cpu} time consumed by the server (both in user-space and in the kernel), using
%standard Linux process monitoring tools.
%
%\begin{figure}[h]
%	\centering
%	\includegraphics[width=0.6\textwidth]{imgs/hq/charts/server-utilization-tasks}
%	\caption{\gls{cpu} time consumption of the \hyperqueue{} server with an increasing
%	number of tasks}
%	\label{fig:hq-server-cpu-consumption-tasks}
%\end{figure}
%
%\Autoref{fig:hq-server-cpu-consumption-tasks} shows how does the \gls{cpu} utilization of the
%server change with a fixed number of workers ($12$) and an increasing number of
%tasks. In this case, the server had to schedule the \emph{simple workflow} with number of tasks
%ranging from $10$ to $200$ thousand. The horizontal axis shows
%the number of tasks that were used for the given benchmark run, and the vertical axis shows the
%\gls{cpu} time consumed by the server over the one-minute period. We can see that the
%amount of \gls{cpu} resources scales approximately linearly with the number of tasks
%scheduled by the server. In the worst case, the server has consumed less than five seconds of
%\gls{cpu} time over one minute of real time, even though it had to schedule
%$200$ thousand tasks within this short period, which is an extreme stress test
%that far exceeds the rate of scheduled tasks in most scientific workflows. Over all the benchmarked
%task counts, the average \gls{cpu} consumption of the server per second and per
%$1000$ tasks was approximately \SI{0.0003}{\second}. In other words, for every
%thousand tasks in the task graph, the server consumed approximately \SI{0.3}{\milli\second}
%\gls{cpu} time every second.
%
%\begin{figure}[h]
%	\centering
%	\includegraphics[width=0.6\textwidth]{imgs/hq/charts/server-utilization-workers}
%	\caption{\gls{cpu} time consumption of the \hyperqueue{} server with an increasing
%	number of workers}
%	\label{fig:hq-server-cpu-consumption-workers}
%\end{figure}
%
%\Autoref{fig:hq-server-cpu-consumption-workers} displays a situation where the number of tasks
%is fixed ($50000$), but the number of workers increases from
%$2$ to $12$. The horizontal axis shows the number of workers
%that were used for the given benchmark run, and the vertical axis shows the \gls{cpu}
%time consumed by the server over the one-minute period. In this case, the consumption of the server
%does not increase by a large amount with more added workers.
%
%In terms of memory consumption, in the largest evaluated case with a task graph containing
%$200$k tasks, the memory consumption of the server (measured through
%\gls{rss}) was approximately \SI{120}{\mebi\byte}, which shows that the server
%also does not consume large amounts of memory.
%
%The amount of used resources will of course vary based on the executed workflow. However, this
%experiment shows that the server consumes very few resources in general, and that it still has a
%lot of leeway available even if some task graphs proved to be more computationally demanding than
%the benchmarked stress test.
%
%\subsection{Encryption overhead}
%\label{sec:hq-exp-encryption-overhead}
%As has already been noted in~\Autoref{hq:architecture}, \hyperqueue{} encrypts all network
%communication between the server, the workers and the client. This encryption is performed because
%the server is typically deployed on a login node, and thus the communication it performs can be in
%theory observed by other users of the cluster. In this experiment, we have evaluated the overhead
%of the encryption, by benchmarking \hyperqueue{} both with encryption enabled and
%disabled.
%
%We have benchmarked three instances of the \emph{simple workflow} containing
%$10$, $50$ and $100$ thousand tasks
%executed with $4$ \hyperqueue{} workers in two configurations; with
%and without the \emph{zero worker} mode. Because the \emph{zero worker} mode does not
%actually execute any tasks, it emphasizes the inner overhead of \hyperqueue{}, and should
%thus also amplify potential differences in communication overhead caused by the encryption.
%
%\begin{figure}[h]
%	\centering
%	\includegraphics[width=0.8\textwidth]{imgs/hq/charts/encryption-overhead}
%	\caption{Overhead of encryption in \hyperqueue{} communication}
%	\label{fig:hq-encryption-overhead}
%\end{figure}
%
%We can observe the results of the experiment in~\Autoref{fig:hq-encryption-overhead}. The horizontal axis shows
%the number of tasks in the benchmarked task graph, and the vertical axis shows the
%\emph{makespan}. In the \emph{zero worker} mode (displayed on the left), it is clear
%that encrypting the communication in fact produces a measurable overhead. In the largest measured
%case with $100$ thousand tasks, the makespan is approximately twice longer with
%encryption enabled. However, as soon as the tasks do any actual work (albeit it being the simplest
%work possible), the encryption overhead is no longer noticeable and it does not affect the total
%makespan of the task graph.
%
%In more realistic workflows, the executed tasks will be almost certainly longer than just executing
%the trivial~\texttt{sleep} command. In that case, the overhead of encryption will be even
%less noticeable; the larger the duration of the executed tasks, the less important is the overhead
%introduced by \hyperqueue{}. If some users would still not want to use encryption for
%some reason, \hyperqueue{} could be easily modified to add the option to disable
%encryption at runtime.
%
%\subsection{Output streaming}
%\label{sec:hq-exp-output-streaming}
%In this experiment we evaluate the effect of the \emph{output streaming} mode, which can be used to
%stream the outputs of \emph{stdout} and/or \emph{stderr} streams from
%individual tasks to the \hyperqueue{} server, to avoid creating a large number of files
%on the filesystem. We have benchmarked two task graphs without dependencies, with
%$10$ and $50$ thousand tasks, where each task generates a a
%fixed amount of data ($10$ and \SI{100}{\kibi\byte} per task). Each task
%simply outputs the corresponding amount of data to its standard output (\emph{stdout}).
%The standard error output is disabled and no output is printed to it. In the largest configuration,
%the task graph produces approximately \SI{5}{\gibi\byte} of data in total.
%
%With output streaming enabled, each task output is streamed to the server, which stores all data
%sequentially into a single log file. Without output streaming, each task simply creates a single
%file on disk and writes its \emph{stdout} output to it. The experiment was performed on
%two different filesystem partitions of the Karolina cluster. The first partition, called SCRATCH,
%uses the Lustre networked filesystem, which is designed for parallel high intensity
%\gls{io} workloads~\cite{karolina_scratch}. It is a representative of a
%high-performance filesystem, which can in theory reach up to \SI{700}{\gibi\byte}/s write
%throughput. The second partition, called PROJECT, uses \gls{gpfs}, which is a much
%slower filesystem implementation that focuses on providing high availability and
%redundancy~\cite{karolina_project}.
%
%\begin{figure}[h]
%	\centering
%	\includegraphics[width=0.7\textwidth]{imgs/hq/charts/io-streaming}
%	\caption{Effect of output streaming on the makespan of tasks}
%	\label{fig:hq-io-streaming}
%\end{figure}
%
%We can see the results of the experiment in~\Autoref{fig:hq-io-streaming}. The horizontal axis displays
%the amount of output produced by each task and the vertical axis shows the makespan of the executed
%task graph. The top row contains data for the SCRATCH filesystem; the bottom row contains data for
%the PROJECT filesystem. Three separate configurations are displayed in the chart. In the
%\emph{Stdout} mode, the task graph was executed without output streaming, while in the
%\emph{Streaming} mode, the task graph was executed with output streaming. The third mode
%will be described later below.
%
%The results differ significantly based on the amount of output produced by each task, and also
%based on the used filesystem. On the SCRATCH Filesystem, output streaming is able to reduce the
%makespan approximately by half, in the case where each task outputs \SI{10}{\kibi\byte} of
%data. However, when task output is larger (\SI{100}{\kibi\byte}), the situation reverses, and
%output streaming becomes slower. This is caused by the fact that the sequential writing into the
%log file by the server becomes a bottleneck, as it cannot be easily parallelized by the filesystem
%(unlike e.g.\ writing to many files in parallel). We have confirmed this by benchmarking a third
%mode, which we call \emph{Streaming (no write)}. In this configuration, output streaming was enabled;
%therefore, the output of tasks was streamed over the network from workers to the server, but then
%the server simply discarded the data and did not write it to the sequential log file. We can see
%that without actually sequentially writing the data to the log file, the makespan is still shorter
%than without output streaming. It is thus clear that the primary bottleneck in this case is in fact
%the final write to the log file.
%
%It should be noted that even though output streaming is slower in this case, it still provides the
%benefit of generating only a single file on the filesystem. It can thus help alleviate disk quota
%limits without any user effort.
%
%With the PROJECT filesystem, output streaming is $4-8$ times faster than writing
%to files directly. The performance benefit of output streaming thus depends heavily on the used
%filesystem, it seems to be particularly helpful on slower filesystems. One interesting result is
%that output streaming is generally faster on the PROJECT filesystem than on the SCRATCH filesystem
%(even though the PROJECT filesystem itself should be much slower). Our conjecture is that this
%might be caused by the network topology of the cluster. The SCRATCH filesystem uses the
%InfiniBand~\cite{infiniband} network connection, which is also used for network communication
%between the server and the workers; therefore, performing file \gls{io} on scratch
%while also sending many network packets might cause network contention.
%
%\begin{table}[h]
%	\centering
%	\begin{tabular}{|r|r|r|r|r|}
%		\hline
%		Output per task [B] & Task count & Size (stream) [MiB] & Size (stdout) [MiB] & Ratio \\ \hline
%		10000               & 10000      & 96                  & 118                 & 1.23x \\ \hline
%		10000               & 50000      & 479                 & 589                 & 1.23x \\ \hline
%		100000              & 10000      & 955                 & 978                 & 1.02x \\ \hline
%		100000              & 50000      & 4775                & 4916                & 1.03x \\ \hline
%	\end{tabular}
%	\caption{Size of task output on disk with and without I/O streaming}
%	\label{tab:hq-io-streaming-size}
%\end{table}
%
%Using output streaming has one additional advantage; it can reduce the total amount of consumed
%disk space, because it creates only a single file on disk. On most filesystems, every file occupies
%at least a single disk block (which typically consumes several KiB of space), therefore creating a
%large number of files also increases disk usage in general.~\Autoref{tab:hq-io-streaming-size} shows the total
%size of the output of all tasks of the benchmarked task graphs on disk. The \emph{Size (stream)}
%column shows the final size of the sequential log file (in this case, output streaming was used),
%the \emph{Size (stdout)} column displays the total size of a directory containing all files with
%outputs of individual tasks (in this case, output streaming was not used) and the
%\emph{Ratio} column shows how much larger was the output in the case where streaming was
%not used. In the case where each task has outputted \SI{10}{\kibi\byte} of data, writing the
%output to individual files on disk resulted in around $23\%$ higher disk usage. In
%the second configuration, where each task has outputted \SI{100}{\kibi\byte} of data, this
%difference was reduced to $3\%$,because the disk space overhead of the files
%became dwarfed by the size of their content.
%
%Output streaming can help users avoid disk quota limits, reduce consumed disk space, and in certain
%situations also help with \gls{io} performance. However, it is also clear that
%streaming so much data to a single location (the server) can be slower than writing to many files
%in parallel, if the given filesystem is optimized for this use-case. As a future improvement,
%streaming could be made more scalable by streaming task outputs to a single file (or a small amount
%of files) \emph{per worker}, to distribute the \gls{io} load, and then merge
%outputs from individual worker log files on demand.
%
%%\subsection{Automatic allocation}
%%\label{sec:hq-exp-autoalloc}
%%- time request resource waste
%%TODO: how to measure this reasonably?
%
%\subsection{LiGen virtual screening workflow}
%\label{sec:hq-exp-ligen}
%This experiment evaluates the achieved hardware utilization and scalability of the LiGen
%virtual screening workflow implemented using \hyperqueue{}. As explained previously
%in~\Autoref{sec:hq-usecase-ligen}, the workflow uses a SMILES file as its main input, which contains
%a description of a single ligand on each line. To allow \hyperqueue{} to balance the load of the
%computation across available resources and nodes, the workflow first splits the single input file into
%several smaller subfiles, each containing a subset of the input lines. Each such file is
%then processed in two tasks; the first expands the ligands into a three-dimensional structure
%stored in a MOL2 file and the second performs scoring on this expanded representation, which
%generates a \gls{csv} file. All the intermediate \gls{csv} files are then combined with a
%single task at the very end of the workflow.
%
%The number of ligands per file and the number of threads that are used for performing
%scoring on each file are configurable parameters. There are various trade-offs associated with
%setting these parameters. The expansion part is serial, therefore using as many files (tasks) as
%possible for this step is beneficial; up to the number of available cores. On the other hand,
%each scoring invocation of LiGen for a file has a non-trivial cost, it needs to perform some
%preprocessing and a part of the computation is performed serially. Therefore, for the scoring
%part, using fewer files is generally a better choice. Yet, with fewer files, there will also be
%less opportunities for load balancing performed by \hq{}, and there might not be enough tasks to
%saturate the available computational resources.
%
%We examine the effect of these two parameters on makespan and hardware utilization
%on an input containing $200$ thousand ligands. The computation duration varies significantly
%based on the length of each ligand; therefore, we have tested two input files. The first one ($uniform$)
%contains $200$ thousand copies of the same ligand, which simulates a balanced workload. The second
%one ($skewed$) contains a variation of ligands with different lengths.
%
%The results can be observed in~\Autoref{fig:hq-ligen-utilization}. The vertical axis displays
%the achieved hardware utilization on a single worker; the horizontal axis shows the progress of
%the computation. The rows denote the two individual input files, while the columns show results
%for different combinations of the two input parameters. The red horizontal line displays the
%total average utilization of the worker over the course of the whole computation; the black
%vertical line splits the expansion and scoring sections of the computation. The average node
%utilization of each section is displayed below the section name. The black ticks on the
%horizontal axis denote the makespan of each benchmarked scenario.
%
%%TODO: swap row/column?
%\begin{figure}[h]
%	\centering
%	\includegraphics[width=\textwidth]{imgs/hq/charts/ligen-aggregation-utilization}
%	\caption{Achieved hardware utilization of the LiGen virtual screening workflow}
%	\label{fig:hq-ligen-utilization}
%\end{figure}
%
%We can see that the parameter affecting the number of ligands per file has a significant effect
%on the resulting makespan; using too few or too many ligands results in a longer execution.
%As expected, the expansion section needs enough tasks to saturate the available cores, because
%it is single-threaded. For $5000$ ligands per file (the rightmost column), the total number of
%expansion tasks is only $40$, therefore the achieved utilization (for both input files) is around
%$32\%$, which corresponds to using only $40$ out of the $128$ available cores. With more than $128$ files
%available, the utilization approaches $100\%$ for the uniform input.
%For the skewed input file, the utilization for the expansion section is generally worse. With
%$100$ ligands per file, it is reduced significantly; unlike with the uniform input, some of the
%files were expanded much quicker than others, which led to more load imbalance.
%
%The scoring part is internally parallel and uses all $8$ assigned cores, therefore it does
%not have to be divided into so many tasks; we can see that even with only $40$ files, it
%achieves around $80\%$ utilization for both input workloads. In fact, with too many files, we
%can see the overhead associated with the serial part of scoring (which needs to be performed for
%each file separately) starts to dominate; even with the uniform workload, the \gls{cpu}
%utilization of the scoring section with $2000$ files reaches only around $63\%$ with the uniform
%input and just $44\%$ with the skewed input.
%
%The two configurations in the middle present a sort of a sweet spot; especially with $1000$
%ligands per file, the total average utilization for both workloads is at or above $88\%$.
%However, it should be noted that the best configuration will change based on the number of used
%workers. This can be observed in~\Autoref{fig:hq-ligen-scalability}, which shows how does the
%workflow scale up to $4$ workers ($512$ cores). The horizontal axis shows the number of workers
%and the vertical axis shows the duration of the executed task graph. We can see that with $4$ workers, the
%configuration with $400$ ligands per task starts to beat the configuration with $1000$ ligands
%per task, simply because the latter configuration is not able to provide enough parallelism
%(tasks) to make efficient use of all the available hardware resources. In general, \hyperqueue{}
%is able to reduce the makespan with additional workers being added for all four tested
%configurations.
%
%\begin{figure}[h]
%	\centering
%	\includegraphics[width=0.8\textwidth]{imgs/hq/charts/ligen-aggregation-scalability}
%	\caption{Scalability of the LiGen virtual screening workflow}
%	\label{fig:hq-ligen-scalability}
%\end{figure}
%
%We can see that even when the overhead of the task runtime does not get in the way,
%it might be necessary to tune the number of tasks for some workflows in order to achieve
%optimal performance.
%
%%TODO: GPU benchmark (with pre-expanded ligands?)

\section{Related work}
\label{hq:related-work}
\Autoref{ch:sota} has described several existing task runtimes, and how they are able to cope
with the mentioned challenges that affect task graph execution on \gls{hpc} clusters.
This section discusses previous work related to the described meta-scheduling approach and compares
selected task runtimes with \hyperqueue{}.

Existing approaches of reconciling workflows with \gls{hpc} allocations can be
divided into several categories. Some works examine what it would take to make existing allocation
managers more workflow friendly. In~\cite{slurm-workflow}, a modification to the Slurm allocation
manager is proposed, which makes it workflow-aware and adds support for fine-grained resource
management to it. This approach splits tasks of a workflow submitted within a single allocation
into a larger number of allocations, and assigns them corresponding priorities and resources, based
on their requirements. While this improves certain scheduling aspects of Slurm, it still creates a
separate allocation for each task, which does not remove the overhead associated with allocations.
This method also assumes prior knowledge of the workflow structure; therefore, it is not possible
to easily dynamically add tasks or computational resources while the workflow is being executed.

Another approach is to design a new kind of an allocation manager designed with tasks and workflows
in mind. An example of this is the Flux~\cite{flux} \gls{hpc} scheduler,
which serves as a re-imagination of a modern allocation manager. It treats the whole cluster
as a unified pool of resources and allows submitting computations that make use of these resources
with high granularity, down to the level of individual cores. Flux also provides dynamic
management of non-computational resources such as storage and power, and even takes them into
account during scheduling, for example by avoiding \gls{io} intensive computations when not
enough \gls{io} bandwidth is currently available. Furthermore, it enables defining generic
resources, unlike traditional allocation managers that can only manage a small set of known
resources, such as \gls{cpu} cores or memory. Providing this kind of flexibility and granularity
puts a lot of pressure on the system. Flux manages it by using a distributed and hierarchical
scheduling design, where each allocation can act as a separate Flux instance that can then recursively
subdivide its resources into additional nested allocations, and even dynamically ask for
reducing or increasing its set of resources.

While modifying allocation managers to remove some of their workflow handling issues or creating
new allocation managers are viable approaches, it should be noted that Slurm and
\gls{pbs} are currently dominating the most powerful
supercomputers~\cite{slurm-schedmd}, and that replacing (or even heavily modifying) the
allocation manager of an \gls{hpc} cluster is a very difficult process, which
requires a lot of implementation work, configuration tuning, documentation rewriting and also
retraining the cluster administrators, and more importantly its users. Furthermore, using the
allocation manager directly as a task runtime has a disadvantage; as it is then no longer easily
possible to run the workflows on personal computers or \gls{ci} servers,
which limits the ability to prototype and test them. \hyperqueue{} provides a more pragmatic approach,
which facilitates local prototyping and is able to work both alongside existing and any future allocation
managers.

Falkon~\cite{falkon} is a framework for scaling the execution of a large number of tasks on
distributed nodes. It focuses on efficient execution of many tasks per second and it also
contains an automatic allocation system for provisioning new computing nodes dynamically.
However, it is not designed primarily for \gls{hpc}, which introduces certain
trade-offs required to allow communication across different \gls{tcpip} networks,
such as using a pull based method for distributing work. Under this scheme, workers ask the
scheduler for work, rather than being assigned work by the scheduler directly; this induces
additional network communication and increases \gls{cpu} usage of the scheduler.
Falkon also does not consider task graph dependencies and complex non-\gls{cpu} resource
management.

E-HPC~\cite{ehpc} tackles the issue of monolithic allocations that waste resources by asking for
a union of resources required for the workflow. It does that by splitting
individual stages of workflows into separate allocations that contain the ideal amount of
resources for that stage. It also allows these stages to dynamically modify their resource
requirements through the use of checkpointing. When a stage asks for a different set of resources,
it is checkpointed, moved to a different allocation and then restarted.
While this approach does alleviate some problems with resource waste, it is relatively
inflexible; workflows have to be manually separated into individual stages with coarse-grained
resource requirements, and these stages are still submitted as individual allocations, rather
than being load balanced across different allocations.

% Multi-level scheduling - Ray, Flux?

Another approach for making it easier and more ergonomic to use \gls{hpc} clusters is taken by
tools and frameworks such as cumulus~\cite{cumulus}, Open OnDemand~\cite{openondemand}, HEAppE~\cite{heappe} or
Lexis~\cite{lexis}. These aim to simplify usage of clusters by providing high-level web
interfaces for managing computations. They focus on user authorization and authentication, data
management, provisioning of resources across different clusters and also partially on
(coarse-grained) workflow execution. This approach can make it easier for users that do not have
experience with the Linux operating system or interacting with the terminal to leverage \gls{hpc}
resources. It is mostly orthogonal to the feature set offered by meta-scheduling task runtimes,
such as \hyperqueue{}, although these two approaches can be combined. For example, the HEAppE
middleware provides support for integration with \hyperqueue{}~\cite{heappe_hq}.

%A more pragmatic approach is to leverage meta-scheduling on top of existing allocation managers,
%which can be performed fully in user-space and without requiring changes to the management
%software of a given cluster. This approach has been successfully leveraged by various tools.

\subsection{Comparison of meta-scheduling task runtimes}
There are hundreds of tools that are designed for executing some kind of a
task graph, whose features partially overlap with \hyperqueue{}, and it is infeasible to
mention all of them here. Instead, we will focus on a smaller subset of task runtimes that leverage
some form of meta-scheduling on top of allocation managers, and can therefore be more directly
compared with \hyperqueue{}. Several representatives of these tools will be discussed in this
section.

\Autoref{tab:hq-tools-comparison} provides a high-level comparison of these tools.
% It should be noted that task runtimes have different features

% !{\kern1em} - space
% @{} - remove space
%    \begin{tabular}{@{}lc GG !{\kern1em} GGG !{\kern1em} GG@{}}
%    \multicolumn{3}{c|}{\makecell{Task\\interfaces}} &
%    \multicolumn{p{4em}}{c|}{Task\newline{}interfaces} &
% https://stackoverflow.com/questions/1357798/how-to-center-cell-contents-of-a-latex-table-whose-columns-have-fixed-widths

% Inspiration taken from https://tex.stackexchange.com/a/377651/95679
\begin{table}[h]
	\footnotesize
	\newcommand*\rot[1]{\footnotesize\hbox to1em{\hss\rotatebox[origin=br]{-90}{#1}}}
	\newcommand*\rotheading[1]{\footnotesize\hbox to1em{\hss\rotatebox[origin=br]{-90}{#1}}}
	\newcommand*\feature[1]{\ifcase#1 -\or\LEFTcircle\or\CIRCLE\fi}
	\newcommand*\f[3]{\feature#1&\feature#2&\feature#3}
	\makeatletter
	\newcommand*\ex[9]{#1\tnote{#2}&#3&%
	\f#4&\f#5&\f#6&\f#7&\f#8&\f#9&\expandafter\f\@firstofone
	}
	% Features
	\newcommand*\yes{\faThumbsOUp}
	\newcommand*\no{\faThumbsODown}
	\newcommand*\cli{C}
	\newcommand*\pythonapi{P}
	\newcommand*\workflowfile{W}
	\makeatother
	\newcolumntype{A}{c@{}c}
	\newcolumntype{B}{c@{}c@{}c}
%    \newcolumntype{C}[1]{>{\centering\arraybackslash}p{#1}}
	\begin{threeparttable}
	%TODO: fix layout
	% 18 columns
	\begin{tabular}{@{}l|c|B|c|B|A|B|c|Ar}
	\toprule
	Task runtime &
	\rotheading{Architecture} &
	\multicolumn{3}{c}{\rotheading{Task interfaces}} &
	\rotheading{Task dependencies} &
	\multicolumn{3}{c}{\rotheading{Meta-scheduling}} &
	\multicolumn{2}{c}{\rotheading{Data transfers}} &
	\multicolumn{3}{c}{\rotheading{Resources}} &
	\rotheading{Iteration} &
	\multicolumn{3}{c}{\rotheading{Deployment}}\\
	\midrule
	& % tool
	& % architecture (master/worker vs distributed?)
	\centering \rot{CLI} & \rot{Python API} & \rot{Workflow file} & % interfaces
	Task dependencies & % dependencies
	\centering \rot{Static} & \rot{Dynamic} & \rot{Automatic allocation} & % meta-scheduling
	\rot{Data transfer between tasks} & \rot{Streaming of stdout/stderr} & % data transfers
	\rot{Basic (CPU/memory)} & \rot{Generic} & \rot{Multi-node tasks} & %resources
	& % dynamic task graphs
	\rot{No Python needed} & \rot{No runtime services needed} & \rot{Implemented in} \\ % deployment
	\midrule
	\textsc{GNU Parallel}~\cite{parallel} & C & \yes & \no & \no & \no & \no & \no & \no &
	\dask{}~\cite{dask} & C & \no & \no & \no & \no & \no & \no & \no\textsuperscript{\dag{}} & \no & \no & \no & \no & \no & \no & \no & \no & Python \\
	\hyperqueue{}~\cite{hyperqueue} & C & \no & \no & \no & \no & \no & \no & \no & \no & \no & \no & \no & \no & \no & \no & \no & Rust\\
	\snakemake{}~\cite{snakemake} & C & \no & \no & \no & \no & \no & \no & \no & \no & \no & \no & \no & \no & \no & \no & \no & Python\\
	\bottomrule
	\label{tab:hq-tools-comparison}
\end{tabular}
\begin{tablenotes}
	\item \hfil$\feature2=\text{provides property}$; $\feature1=\text{partially provides property}$;
	$\text{\feature0}=\text{does not provide property}$;
	\item \hfil\textsuperscript{\dag}with Dask-Jobqueue~\cite{dask-jobqueue} plugin;
	\textsuperscript{*}end-user tool available
\end{tablenotes}
\caption{Comparison of meta-scheduling task runtimes}
%\label{tab:hq-tools-comparison}
\end{threeparttable}
\end{table}

% TODO: merge tasks into categories
% Dask + Parsl + funcx/Globus

\subsection*{GNU parallel}
% architecture: centralized
% encryption: yes
% interfaces: CLI
% meta-scheduling: no
% autoalloc: no
% deps: no
% data transfer between tasks: no
% stdout/stderr streaming: yes
% resources: no
% multi-node: no
% dynamic task graphs: no
% Python: no
% external services: no
% language: Perl

- basic functionality of HQ

% https://docs.nersc.gov/jobs/workflow/gnuparallel/

\subsection*{Dask}
% architecture: centralized
% encryption: yes
% interfaces: Python
% meta-scheduling: automatic (with Dask-Jobqueue)
% autoalloc: yes (with Dask-Jobqueue)
% deps: explicit
% data transfer between tasks: yes
% stdout/stderr streaming: yes
% resources: basic, arbitrary
% multi-node: no
% dynamic task graphs: yes
% Python: yes
% external services: no
% language: Python

\dask{} was already extensively decribed in~\Autoref{sec:rsds-dask}. It is a distributed task
runtime written in Python, which allows parallelizing and distributing Python code in an easy
way. \dask{} offers a low-level \gls{api} for building task graphs explicitly from Python
function calls, however one of its most powerful features is the ability to parallelize
numpy/pandas code.

- only one autoalloc queue
- already described in RSDS chapter
- especially useful for numpy/pandas parallelization

\subsection*{SnakeMake}
% architecture: centralized
% encryption: no
% interfaces: Python, workflow files
% meta-scheduling: manual partitioning
% autoalloc: yes
% deps: filesystem
% data transfer between tasks: no
% stdout/stderr streaming: no
% resources: basic, arbitrary
% multi-node: kind-of?
% dynamic task graphs: no
% Python: yes
% external services: no
% language: Python

- lot of additional functionality
- useful for bash workflows
- cluster mode explicitly discouraged (https://docs.nersc.gov/jobs/workflow/snakemake/)

\subsection*{Ray}
% architecture: distributed
% encryption: yes
% interfaces: Python
% meta-scheduling: no
% autoalloc: no
% deps: explicit
% data transfer between tasks: yes
% stdout/stderr streaming: no
% resources: basic, arbitrary
% multi-node: no
% dynamic task graphs: yes
% Python: yes
% external services: yes
% language: C++/Python

- distributed scheduling
- useful for ML
- tasks are low level

\subsection*{Parsl}
% architecture: Centralized
% encryption: yes
% interfaces: Python
% meta-scheduling: yes
% autoalloc: yes
% deps: explicit
% data transfer between tasks: yes
% stdout/stderr streaming: no
% resources: basic
% multi-node: yes
% dynamic task graphs: yes
% Python: yes
% external services: no
% language: Python

- very Python centric
- executors have to be fine-tuned per workflow
- only basic resource support
- `WorkQueueExecutor` most similar to HyperQueue

\subsection*{Pegasus}
% architecture: centralized
% encryption: yes
% interfaces: workflow, Python
% meta-scheduling: manual
% autoalloc: yes?
% deps: yes
% data transfer between tasks: no
% stdout/stderr streaming: yes
% resources: basic
% multi-node: no (https://htcondor.readthedocs.io/en/latest/users-manual/quick-start-guide.html)
% dynamic task graphs: no
% Python: no, Java
% external services: yes (HTCondor)
% language: Python, Java

- hard-coded clustering
- level-based clustering
- bin-packing clustering
- data movement
- multiple backends (cloud, HPC, local, ...)
- HTCondor
- https://dl.acm.org/doi/pdf/10.1145/1341811.1341822

\subsection*{Balsam}
% https://arxiv.org/abs/1909.08704
% https://balsam.readthedocs.io/en/latest/

% architecture: centralized
% encryption: yes
% interfaces: Python
% meta-scheduling: automatic
% autoalloc: yes
% deps: explicit
% data transfer between tasks: yes
% stdout/stderr streaming: no
% resources: basic
% multi-node: yes
% dynamic task graphs: yes
% Python: yes
% external services: PostgreSQL
% language: Python

%TODO: benchmark

\subsection*{HyperShell}
% architecture: centralized
% encryption: yes
% interfaces: Bash, Python
% meta-scheduling: yes (manual bundle size)
% autoalloc: yes
% deps: no
% data transfer between tasks: no
% stdout/stderr streaming: yes
% resources: no
% multi-node: no
% dynamic task graphs: no
% Python: yes
% external services: no (SQLite)
% language: Python

\subsection*{AutoSubmit}
% architecture: centralized
% encryption: yes
% interfaces: workflow file
% meta-scheduling: manual
% autoalloc: yes
% deps: explicit
% data transfer between tasks: no
% stdout/stderr streaming: no
% resources: basic
% multi-node: yes
% dynamic task graphs: no
% Python: yes
% external services: no
% language: Python

- focuses on experiment tracking and provenance

\subsection*{FireWorks}
%TODO: FireWorks https://onlinelibrary.wiley.com/doi/10.1002/cpe.3505
%https://docs.nersc.gov/jobs/workflow/fireworks/

% architecture: centralized
% encryption: yes
% interfaces: workflow file, Python
% meta-scheduling: yes
% autoalloc: yes
% deps: explicit
% data transfer between tasks: no
% stdout/stderr streaming: no
% resources: no
% multi-node: no
% dynamic task graphs: yes
% Python: yes
% external services: MongoDB
% language: Python

- weird metascheduling

\subsection*{PyCOMPSs}
% architecture: Centralized
% encryption: yes
% interfaces: Bash, Python
% meta-scheduling:
% autoalloc: yes
% deps: explicit
% data transfer between tasks: yes
% stdout/stderr streaming: no
% resources: basic
% multi-node: yes
% dynamic task graphs: yes
% Python: yes
% external services:
% language: Java, Python

\subsection*{Merlin}
Merlin~\cite{merlin} is a ...  - similar idea to% https://merlin.readthedocs.io/en/latest/tutorial/1_introduction/?h=many#how-can-merlin-run-so-many-simulations
HyperQueue
- wraps Maestro and Celery
- workflows defined in files
- does not have autoallocation
- encrypts with per-user key, same as HQ
- more high-level
- resources: only CPUs and nodes, can be simulated with different Celery queues
- dependencies: Python, Redis

% architecture: centralized
% encryption: yes
% interfaces: workflow files
% meta-scheduling: yes
% autoalloc: no
% deps: explicit
% data transfer between tasks: no
% stdout/stderr streaming: no
% resources: basic
% multi-node: yes
% dynamic task graphs: no
% Python: yes
% external services: Redis
% language: Python

\subsection*{gwf}
% https://github.com/gwforg/gwf
%- dependencies through files
%- basic resources
%- fault-tolerance partially manual
%- task per Slurm allocation

% Ignored
% - dagger (https://github.com/trustyou/dagger) - obsolete
% - DeepDIVA/Gale (https://github.com/v7labs/Gale) - too niche, ML focused
% - signac (https://github.com/glotzerlab/signac) - focuses on managing datasets
% - noWorkflow (https://github.com/gems-uff/noworkflow) - focuses on extracting provenance from
% Python scripts
% - Reshi (https://arxiv.org/pdf/2208.07905) - recommends task to node scheduling (for Slurm?)
% - Design Principles of Dynamic Resource Management for High-Performance Parallel Programming
% Models (https://arxiv.org/pdf/2403.17107) - general principles for allocations, focuses on MPI
% - QueueDO (https://bitbucket.org/berkeleylab/qdo/src/master/) - very simple version of HQ
% - Swift (Swift: Fast, Reliable, Loosely Coupled Parallel Computation) - wtf is this?
% - Mercury (https://www.usenix.org/system/files/conference/atc15/atc15-paper-karanasos.pdf) -
% decentralized scheduling
% - Falkon (https://dl.acm.org/doi/pdf/10.1145/1362622.1362680) - something like HQ
% - Askalon (ASKALON: A Development and Grid Computing Environment for Scientific Workflows) -
% old, IDE for workflows?
% - VGrADS: Enabling e-Science Workflows on Grids and Clouds with Fault Tolerance - abstraction
% over cloud and HPC
% - Hawk: Hybrid Datacenter Scheduling - data centers, hybrid centralized/distributed scheduling
% - Tarema (https://arxiv.org/pdf/2111.05167) - Kubernetes, profiles HW to cluster nodes with
% similar performance
% - Tigres (https://ieeexplore.ieee.org/document/7515681), RADICAL (https://arxiv.org/pdf/1609
% .03484) - templates for building workflows
% - pydra (https://github.com/nipype/pydra) - too niche, has some iteration support, not different enough from Dask
% - SciLuigi - not documented enough

\section*{Summary}
This chapter has presented a meta-scheduling design for executing task graphs on
\gls{hpc} clusters that was built from the ground up to overcome the issues mentioned
in~\Autoref{ch:sota}. It has also introduced \hyperqueue{}, a distributed task
runtime that implements this design through an \gls{hpc} focused programming model,
which enables ergonomic execution of workflows on supercomputers. It is able to meta-schedule tasks
to provide load balancing among separate allocations, while respecting complex resource
requirements that can be arbitrarily configured separately for each task. Its automatic allocator
can submit allocations fully autonomously on behalf of the user, which further simplifies the
execution of \hyperqueue{} task graphs. It also offers both a straightforward
\gls{cli} and a Python \gls{api} for more complex use-cases, and it is
trivial to deploy on supercomputers.

The overhead and scalability of \hyperqueue{} was evaluated in various situations. The
results indicate that it consumes little resources, introduces a reasonable amount of overhead and
can scale to tens of nodes and thousands of cores. The experiments have also shown that it is more
than competitive against \dask{}, a very popular task runtime that is being
commonly used for running task graphs on distributed clusters.

Several ideas for the future improvement of \hyperqueue{} will be summarized in the final
chapter.
