There is a large body of tools designed for executing arbitrary task graphs on diverse computing
platforms, ranging from consumer-grade laptops, through cloud deployments, to distributed and
\gls{hpc} clusters. They are known under various terms, such as task executors,
job managers, distributed job schedulers, dataflow engines or orchestrators. We will use the term
\emph{task runtime} for all such task execution tools in this thesis, as has already been
discussed in the previous chapter. These runtimes are quite popular, and they are being used for
computing all kinds of scientific workflows on \gls{hpc}
clusters~\cite{hpc_tasks, hpc_tasks_2, hpc_tasks_3, pegasus}.

Examples of such task runtimes include e.g.\ \dask{}~\cite{dask},
Parsl~\cite{parsl}, Ray~\cite{ray},
PyCOMPSs~\cite{pycompss}, HyperLoom~\cite{hyperloom},
Pydra~\cite{pydra}, Snakemake~\cite{snakemake},
SciLuigi~\cite{sciluigi}, Merlin~\cite{merlin},
Autosubmit~\cite{autosubmit} or
FireWorks~\cite{fireworks}. Each task runtime defines its own instance of a task-based
programming model, and has a different set of trade-offs in areas such as performance and
scalability, fault tolerance, ease-of-use, ease-of-deployment and others.

The goal of this thesis is to design approaches for seamless and performant execution of task
graphs on supercomputers, and for that it is first necessary to understand what are the challenges
that users are facing in this area, and how do existing task runtimes deal with them.
\Autoref{ch:rsds} will describe and analyze \dask{}, a state-of-the-art
task runtime, in detail, and~\Autoref{hq:related-work} will then provide a more general comparison
of selected task runtimes with the proposed \hyperqueue{} tool.

This chapter provides a general overview of the most important aspects of task graph execution on
supercomputers, and in particular of the challenges associated with it. These challenges can limit
the achievable hardware utilization and also worsen the developer experience of task graph authors.
Most of the challenges stem from the sheer scale of \gls{hpc} workloads, and
primarily from the complexity and idiosyncrasies of supercomputers, which have been historically
designed for different kinds of programming models. These challenges are also further exacerbated
when using off-the-shelf tools that were not designed for \gls{hpc} use-cases.

Some of the described challenges are more related to scalability, performance and hardware
utilization, while some of them affect the ergonomics and developer experience of designing and
executing task graphs. In addition to enumerating both existing and potential problems, this
chapter also mentions how some existing tools deal with them, and what could be some desired
properties of an ideal task runtime that would be able to alleviate the described challenges.

%I have personally encountered most of these challenges during several years of working with
%various \gls{hpc} use-cases

\section{Allocation manager}
\label{challenge:allocation-manager}
Users of \gls{hpc} clusters are not typically allowed to directly perform
arbitrary computations on the computational nodes (machines designed to perform expensive
computations) of the cluster. Instead, they connect to machines that are usually called
\emph{login nodes}, from which they have to enqueue their desired computation into a queue
handled by a submission system that manages the hardware resources of the cluster. We will use the
term \emph{allocation manager} for these submission systems and the term
\emph{allocation}\footnote{The term \emph{job} is also commonly used for the concept of
\gls{hpc} computational requests.} for a computational request submitted by a
user into these managers.

Allocation managers are required for providing fair access to the resources of the cluster, because
\gls{hpc} clusters are typically used by many people at the same time. Without a
centralized management, hardware resources could be inadvertently shared among multiple users at
once, which could have undesirable performance and security implications, and could lead to
oversubscription. Furthermore, usage of these clusters is usually metered. Users can typically only
use a certain amount of resources assigned to their \emph{computational project}, and when their
resources run out, they have to ask (or pay) for more resources. Allocation managers thus also
implement user and project accounting, so that there is a clear historical record of how many
resources were consumed by individual users of the cluster.

The majority of \gls{hpc} clusters~\cite{slurm-schedmd} use one of the two
most popular allocation manager implementations, either Slurm~\cite{slurm} or
\gls{pbs}~\cite{pbs}\footnote{Or some of its many derivatives,
such as TORQUE~\cite{torque} or OpenPBS~\cite{openpbs}}. For simplicity,
Slurm will be used as a default representative of allocation managers in the rest of this thesis
(unless otherwise noted), since it shared all the important described constraints with
\gls{pbs}.

The following process describes how computations are typically executed on
\gls{hpc} clusters that use an allocation manager:

\begin{enumerate}
	\item The user enqueues a computational request (allocation) into the manager from a login node. The
	      request typically has to specify at least how many nodes should be allocated and the
	      maximum duration of the computation (usually labeled as \emph{wall-time}), after which the
	      computation will be forcibly stopped by the manager. It can also contain additional configuration,
	      such as what kinds of nodes should be allocated or what the priority of the request is.
	\item The allocation manager puts the request into a queue and schedules it to be executed at some time
	      in the future. Since users submit their allocations into the manager continuously, each allocation
	      has different properties and priorities, and it is not possible to precisely predict for how long
	      an allocation will run, the schedule can be very dynamic and unpredictable, so users might have to
	      wait seconds, minutes, hours or even days before their allocation starts to execute.
	\item Once the allocation gets to the front of the queue and there are enough resources available, the
	      manager provisions the requested amount of hardware resources (typically a number of whole
	      computational nodes) and either executes a script (a \emph{batch} allocation) or
	      provides the user with an interactive terminal session on one of the allocated nodes (an
	      \emph{interactive} allocation). Allocations are often configured in a way that provides
	      their authors with exclusive access to the allocated hardware resources, in which case no other
	      user will be able to use these resources (nodes) until the allocation finishes.
	\item Once the executed script finishes (or the wall-time duration is reached), the allocation ends, and
	      its hardware resources are released so that they can be used by another allocation in the future.
\end{enumerate}

Although it might not be obvious from the above description at first, the presence of an allocation
manager actually presents perhaps the largest obstacle for ergonomic execution of task graphs on
\gls{hpc} systems. Instead of executing their task graphs directly, users have to
think about how to split the task graph into allocations and then submit them. While this is true
of any computation executed on \gls{hpc} clusters in general, in the case of task
graphs it is especially difficult because of their complex structure. The fact that mapping complex
task workflows to \gls{hpc} allocations is non-trivial has been noted in existing
research~\cite{glume,slurm-workflow}, and it even led to proposals for making existing allocation
managers workflow-aware~\cite{slurm-workflow} or even creating new allocation managers that
have better support workflows~\cite{flux}.

To execute a task graph using an allocation manager, it is necessary to find a way to map tasks
to allocations, ideally in a way that efficiently utilizes \gls{hpc} resources and
is as transparent as possible for the users. There are several ways of performing this
task-to-allocation assignment, which will be described below, along with a set of challenges and
limitations that are mostly caused by various performance bottlenecks of existing allocation
managers.

\subsection*{Execute the whole task graph in a single allocation}
At first, it might seem that executing a task graph within a single allocation is a simple
solution, since all the user has to do is submit an allocation that will eventually execute the
complete task graph (using some task runtime), which is a similar approach that would be used for
executing the task graph e.g.\ on a personal computer.

It is indeed simple -- when it is possible at all. The problem is that allocation managers tend
to place fairly strict limits on the maximum possible execution duration of an allocation (the
wall-time) and also on the number of resources (nodes) that can be requested at once, to ensure a
fairer assignment of resources to users of a cluster. Therefore, when a task graph has too many
tasks, takes too long to execute, or the user wants to leverage more computational resources than
can fit in a single allocation, this approach will not work.

In a way, if the task graph computation is short enough that it can fit within a single allocation,
it might not always even make sense to use an \gls{hpc} cluster to compute it. A
more realistic scenario is that even if an individual task graph can be executed quickly, users
might want to execute many such task graphs (for example to execute many experiments with different
parametrizations). This situation can be seen as a special case of a large task graph that consists
of many disjoint components (smaller task subgraphs). In this case, it will again typically not be
possible to execute all such task graphs inside a single allocation.

Even when a task graph can be reasonably executed within a single allocation, this approach might
lead to underutilization and resource waste. Consider a typical data analysis or a machine
learning workflow that works in several stages. First, it loads a dataset from disk and
pre-processes it, then it trains a machine learning model, and finally it performs some
post-processing step, which e.g.\ analyzes the resulting model. The pre-processing and
post-processing steps are usually not very resource intensive and run on \glspl{cpu}
only, while the training step typically consumes a lot of resources and is usually executed on a
\gls{gpu} accelerator. To execute such a workflow, we will need to ask for a set
of resources that contain such an accelerator. The issue is that all resources of an allocation are
reserved for its whole duration; therefore, we will have to pay the price for the expensive
accelerated nodes (and prevent others from using them) even during the execution of workflow steps
in which they will be underutilized or completely idle. This is caused by the fact that all
allocated resources are tied to the lifetime of the whole allocation, and using this approach we
have to ask for a set of resources that form a union of the resource requirements of all tasks of
the workflow.

One additional disadvantage of this approach is queuing latency. Large allocations that require a
long wall-time duration and/or request many computational nodes typically spend a much longer time
in the allocation manager queue, because it is more difficult for the manager to make sure that all
the requested resources are available at the same time. As a concrete example, ten allocations that
require a one hour wall-time can potentially be completed much quicker than a single allocation
that requests a ten hour wall-time, because these allocations do not need to run at the same time,
and are therefore easier for the manager to schedule. Similar behavior can be observed w.r.t.\
the number of requested nodes.

In terms of task runtime support, this approach can be used with essentially all existing task
runtimes, since it does not require any special support from the runtime. The user simply submits a
single allocation, and when it starts, the whole task graph is computed with a task runtime all in
one go. This approach is thus relatively simple for the user, although as has been mentioned, it
might not be feasible for many use-cases, and it can lead to resource waste and a long latency
before receiving the first results of finished tasks.

\subsection*{Execute each task as a separate allocation}
The previous approach mapped all tasks to a single allocation. An opposite extreme would be to map
each task to a separate allocation. This approach might seem intuitive, because from a certain
point of view, \gls{hpc} allocation managers can also be viewed as task runtimes,
if we consider allocations to be tasks; therefore, it might be tempting to treat them as such. Both
\gls{pbs} and Slurm even support a crude notion of dependencies between
allocations, which could allow users to express computational graphs. Indeed, in an ideal world,
there would be no difference between an allocation manager and a task runtime, and users could
simply construct an arbitrarily granular task graph and execute it simply by submitting it directly
into the allocation manager.

However, in practice, this approach is mostly infeasible, at least with the currently popular
allocation managers (\gls{pbs} and Slurm), because they introduce a non-trivial
amount of overhead per each allocation. In an ideal scenario, Slurm is able to launch a few hundred
allocations per second~\cite{slurm-throughput}. However, more realistically, users on a crowded
cluster might experience at least a few hundred milliseconds overhead per allocation, if not more,
which is order(s) of magnitude more than the overhead of a typical task
runtime~\cite{rsds}.

Even though there is probably room for reducing the overhead of contemporary
\gls{hpc} allocation managers, it is important to note that some of the
performance limitations are inherent. Allocation managers have to (among other things) provide
accurate accounting, handle robust and secure provisioning and cleanup of hardware resources
provided to allocations, manage user and process isolation on computational nodes and ensure user
fairness. These responsibilities are out of scope for most task runtimes, and therefore it is not
surprising that they can usually achieve much higher performance. While in theory, it could be
possible to design an allocation manager that can also act as a (performant) task runtime at the
same time (and some efforts have been made on this front~\cite{flux}), it is
probably infeasible to modify the two most prominent allocation managers that are used almost
ubiquitously in the current \gls{hpc} ecosystem to support this use-case.

Due to this overhead, allocation managers usually limit the number of allocations that can be
enqueued by a single user at any given time (e.g.\ to a few hundred or thousand), to ensure that
the manager is not overloaded and that it can continue to serve requests from other users of the
cluster. Therefore, unless the task graph is very small, it will most likely be infeasible to
create a separate allocation for each task.

It should be noted that both Slurm and \gls{pbs} allow partially overcoming this
limitation through \emph{allocation arrays} (also labeled as job arrays). This concept allows users to submit a large amount
of computations with the same shape and properties in a single allocation. However, it still has
several disadvantages. One is a lack of flexibility; it does not easily allow submitting
heterogeneous tasks with different resource requirements, and it also has only a very crude support
for dependencies between the submitted tasks. Another disadvantage is fault tolerance; if some of
the tasks of the array fail, users might have to manually identify them and resubmit them in
another allocation, which is far from effortless. And of course, clusters do impose limits for
allocation arrays as well, and therefore even the array approach might not be enough to encompass
all tasks of massive task graphs.

Apart from the maximum allocation count, there are other limitations that can be imposed by
allocation managers. Some tasks of scientific workflows can be granular, and require only a handful
of resources, e.g.\ a single \gls{cpu} core. To avoid wasting resources, an
allocation mapped to such a task should thus ask only for the exact set of resources needed by it.
However, allocation managers are sometimes configured in a way that only offers node-level
granularity of hardware resources, and thus does not allow users to request less than a whole
computational node~\cite{it4i_node_scheduling_policy}. In these cases, it would be wasteful if we had to
request a whole computational node e.g.\ for a simple task that runs for just a few seconds and
requires only a handful of cores.

There are two primary reasons allocation managers limit the granularity of allocations. First,
reducing the granularity of allocations lessens the overall overhead. If users are able to ask
e.g.\ for individual cores of each node, the manager would have to manage many more allocations
than when users ask for complete nodes, which could quickly become unmanageable. The second reason
is security. If the manager provides the ability to reserve only a fraction of a node, allocations
from multiple users can execute on the same node at the same time. This reduces isolation between
users, which might potentially have security implications. It can also affect performance, since
concurrently running allocations might compete for some shared resource on the same node (e.g.\ the
operating system process scheduler). Since each node runs a single instance of an operating system,
it forms a natural isolation domain, and it is thus also frequently used as a unit of granularity
for allocations.

Another reason using an allocation manager directly as a task runtime might be impractical is
that it makes debugging and prototyping of task graphs more difficult. It is useful to have the
ability to examine the execution of a workflow locally, e.g.\ on the user's personal computer,
before running it on a cluster. However, if the task graph were implemented directly in an
allocation manager, users would have to deploy tools like \gls{pbs} or Slurm
locally, which can be challenging.

To sum up, using an allocation manager directly as a task runtime is impractical, primarily because
of the overhead associated with each allocation and the resulting difference in task and allocation
granularity, which can lead to resource waste. Therefore, users that want to execute a task graph on an
\gls{hpc} system usually use a separate task runtime rather than defining
task graphs using the allocation manager directly.

\subsection*{Partition the task graph into multiple allocations}
As we have seen, both of the mentioned extreme approaches had severe disadvantages. The remaining
approach is to devise a general task-to-allocation assignment, by partitioning the task graph into
smaller subgraphs and submitting each subgraph as a separate allocation. This approach allows
mapping a large number of tasks into a smaller number of allocations, and thus amortize the allocation
overhead. Most users will probably sooner or later converge to a similar approach, once their task
graph becomes sufficiently large and complex, and they will have to reconcile the coarse-grained
nature of allocations with the fine-grained nature of tasks.

This process is far from straightforward if it is performed manually, i.e.\ if the user has to
manually split the task graph when preparing the allocations, and then start an independent
instance of a task runtime inside each allocation. In this case, users need to figure out how to
optimally partition the task graph, which is itself a notoriously difficult
NP-hard~\cite{graph_partitioning} problem in general, and then submit the subgraphs as separate
allocations.

Furthermore, this approach might require implementation efforts outside the boundaries of the used
task-based programming model, which can be cumbersome. For example, the intermediate outputs of
computed tasks of a partitioned subgraph might have to be persisted to a filesystem before the
corresponding allocation ends, and the results from multiple allocations then have to be merged
together to produce all the necessary data outputs of the complete task graph. Another issue is
that if some tasks fail and their corresponding allocation ends, the task runtime does not get
a chance to recompute them. Users will thus need to identify such failed tasks and then selectively
resubmit them into new allocations, which can get complicated quickly.

In general, while this approach can overcome the limitations of executing the workflow inside a
single allocation and of using the allocation manager itself as a task runtime, it also requires
non-trivial effort from users, because they essentially have to reimplement parts of task runtime
functionality on top of the allocation manager.

% TODO: obrázky!

There are task runtimes that can help with automating a part of this effort, by operating on a
level above the allocation manager, instead of always running only within the context of a single
allocation. Thanks to this property, they are able to represent task graphs that span multiple
allocations. We will use the term \emph{meta-schedulers} for these task runtimes.

An example of such a tool is \snakemake~\cite{snakemake}, which allows users to define
complex task graphs that be executed in multiple allocations. It is also able to partition the task
graph into \emph{groups}, where all tasks of a single group are always executed inside
the same allocation. However, this partitioning has to be specified explicitly by the author of the
workflow, which can be laborious.

While automating the partitioning process is useful, a \emph{static} division of
subgraphs into allocations (be it performed manually or in an automated fashion) that has to be
decided \emph{before} the allocations are submitted might still lead to suboptimal
hardware utilization. When the individual subgraphs are executed by independent instances of a task
runtime that run in separate allocations and do not communicate with each other, it will not be
possible to load balance tasks across different allocations, which can limit the achieved hardware
utilization.

\subsection*{Fully dynamic scheduling across allocations}
Ideally, users would not have to think about task graph partitioning (or even the allocation
manager) at all; they should be able to construct a task graph and execute it directly on an
\gls{hpc} cluster in a straightforward way, by letting the task runtime perform
the task graph partitioning and load balancing across multiple allocations automatically for them.
This could be achieved by integrating allocation managers and task runtimes more closely. We have
seen that adding support for executing fine-grained task graphs to allocation managers is probably
not realistic, since they have too many other responsibilities. However, a promising approach is to
leverage meta-scheduling, i.e.\ schedule tasks on top of the allocation manager, to enable flexible
load balancing across different allocations without forcing users to devise the task to allocation
mapping themselves.

\Autoref{ch:hyperqueue} will further explore this approach by introducing a design for a fully
dynamic mapping of tasks to allocations that can overcome the challenges mentioned in this section.

\section{Cluster heterogeneity}
Even though task graphs are designed to be portable and ideally should not depend on any specific
execution environment, for certain types of tasks, we need to be able to describe at least some
generic environment constraints. For example, when a task executes a program that leverages the
CUDA programming framework~\cite{cuda}, which is designed to be executed on an
NVIDIA graphics accelerator, it has to be executed on a node that has such a
\gls{gpu} available, otherwise it will not work properly.

It should thus be possible for a task to define \emph{resource requirements}, which specify
resources that have to be provided by an environment that will execute such a task. For example, a
requirement could be the number of cores (some tasks can use only a single core, some can be
multi-threaded), the amount of available main memory, a minimum duration required to execute the
task or (either optional or required) presence of an accelerator like a \gls{gpu}
or an \gls{fpga}. In order to remain portable and independent of a specific
execution environment, these requirements should be abstract and describe general, rather than
specific, types of resources.

The challenge related to resource requirements of \gls{hpc} tasks specifically is
the diverse kinds of hardware present in modern \gls{hpc} clusters, which have
started to become increasingly heterogeneous in recent years. This trend can be clearly seen in the
TOP500 list of the most powerful supercomputers~\cite{top500analysis}. Individual cluster
nodes contain varying amounts and types of cores and sockets, main memory,
\gls{numa} nodes or accelerators like \glspl{gpu} or
\glspl{fpga}. Since \gls{hpc} software is often designed to
leverage these modern \gls{hpc} hardware features, this complexity is also
propagated to tasks and their resource requirements.

Tasks might declare a combination of several requirements, for example two
\glspl{gpu}, $16$ cores and \SI{32}{\gibi\byte} of main memory. They can also be designed in
a way that allows them to leverage an open-ended range of resources, e.g.\ a task might require at
least four cores, but if more are available, it could use as many as possible. And some tasks might
even support several variants of requirements. For example, a task might either use four cores and a
single \gls{gpu} (if there is one available), or it could use more cores (and no \gls{gpu})
to offset the absence of an accelerator.

A resource requirement that is fairly specific to \gls{hpc} systems is the usage
of multiple nodes per single task. This requirement is necessary for programs that are designed to
be executed in a distributed fashion, such as programs using \gls{mpi}, which are
quite common in \gls{hpc}. The use-case of tasks using multiple nodes is
discussed in more detail later in this chapter.

Existing task runtimes usually support some notion of resource requirements, although they can be
somewhat limited. Some runtimes support only a fixed set of known resources (typically the number
of cores and the amount of memory), which is not enough to describe complex resources of
heterogeneous clusters. Other runtimes, such as \dask{} or
\snakemake{}, support arbitrary resources, although they do not allow expressing
more complex patterns, such as the mentioned case where a task could define multiple resource
requirement configurations. Supporting multiple nodes for a single task is not supported by most
runtimes at all, because their programming model often assumes that each task executes on a
single node only.

To support the mentioned scenarios, task runtimes should ideally allow users to specify arbitrarily
fine-grained and abstract resource requirements for each task, with support for multiple
requirement variants per task and multi-node tasks. They should also take these requirements into
account when scheduling, both to make sure that they are upheld, but also to use this provided
information to utilize the available hardware effectively.

\section{Data transfers}
After a task is computed, it can produce various outputs, such as standard error or output streams,
files created on a filesystem or data objects that are then passed as inputs to dependent tasks.
There are many approaches to storing and transferring these outputs.

Some task frameworks store task outputs on the filesystem, since it is relatively simple to
implement and it provides support for basic data resiliency out-of-the-box. A lot of existing
software (that might be executed by a task) also makes liberal use of the filesystem, which can
make it challenging to avoid filesystem access altogether. However, \gls{hpc}
nodes often do not contain any local disks, but instead use shared filesystems that are accessed
over a network. While this can be seen as an advantage, since with a shared filesystem it is much
easier to share task outputs among different workers, it can also be a severe bottleneck. Shared
networked filesystems can suffer from high latency and accessing them can consume precious network
bandwidth that is also used e.g.\ for managing computation (sending commands to workers) or for
direct worker-to-worker data exchange. Furthermore, data produced in \gls{hpc}
computations can be large, and thus storing it to a disk can be a bottleneck even without
considering networked filesystems.

These bottlenecks can be alleviated by transferring task outputs directly between workers over the
network (preferably without accessing the filesystem in the fast path), which is implemented e.g.\
by \dask{}. Some runtimes (for example HyperLoom~\cite{hyperloom})
also leverage \gls{ram} disks, which provide support for tasks that need to
interact with a filesystem, while avoiding the performance bottlenecks associated with disk
accesses. It is also possible to use \gls{hpc} specific technologies, such as
InfiniBand~\cite{infiniband} or \gls{mpi}, to improve data transfer
performance by fully exploiting the incredibly fast interconnects available in
\gls{hpc} clusters.

Data outputs produced by tasks tend to be considered immutable in existing task runtimes, since a
single output can be used as an input to multiple tasks, and these might be executed on completely
different computational nodes; therefore, it would be challenging to synchronize concurrent
mutations of these objects. A problem that can arise with this approach is that if the data outputs are large,
but the computation of tasks that work with the data is short, then the serialization overhead (or
even memory copy overhead, if the dependent task is executed on the same node) can dominate the
execution time. Such use-cases can be solved with stateful data management, for example in the form
of \emph{actors}, which can be seen as stateful tasks that operate on a single copy
of some large piece of data, without the need to transfer or copy it.

\section{Fault tolerance}
Fault tolerance is relevant in all distributed computing environments, but
\gls{hpc} systems have specific requirements in this regard. As was already
mentioned, computational resources on \gls{hpc} clusters are provided through
allocation managers. Computing nodes allocated by these managers are provided only for a limited
duration, which means that for long-running computations, some nodes will disconnect and new nodes
might appear dynamically during the lifecycle of the executed workflow. Furthermore, since the
allocations go through a queue, it can take some time before new computational resources are
available. Therefore, the computation can remain in a paused state, where no tasks are being
executed, for potentially long periods of time.

It is important for task runtimes to be prepared for these situations; they should handle node
disconnections gracefully, even if a task is being executed on a node that is disconnected, and
they should be able to restart previously interrupted tasks on newly arrived workers. In general,
in \gls{hpc} scenarios, worker instability and frequent disconnects should be
considered the normal mode of operation, rather than just a rare edge case.

\section{Multi-node tasks}
Many existing \gls{hpc} applications are designed to be executed on multiple
(potentially hundreds or even thousands) nodes in parallel, using e.g.\ \gls{mpi}
libraries or other communication frameworks. Multi-node execution could be seen as a special
resource requirement, which states that a task should be executed on multiple workers at once.
Support for multi-node tasks is challenging, because it affects many design areas of a task
runtime:
\begin{description}[wide=0pt]
	\item[Scheduling] When a task requires multiple nodes for execution and not enough nodes are available at a given
		moment, the scheduler has to decide on a strategy that will allow the multi-node task to execute.
		If it were constantly trying to backfill available workers with single-node tasks, the
		multi-node tasks could be starved.

		The scheduler might thus have to resort to keeping some nodes idle for a while to enable the
		multi-node task to start as soon as possible. Another approach could be to interrupt the currently
		executing tasks and checkpoint their state to make space for a multi-node task, and then resume
		their execution once the multi-node task finishes.

		In a way, this decision-making already has to be performed on the level of individual cores even
		for single-node tasks, but adding multiple nodes per task makes the problem much more difficult.
	\item[Data transfers] It is relatively straightforward to express data transfers between single-node tasks in a task
		graph, where a task produces a set of complete data objects, which can then be used as inputs for
		dependent tasks. With multi-node tasks, the data distribution patterns become more complex, because
		when a task that is executed on multiple nodes produces a data object, the object itself might be
		distributed across multiple workers, which makes it more challenging to use in follow-up tasks.
		The task graph semantics might have to be extended by expressing various data distribution
		strategies, for example a reduction of data objects from multiple nodes to a single node, to
		support this use-case.

		When several multi-node tasks depend on one another, the task runtime should be able to exchange
		data between them in an efficient manner. This might require some cooperation with the used
		communication framework (e.g.\ \gls{mpi}) to avoid needless repeated serialization
		and deserialization of data between nodes.
	\item[Fault tolerance] When a node executing a single-node task crashes or disconnects from the runtime, its task can be
		rescheduled to a different worker. In the case of multi-node tasks, failure handling is generally
		more complex. For example, when a task is executing on four nodes and one of them fails, the
		runtime has to make sure that the other nodes will be notified of this situation, so that they can
		react accordingly (either by finishing the task with a smaller number of nodes or by failing
		immediately).
\end{description}

Most existing task runtimes only consider single-node tasks and do not provide built-in support for
multi-node tasks, which forces users to use various workarounds, for example by emulating a
multi-node task with a single-node task that uses multi-node resources that are not managed by the
task runtime itself. To enable common \gls{hpc} use-cases, task runtimes should
be able to provide first-class support for multi-node tasks and allow them to be combined with
single-node tasks in a seamless manner. Advanced multi-node task support could be provided e.g.\ by
offering some kind of built-in integration with \gls{mpi} or similar common
\gls{hpc} technologies.

\section{Scalability}
The massive scale of \gls{hpc} hardware (node count, core count, network
interconnect bandwidth) opens up opportunities for executing large-scale task graphs, but that in
turn presents unique challenges for task runtimes. Below you can find several examples of
bottlenecks that might not matter in a small computational scale, but that can become problematic
in the context of \gls{hpc}-scale task graphs.

\begin{description}[wide=0pt]
	\item[Task graph materialization] Large computations might require building massive task graphs that contain millions of tasks. The
		task graphs are typically defined and built outside of computational nodes, e.g.\ on login nodes of
		computing clusters or on client devices (e.g.\ laptops), whose performance can be limited. It can
		take a lot of time to build, serialize and transfer such graphs over the network to the task
		runtime that runs on powerful computational nodes. This can create a bottleneck even before any
		task is executed. This has been identified as an issue in some existing task
		runtimes~\cite{dask-client-perf}.

		In such case, it can be beneficial to provide an \gls{api} for defining task graphs
		in a symbolic way by representing a potentially large group of similar tasks with a compressed
		representation to reduce the amount of consumed memory. For example, if we want to express a
		thousand tasks that all share the same configuration, and differ e.g.\ only in an input file that
		they work with, we could represent this as a group of thousand tasks, rather
		than storing a thousand individual task instances in memory. This could be seen as an
		example of the classical Flyweight design pattern~\cite{gof}.

		Such symbolic graphs could then be sent to the runtime in a compressed form and re-materialized
		only at the last possible moment. In an extreme form, the runtime could operate on such graphs in a
		fully symbolic way, without ever materializing them.
	\item[Communication overhead] Scaling the number of tasks and workers will necessarily put a lot of pressure on the communication
		network, both in terms of bandwidth (sending large task outputs between nodes) and latency (sending
		small management messages between the scheduler and the workers). Using \gls{hpc}
		technologies, such as \gls{mpi} or a lower-level interface like
		\gls{rdma}, could provide a non-trivial performance boost in this regard. Some
		existing runtimes, such as \dask{}, can make use of such
		technologies~\cite{dask-ucx}.

		As we have demonstrated in~\cite{pspin, spin2}, in-network computing can be also used to
		optimize various networking applications by offloading some computations to an accelerated
		\gls{nic}. This approach could also be leveraged in task runtimes, for example to
		reduce the latency of management messages between the scheduler and workers or to increase the
		bandwidth of large data exchanges among workers, by moving these operations directly onto the
		network card. This line of research is not pursued in this thesis, although it could serve as an
		interesting idea to be explored.
	\item[Scheduling] Task scheduling is one of the most important responsibilities of a task runtime, and with large
		task graphs, it can become a serious performance bottleneck. Existing task runtime, such as
		\dask{}, can have problems with keeping up with the scale of
		\gls{hpc} task graphs. The scheduling performance of various task scheduling
		algorithms will be examined in detail in~\Autoref{ch:estee}. \Autoref{ch:rsds}
		then will describe the performance overhead of the \dask{} runtime and its
		scheduler.
	\item[Runtime overhead] A typical architecture of a task runtime consists of a centralized component that handles task
		assignments, and a set of connected workers. As we have shown in~\cite{rsds}, task
		runtimes with this architecture should be mindful of their task execution and communication
		overhead. For example, even with an overhead of just \SI{1}{\milli\second} per task, executing
		a task graph with a million tasks would result in total accumulated overhead of more than fifteen
		minutes. Our results indicate that increasing the performance of the central scheduling and
		management component of a task runtime can have a large positive effect on the overall time it
		takes to execute the whole task graph. This is explored in detail in~\Autoref{ch:rsds}.

		However, the performance of the central server cannot be endlessly increased, and at some
		point, a centralized architecture will become a bottleneck. Even if the workers exchange
		data outputs directly between themselves, the central component might become overloaded
		simply by coordinating and scheduling the workers using management messages.

		In that case, a decentralized architecture could be leveraged to avoid the reliance on a central
		component. Such a decentralized architecture can be found e.g.\ in Ray~\cite{ray}.
		However, to realize the gains of a decentralized architecture, task submission itself has to be
		decentralized in some way, which might not be a natural fit for common task graph workflows. If all
		tasks are generated from a single component, the bottleneck will most likely remain even in an
		otherwise fully decentralized system.
\end{description}

\section{Iterative computation}
There are various \gls{hpc} use-cases that are inherently iterative, which means
that they perform some computation repeatedly, until some condition (which is often determined
dynamically) is satisfied. For example, the training of a machine learning model is typically
performed in iterations (called epochs) that continue executing while the prediction error of the
model keeps decreasing. Another example could be a molecular dynamics simulation that is repeated
until a desired accuracy (or other property) has been reached.

One approach to model such iterative computations would be to run the whole iterative process inside
a single task. While this is simple to implement, it might not be very practical, since such
iterative processes can take a long time to execute, and performing them in a single task would
mean that we could not leverage desirable properties offered by the task graph abstraction, for
example fault tolerance. Since the computation \emph{within} a single task is
typically opaque to the task runtime, if the task fails, it would then need to be
restarted from scratch.

A better approach might be to model each iteration as a separate task. In such case, the task
runtime might be able to restart the computation from the last iteration, if a failure occurs.
However, this approach can be problematic if the number of iterations is not known in advance,
since some task runtimes expect that the structure of the task graph will be immutable once the
graph has been submitted for execution.

To support iterative computation, task runtimes should allow their users to stop the execution of a
task graph (or its subgraph) once a specific condition is met, and also to add new tasks to the
task graph in a dynamic fashion, if it is discovered during its execution that more iterations are
needed. \dask{} and \ray{} are examples of task runtimes
that allow adding tasks to an existing task graph dynamically.

\section{Deployment}
\label{challenge:deployment}
Even though it might sound trivial at first, an important aspect that can affect the ergonomics of
executing task graphs (or any kind of computation, in general) on an \gls{hpc}
cluster is ease-of-deployment. Supercomputing clusters are notable for providing only a severely
locked-down environment for their users, which does not grant elevated privileges and requires
users to either compile and assemble the runtime dependencies of their tools from scratch or choose
from a limited set of precompiled dependencies available on the cluster, which can have
incompatible versions or a non-optimal configuration.

Deploying any kind of software (including task runtimes) that has non-trivial runtime dependencies,
or non-trivial installation steps (if it is not available in a pre-compiled form for the target
cluster) can serve as a barrier for using it on an \gls{hpc} cluster. Many
existing task runtimes are not trivial to deploy. For example, several task runtimes, such as
\dask{}, \snakemake{} or \pycompss{}, are
implemented in Python, a language that typically needs an interpreter to be executed, and also a
set of packages in specific versions, that need to be available on the filesystem of the target
cluster. Installing the correct version of a Python interpreter and Python package dependencies can
already be slightly challenging in some cases, but there can also be other issues, such as the
dependencies of the task runtime conflicting with the dependencies of the software executed by the
task graph itself. For example, since \dask{} is a Python package, it depends
on a certain version of a Python interpreter and a certain set of Python packages. It supports
tasks that can directly execute Python code in the process of a \dask{} worker.
However, if the executed code requires a different version of a Python interpreter or Python
packages than \dask{} itself, this can lead to runtime errors or subtle
behavior changes\footnote{It is not straightforward to use multiple versions of the same Python package dependency in a
single Python virtual environment. Conflicting versions thus simply override each other.}.

Since it is probable that users of \gls{hpc} clusters will already have enough
issues with installing the software executed by their workflows, the task runtime itself should
ideally provide frictionless deployment, and it should not interact with external dependencies
required by the executed tasks.

\section*{Summary}
Even though more \gls{hpc} peculiarities can always be found, it should be already
clear from all the mentioned challenges that \gls{hpc} use-cases that leverage
task graphs can be very complex and may require specialized approaches in order to reach maximum
efficiency. The challenges that were described in this chapter belong to two broad categories that
also form the main focus of this thesis; how to make task graph execution on
\gls{hpc} effortless for the users, and how to make it use hardware resources
efficiently.

The rest of the thesis will analyze challenges belonging to both categories, and discuss approaches
for alleviating them. Chapters~\ref{ch:estee} and~\ref{ch:rsds} focus on
the performance aspects, namely on task scheduling and the overhead introduced by task
runtimes.~\Autoref{ch:hyperqueue} then also deals with the ergonomic challenges, and proposes
a design for an \gls{hpc}-tailored task runtime that ties all the described
approaches together to enable truly first-class task graph execution on \gls{hpc}
clusters.
