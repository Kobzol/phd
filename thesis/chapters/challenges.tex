\section{Inside/outside of HPC jobs}

\section{Deployment}
- Python is hard to deploy

Task-based programming is quite popular, as it provides a simple way to define complex workflows
which can then be executed in a wide range of environments, ranging from consumer laptops, through
cloud deployments, to distributed \gls{hpc} clusters. However, as any programming
model, it also has some disadvantages and problems. It is crucial to understand these limitations
in order to design approaches for overcoming them.

This chapter thus describes various challenges and requirements of this programing model, both in
terms of efficient and scalable execution, and in terms of the offered productivity and ergonomics.
It focuses specifically on challenges and requirements required by \gls{hpc}
use-cases, which introduce a unique set of constraints stemming both from the inherent complexity
of \gls{hpc} software and hardware and also from the sheer computational scale
required to efficiently utilize \gls{hpc} resources.

In addition to the challenges, we will also mention various desired properties and features which
should be offered by task runtimes in order to either support the mentioned requirements or to
alleviate the mentioned problems.

\section{Allocation manager}
\label{sec:allocation-manager}
Users of \gls{hpc} clusters are not typically allowed to directly perform
arbitrary computations on the computational nodes (machines designed to perform expensive
\gls{hpc} computations) of the cluster. Instead, they can connect to machines that
are usually called \emph{login nodes}, from where they have to enqueue their desired
computation into a queue handled by a submission system that manages the hardware resources of the
cluster and user accounting. We will use the term \emph{allocation manager} for these submission
systems, and the term \emph{allocation}\footnote{The term \emph{job} is also commonly used for the concept of
\gls{hpc} computational requests. However, this term will be used for a different concept described later in the thesis, therefore we use \emph{allocation} instead.} for a computational
request submitted by a user into the allocation manager. The majority of \gls{hpc}
clusters~\cite{slurm-schedmd} use one of the two most popular allocation managers, either
\gls{pbs}~\cite{pbs} or Slurm~\cite{slurm}).

Each allocation states (amongst other things) the desired computational resources that the user
wants to utilize, primarily how many nodes they want to allocate, and what is the expected maximum
duration of their computation. The allocation is then submitted into a \emph{queue}
and starts executing only once there are enough free computational resources. Allocation managers
provide fair access to the cluster resources, to avoid their oversubscription and also to handle
accounting of the used resources. They tend to have fairly strict limits on the number of
allocations that users can submit and the number of nodes that they can have reserved for their
allocations at any given time. Because of these limits, allocations tend to be quite
coarse-grained. They typically ask for a whole node at minimum, and usually run at least for
minutes, but more typically hours or even days.

Since users have to create allocations to compute anything on the cluster, and thus they cannot
simply execute their task graphs directly, a question naturally arises -- how to map tasks (or task
graphs) to allocations in a way that will efficiently utilize \gls{hpc} resources?
Several ways of performing this task-to-allocation mapping are described below, however all of them
come with significant disadvantages.

\subsection*{Execute the whole task graph in a single allocation}
The simplest situation is when a task graph can be executed with a single allocation. If it does
not have a large number of tasks, or if it can be executed relatively quickly, users can create an
allocation that will compute the whole task graph. This approach is quite simple for the user,
since they just execute the task graph using a task runtime in the same way as they would on a
cluster without an allocation manager, or on a personal computer. The only difference is that they
have to define and submit an allocation that will bootstrap the computation.

However, since allocations are bound both by node count and a time limit, this approach is only
usable for rather small task graphs. Indeed, if the computation is short, it might not even make
sense to use an \gls{hpc} cluster to compute it. A more realistic scenario is that
even if an individual task graph can be executed quickly, users might want to execute many such
task graphs (for example to execute many experiments with different parametrizations). This
situation can be seen as a special case of a large task graph that consists of many disjoint
components (smaller task subgraphs). In this case, it will typically not be possible to execute all
such task graphs inside a single allocation.

\subsection*{Execute each task as an individual allocation}
From a certain point of view, \gls{hpc} allocation managers can also be viewed as
task runtimes that operate on a very coarse level -- their tasks being allocations that potentially
span hundreds of nodes, run for days or even longer and consist of many different program
executions. Ideally, there would be no difference between an allocation manager and a task runtime,
and users would just be able to construct an arbitrarily granular task graph and execute it
directly on an \gls{hpc} cluster in a straightforward way.

% https://rse.princeton.edu/2020/01/monitoring-slurm-efficiency-with-reportseff/
While this approach can certainly look tempting, in practice it is not usually feasible to use the
currently popular allocation managers (\gls{pbs} and Slurm) in this way, because
they operate on a level that is far too coarse for complex task graphs. While they do support
expressing dependencies between individual allocations in a crude way, they tend to have large
overhead per each allocation\todo[inline]{cite}, which can be several orders of magnitude
larger than for typical task runtimes (e.g.\ seconds vs milliseconds). Furthermore, they seldom
allow the user to create more than a few hundreds of allocations at the same time, both to provide
fairness and also because they simply cannot scale to such a number of allocations.

It should be noted that even though there is definitely room for improving the performance of
\gls{hpc} allocation managers, some of their complexity and performance
limitations are inherent. They have to provide accurate accounting, handle robust and secure
cleanup of hardware resource sprovided to allocations, manage user and process isolation on the
computational nodes, ensure user fairness and many other things. Many of these responsibilities are
out of scope for task runtimes, which enables them to achieve higher performance.

Another problem is node granularity. For ``small'' tasks that only use e.g.\ a few cores, users
would like to schedule and execute multiple tasks on a single node at the same time, to leverage
the available hardware resources efficiently. While allocation managers are able to create
allocations that require only a fraction of a node, this functionality is not always available.
Either for security reasons, because tasks from multiple users can then run on the same node at the
same time, which reduces user isolation, or for performance reasons, because the overhead of
scheduling a large number of allocations (in theory many allocations per each node) can become
unmanageable for the allocation manager~\cite{it4i_node_scheduling_policy}. When the manager is configured
in a way that a single allocation has to span at least a (complete) single node, it can lead to
wasted resources if a single task is unable to leverage the whole computational node.

Another reason why users might not want to use the allocation manager directly as a task runtime is
that it is useful to debug and prototype task graphs in a small-scale scenario (e.g.\ locally, on a
personal computer), before executing it on a large-scale \gls{hpc} system.
However, it can be quite challenging for users to deploy systems like \gls{pbs} or
Slurm locally. Therefore, they would need to use a different task runtime locally than on the
target \gls{hpc} platform, which woult not be very practical.

The mentioned issues stem from a dichotomy between the coarse-grained focused allocation manager
and more fine-grained focused task runtimes, which can create a barrier for
\gls{hpc} users. Users that want to execute a task graph on an
\gls{hpc} system thus usually use a separate task runtime (e.g.\
Dask~\cite{dask}) rather than using the allocation manager directly.

\subsection*{Partition the task graph into a smaller number of allocations}
The third approach that users may take is to partition their task graphs into smaller subgraphs,
and then submit each subgraph as a single allocation, where the subgraph will be computed by an
independent instance of a task runtime.

This is sort of an ultimate approach that the users will probably sooner or later converge to, once
their task graph becomes sufficiently complex and large, and they will have to somehow reconcile
the coarse-grained nature of allocations with the fine-grained nature of tasks and overcome the
overhead of creating too many allocations.

This process is not straightforward, especially if users have to perform the partitioning manually.
Graph partitioning itself is a notoriously difficult problem that is
NP-hard~\cite{graph_partitioning}\todo[inline]{Ada: Is this OK?}, and it is thus difficult to decide
beforehand how exactly should the task graph be split into allocations. Furthermore, if the
partitioning of tasks into allocations is performed statically, before the computation begins, then
it might lead to suboptimal hardware utilization, as it will not be possible to load-balance tasks
across different allocations, even if multiple allocations do run concurrently.

In addition to partitioning the task graph, this approach typically requires further implementation
efforts that are outside the boundaries of the task-based programming model. As an example, the
intermediate outputs of computed tasks of a partitioned subgraph might have to be persisted (to a
storage system) before the corresponding allocation ends, and the results from multiple allocations
then have to be merged together. In order to support recomputation of failed tasks, and submission
of new tasks while a task graph is already executing, it should also be possible to periodically
submit new allocations that could dynamically provide needed computational resources, until the
whole task graph is computed. This reduces the ergonomics of using task graphs, because it
basically requires users to reimplement parts of the task runtime behavior on top of the allocation
manager, in order to overcome its limitations.

\vspace{5mm}
The gap between allocation managers and task runtimes creates a disconnect for users attempting to
scale their task graph computation. Executing a task graph on a personal computer tends to be quite
simple. After that, moving to an \gls{hpc} cluster and executing the entire task
graph inside a single allocation is also quite straightforward. But once the task graph has to be
partitioned into multiple allocations, the simple abstraction of implicitly parallel task graphs
that can be executed with a single command quickly falls apart, as the user has to perform a lot of
additional work to make this scenario execute efficiently.

Ideally, users would not have to think about the allocation manager at all; they should be able to
construct a task graph and execute it directly on an \gls{hpc} cluster in a
straightforward way, by letting some tool perform the partitioning and load balancing across
allocations automatically for them. This could be achieved either by adding support for executing
fine-grained task graphs to allocation managers or by adding support for communicating with
allocation managers to task runtimes, to enable transparent execution of task graphs on
\gls{hpc} systems. This functionality is provided by several
\emph{meta-schedulers}, which will be mentioned in the next chapter.

\section{Cluster heterogeneity}
Even though task graphs are designed to be portable and ideally should not depend on any specific
execution environment, for certain types of tasks, we need to be able to describe at least some
generic environment constraints. For example, when a task executes a program that leverages the
CUDA\todo{explain} framework, which is designed to be executed on a graphics
accelerator, it has to be executed on a node that has a \gls{gpu} available,
otherwise it will simply not work.

It should thus be possible for an \gls{hpc} task to define
\emph{resource requirements}, which specify resources that have to be provided by an environment that
will execute such task. These requirements can be quite diverse. For example, a requirement could
be the number of cores (some tasks can use only a single core, some can be multithreaded), the
amount of available main memory, a minimum duration required to execute the task or (either
optional or required) presence of an accelerator like a \gls{gpu} or an
\gls{fpga}\@. In order to remain portable and independent of a specific execution
environment, these requirements should be abstract and describe general, rather than specific,
types of resources.

The challenge related to resource requirements of \gls{hpc} tasks specifically is
the diverse hardware present in modern \gls{hpc} clusters, which have started to
become increasingly heterogeneous in recent years. This trend can be clearly seen in the TOP500
list of most powerful supercomputers~\cite{top500analysis}. Individual cluster nodes contain
varying amounts and types of cores and sockets, main memory, \gls{numa} nodes or
accelerators like \glspl{gpu} or \glspl{fpga}. Since
\gls{hpc} software tries to leverage all these modern \gls{hpc}
hardware features, this complexity is also propagated to tasks and their resource requirements,
which can become relatively complex.

Some types of tasks might require a combination of several requirements, for example two
\glspl{gpu}, sixteen cores and 32 GiB of main memory. Some tasks are designed in a
way that allows them to leverage an open-ended range of resources, e.g.\ a task might require four
cores, but if more are available, it could use as many as possible. Furthermore, some tasks might
even support several variants of requirements, for example a task might either use four cores and a
single \gls{gpu} (if there is one available), or it could use more cores (and no \gls{gpu})
to offset the absence of an accelerator.

A resource requirement that is fairly specific to \gls{hpc} systems is the
requirement of using multiple nodes per single task. This requirement is necessary for programs
that are designed to be executed in a distributed fashion, such as programs using
\gls{mpi}, which are quite common in \gls{hpc}. This requirement
is not supported in many task runtimes, because their programming model assumes that a task
performs an atomic computation that executes on a single node. The use-case of tasks using multiple
nodes is discussed in more detail later in this chapter.

To support the mentioned scenarios, task runtimes should allow users to specify arbitrarily
fine-grained and abstract resource requirements for each task. They should also allow users to
attach resources that will satisfy these requirements to each individual instance of an execution
environment that will execute the tasks. Runtimes should also be able to take these requirements
into account when scheduling, both to make sure that the requirements are upheld, and also to
utilize the available hardware effectively.

\section{Data transfers}
After a task is computed, it can produce various data outputs, standard error or output streams,
files created on the disk or data objects that are then passed as inputs to dependent tasks. There
are many ways of storing and transferring these outputs. Some task frameworks store task outputs on
the filesystem, since it is relatively simple to implement, and it provides support for basic data
resiliency out-of-the-box.

\gls{hpc} nodes might not contain any local disks, but instead use shared
filesystems accessed over a network. While this can be seen as an advantage, since with a shared
filesystem it is much easier to share task outputs amongst different workers, it can also be a
severe bottleneck. Shared networked filesystems can suffer from quite high latency, and accessing
them can consume precious network bandwidth that is also used e.g.\ for managing computation
(sending commands to workers) or for direct worker-to-worker data exchange. Furthermore, data
produced in \gls{hpc} computations can be quite large, and thus storing it to a
disk can be a bottleneck even without considering networked filesystems.

These bottlenecks can be alleviated by transferring task outputs directly between workers over the
network (preferably without accessing the filesystem in the fast path), by streaming outputs
between tasks without the need to store them or by leveraging \gls{ram}
disks~\cite{hyperloom}. Making use of \gls{hpc} specific technologies,
such as \gls{mpi} or InfiniBand, could be also worthwhile to leverage the very
fast interconnects available in \gls{hpc} clusters.

Data outputs produced by tasks tend to be considered immutable in existing task runtimes, since a
single output can be used as an input to multiple tasks, and these might be executed on completely
different computational nodes. A problem that can arise with this approach is that if the data
outputs are large, but the computation within tasks that work with the data is short, the
serialization overhead (or even memory copy overhead, if the dependent task is executed on the same
node) can dominate the execution time. Such use-cases can be solved with stateful data management,
for example in the form of \emph{actors}, which can be considered stateful tasks that
operate on a single copy of some large piece of data.

\section{Fault tolerance}
Fault tolerance is relevant in all distributed computing environments, but
\gls{hpc} systems have specific requirements in this regard. As was already
mentioned, computational resources on \gls{hpc} clusters are provided through
allocation managers. Computing nodes allocated by these managers are provided only for a limited
duration, which means that for long-running task graphs, some nodes will disconnect and new nodes
will appear dynamically during the execution of the task graph. Furthermore, since the allocations
go through a queue, it can take some time before new computational resources arrive, therefore the
task graph can remain in a paused state, where no tasks are being executed, for potentially long
periods of time.

It is important for task runtimes to be prepared for these situations; they must handle node
disconnections gracefully, even if a task was being executed on a node that disconnects, and they
should be able to restart previously interrupted tasks on newly arrived workers. In general, in
\gls{hpc} scenarios, worker instability and frequent disconnects should be
considered the norm, not just a rare edge case.

\section{Multi-node tasks}
Many existing \gls{hpc} applications are designed to be executed on multiple
(potentially hundreds or even thousands) nodes in parallel, using e.g.\ \gls{mpi}
libraries or other communication frameworks. Multi-node execution could be seen as a special
resource requirement, which states that a task should be executed on multiple workers at once.

Support for multi-node tasks affects many design areas of a task runtime:
\begin{description}
	\item[Scheduling] When a task requires multiple nodes for execution and not enough nodes are available at a given
		moment, the scheduler has to decide on a strategy that will allow the multi-node task to execute.
		If it was constantly trying to backfill available workers with single-node tasks, the multi-node
		tasks could be starved.

		The scheduler might thus have to resort to keep some nodes idle for a while to enable the
		multi-node task to start as soon as possible. Another approach could be to interrupt the currently
		executing tasks and checkpoint their state to make space for a multi-node task, and then resume
		their execution once the multi-node task finishes.

		In a way, this decision-making already has to be performed on the level of individual cores even
		for single-node tasks, but adding multiple nodes per task makes the problem much more difficult.
	\item[Data transfers] It is relatively straightforward to express data transfers between single-node tasks in a task
		graph, because they naturally correspond to dependencies (edges) between the tasks. With multi-node
		tasks, the data distribution patterns become more complex, for example data can be replicated from
		a single node to multiple nodes when a multi-node task starts or gathered (reduced) from multiple
		nodes to a single node when such task finishes.

		When several multi-node tasks depend on one another, the task runtime should be able to exchange
		data between them in an efficient manner. This might require some cooperation with the used
		communication framework (e.g.\ \gls{mpi}) to avoid needless repeated serialization
		and deserialization.
	\item[Fault tolerance] When a node executing a single-node task crashes or disconnects from the runtime, its task can be
		rescheduled to a different worker. In the case of multi-node tasks, failure handling requires more
		communication and is generally more complex. When a task is executing on four nodes and one of them
		fails, the runtime has to make sure that the other nodes will be notified of this situation, so
		that they can react accordingly (either by finishing the task with a smaller number of nodes or by
		also failing immediately).
\end{description}

To enable common \gls{hpc} usecases, task runtimes should be able to provide some
support for multi-node tasks and allow them to be combined with single-node tasks. Advanced
multi-node task support could be provided e.g.\ by offering some kind of integration with
\gls{mpi} or similar common \gls{hpc} technologies.

\section{Scalability}
The sheer scale of \gls{hpc} performance (node count, core count, network
interconnect bandwidth) opens up opportunities for executing large scale task graphs, but that in
turn presents unique challenges for task runtimes. Below you can find several examples of
bottlenecks that might not matter in a small computational scale, but that can become problematic
in the context of \gls{hpc}-scale task graphs.

\begin{description}
	\item[Task graph materialization] Large computations might require building massive task graphs that contain millions of tasks. The
		task graphs are typically defined and built outside the task runtime itself, for example on the
		login nodes of computing clusters or on client devices (e.g.\ laptops), which can provide only
		relatively low performance. It can be quite slow to build, serialize and transfer such graphs over
		the network to the task runtime. This can create a bottleneck even before any task is executed.
		This has been identified as an issue in existing task runtimes~\cite{dask-client-perf}.

		In such case, it can be beneficial to provide an \gls{api} for defining task graphs
		in a symbolic way, for example by representing a potentially large group of similar tasks by a
		single entity. Such symbolic graphs could then be sent to the runtime in a compressed form and
		re-materialized only at the last possible moment. In an extreme form, the runtime could operate on
		such graphs in a fully symbolic way, without ever materializing them.
	\item[Communication overhead] Scaling the number of tasks and workers will necessarily put a lot of pressure on the communication
		network, both in terms of bandwidth (sending large task outputs between nodes) and latency (sending
		small management messages between the scheduler and the workers). Using \gls{hpc}
		technologies, such as \gls{mpi} or a lower-level interface like
		\gls{rdma}, could provide a non-trivial performance boost in this regard.

		As we have demonstrated in~\cite{pspin, spin2}, in-network computing, an active area of
		research, can be also used to optimize various networking applications by offloading some
		computations to an accelerated \gls{nic}. This approach could also be leveraged for
		task runtimes, for example by reducing the latency of management messages between the scheduler and
		workers or by increasing the bandwidth of large data exchanges amongst workers, by moving these
		operations directly onto the network card.
	\item[Runtime overhead] As we have shown in~\cite{rsds}, task runtimes with a centralized scheduler have to
		make sure that their overhead remains manageable. Even with an overhead of just
		$1ms$ per task, executing a task graph with a million tasks would result in
		total accumulated overhead of twenty minutes! Our results indicate that increasing the performance
		of the central scheduling and management component of a task runtime can have a large positive
		effect on the overall time it takes to execute the whole task graph.

		However, the performance of the central server cannot be increased endlessly, and from some point,
		using a centralized architecture, which is common to task runtimes, itself becomes a bottleneck.
		Even if the workers exchange large output data directly between themselves, any single, centralized
		component may become overloaded simply by coordinating and scheduling the workers.

		In that case, a decentralized architecture could be leveraged to avoid the reliance on a central
		component. Such a decentralized architecture can be found e.g.\ in Ray~\cite{ray}.
		However, to realize the gains of a decentralized architecture, task submission itself has to be
		decentralized in some way, which might not be a natural fit for common task graph workflows. If all
		tasks are generated from a single component, the bottleneck will most likely remain even in an
		otherwise fully decentralized system.
\end{description}

\section{Iterative computation}
A natural way of executing task graphs is to describe the whole computation with a single task
graph, submit the graph to the task runtime and wait until all the tasks are completed. However,
there are some computations that need a more iterative approach. Training a machine learning model
can be stopped early if the loss is no longer decreasing. A chemical or physical simulation is only
considered completed once a desired accuracy has been reached, which might take a previously
unknown number of steps. These scenarios and many others like them, are quite common in
\gls{hpc} use cases.

To support iterative computation, task runtimes should allow the user to stop the execution of a
task graph (or its subgraph) once a specific condition is met, and also to add new tasks to the
task graph in a dynamic fashion, if it is discovered that more iterations are needed.

\section{Summary}
Even though more \gls{hpc} use-cases and oddities could always be found, it is
already clear from the mentioned challenges that \gls{hpc} use-cases that
leverage task graphs can contain a lot of complexity. It could be possible to add support for some
mentioned requirements to existing task runtimes, which are described in the next section. However,
the described challenges are so diverse and complex that a dedicated approach which considers them
holistically could provide a better solution that would avoid both ergonomics and performance from
being compromised.

The aforementioned requirements will serve as a basis for further research in the proposed thesis.
The goal of the thesis is to design approaches for executing task graphs on
\gls{hpc} systems that take the aforementioned requirements into account. These
approaches will leverage the \hyperqueue{} task runtime, which is described further
in \Autoref{ch:hyperqueue}.
