In order to discuss task-based programming, it is useful to provide a vocabulary of terms related
to it and a set of definitions that describe the basic properties of task graphs and also the rules
of their execution. These definitions necessarily have to be relatively general, so that they can
be applied to a wide range of systems; even though the previous chapter has specified a relatively
precise subset of task-based programming models that will be examined and analyzed in this thesis,
there is still a large number of tools and systems that belong to this area of interest, each with
their own distinct concepts and semantic rules. Therefore, it would be infeasible to provide a
single unifying and complete task theory that would encompass details of all these tools and
programming models.

This chapter defines a set of concepts related to tasks, with a particular focus on properties that
are important for task graph execution in \gls{hpc} environments, which will be
further described in~\Autoref{ch:sota}. The definitions described in this chapter are
specifically adapted to the needs and terminology of this thesis, as is usual for other research
works. They form the lowest common denominator that can be applied to the specific task execution
tools and programming models discussed throughout this thesis. The presented terms are similar to
definitions that can be found in existing works~\cite{task_scheduling,hagras2003static,wang2018list}, although they differ in
several aspects. In particular, we do not assume that task execution times nor output data sizes
are known in advance, and we define a very general concept of resource management, which can be
used to model complex resources of heterogeneous clusters. Some of the definitions related to
resource management will be further extended in~\Autoref{ch:hyperqueue}.

\section{Task graphs}
\label{sec:task-graphs}
A computational workflow in the task-based programming model is represented with a
\gls{dag} that we will label as a \emph{task graph}. From a high-level
perspective, it describes which individual computational steps should be performed, what are the
constraints for where and in which order they should be computed and how should data be transferred
between the individual steps of the workflow.

There are many variations of task graphs that differ in the computational properties they are able
to describe. In the most basic form, task graph vertices represent computations that should be
performed and the graph arcs (edges) represent dependencies between the computations, which enforce
an execution order. However, numerous other concepts can also be encoded in a task graph. For
example, in addition to dependencies, the arcs could represent abstract communication channels,
through which the outputs of one computation are transferred to become the inputs of another
computation that depends on it. As another example, there could be a special type of arc which
specifies that the outputs of a computation will be streamed to a follow-up computation, instead of
being transferred in bulk only after the previous computation has finished.

As was already noted, the exact semantics of vertices and arcs of task graphs depend heavily on the
specifics of tools that implement them, and it is thus infeasible to provide a single definition
that would fit all variants used ``in the wild''. To provide a baseline definition, we will
formally define a task graph that can model dependencies between tasks, the notion of transferring
data outputs between tasks, and also the resources needed for the execution of individual tasks.

\newcommand{\alltaskpairs}{\forall t_1\in{}T, \forall t_2\in{}T}

\vspace{2mm}\makedef{def:task_graph}{Task graph} A \emph{task graph} is a tuple
$G = \taskgraphinner{\setresourcekindstaskgraph}$, where:
\begin{itemize}[itemsep=0pt]
	\item $\settasks$ is a set of \emph{tasks}.
	\item $\setobjects$ is a set of \emph{data objects}.
	\item $\setarcs \subseteq ((\settasks\times{}\setobjects) \cup (\setobjects\times{}\settasks))$
	is a set of arcs.
	\item $(\settasks \cup \setobjects, \setarcs)$ has to form a finite directed acyclic graph.

	The absence of cycles is important; otherwise, the task graph could not be executed.
	\item For every data object, there has to be exactly one task that produces it: \vspace{1mm}\\
	$\forall o \in \setobjects: |\setarcs \cap (\settasks \times \{o\})| = 1$
	\item $\setresourcekindstaskgraph$ is a set of \emph{resource kinds}. Each resource kind
	      describes a type of resource (e.g.\ a \gls{cpu} core or a \gls{gpu} device)
	      that might be required to execute a specific task.
	\item \resourcerequirement{\fntaskres}{N} is a function that defines the
	      \emph{resource requirement} of a task for a given resource kind.

	      A resource requirement specifies what amount of the given resource kind is required to
		  be available at a computational provider so that it can execute the given task.
		  $\fntaskres(t, r) = 0$ specifies that task $t$ does not require resource kind $r$ for its
		  execution.
\end{itemize}

\vspace{2mm}Below, we will define several terms that will be useful for describing task
graphs and their properties. For the following definitions, assume that we work with a
task graph $G = \taskgraphinner{\setresourcekindstaskgraph}$:
\begin{itemize}[itemsep=0pt]
	\item If there is an arc from a task to a data object ($(t,o) \in (\settasks\times{}\setobjects)$),
		  then we call
	      $t$ the \emph{producer} of $o$ and
	      $o$ the \emph{output} of $t$.
	\item If there is an arc from a data object to a task ($(o,t) \in (\setobjects\times{}\settasks)$), then we
		  call $t$ the \emph{consumer} of $o$ and $o$ the \emph{input} of $t$.

	\item Let us introduce a binary relation $\mathit{DR}_G$: \\
	$\mathit{DR}_G = \{(t_1, t_2)\in{}(\settasks\times{}\settasks) \mid t_1 \neq t_2 \land
		      \exists{}o\in{}O\colon (t_1, o)\in{}\setarcs
		      \land (o, t_2)\in{}\setarcs\}$ \\ When
	      $(t_1, t_2) \in \mathit{DR}_G$, we say that $t_2$ \emph{directly depends} on
	      $t_1$. We can also state that $t_2$ consumes the output produced
	      by $t_1$.

	\item Let us introduce a binary relation $D_G$, which forms the transitive closure of
	      $\mathit{DR}_G$. More explicitly, tasks $(t, t')$ belong to $D_G$ if there
	      exists a sequence $(t_1, t_2, \ldots, t_n)$ such that $t = t_1 \land t' = t_n$ and
		  $\forall i \in \{1,2,\ldots,n - 1\}: (t_i, t_{i+1}) \in DR_G$.

	      When $(t_1, t_2) \in D_G$, we say that $t_2$ \emph{depends} on
	      $t_1$ and that $t_1$ is a \emph{dependency} of
	      $t_2$.

	\item We call tasks without any inputs \emph{source tasks}: \vspace{1mm}\\ $S_G = \{ t\in{}\settasks \mid
		      \forall{}t_d\in{}\settasks\colon (t_d, t)\notin D_G\}$

		  It is a simple observation that unless the task graph is empty ($\settasks = \emptyset$), there is always at least one source task in the graph, because the graph is acyclic and finite.
	\item We call tasks that are not depended upon by any other task \emph{leaf tasks}: \vspace{1mm}\\
	      $L_G = \{ t\in{}\settasks \mid \forall{}t_d\in{}\settasks: (t, t_d)\notin D_G\}$.
\end{itemize}

An example of a simple task graph is shown in~\Autoref{fig:task-graph-example}. Tasks are represented as
circles, data objects as (rounded) rectangles and arcs as arrows. Task $t_1$
generates two data objects, which are then used as inputs for four additional tasks. The outputs of
these four tasks are then aggregated by a final task $t_6$. This could correspond
e.g.\ to a workflow where $t_1$ generates some initial data, tasks
$t_{2-5}$ perform some computation on that data and $t_6$ then
performs a final postprocessing step and stores the results to disk.

\begin{figure}[h]
	\centering
	\resizebox{!}{35mm}{
	\begin{tikzpicture}
			\tikzset{%
				data/.style={rectangle, draw, rounded corners, minimum size=8mm},
			}
            \graph[
                grow right sep=11mm,
            ] {
                "$t_1$"[task] -> {
                    "$o_{1a}$"[data] -> {
                        "$t_2$"[task] -> "$o_{2}$"[data],
                        "$t_3$"[task] -> "$o_{3}$"[data],
                        "$t_4$"[task] -> "$o_{4}$"[data]
                    },
                    "$o_{1b}$"[data] -> {
                        "$t_5$"[task] -> "$o_{5}$"[data]
                    }
                } -> "$t_6$"[task]
            };
        \end{tikzpicture}
	}
	\caption{Simple task graph with six tasks and six data objects}
	\label{fig:task-graph-example}
\end{figure}

Note that the presented definition of a task graph does not describe its semantics, i.e.\ how will
the graph be created and executed or what will be the interactions between tasks and data objects.
This depends on the specific tool that will execute the task graph. A baseline formal definition of
task graph execution properties will be provided in~\Autoref{sec:task-graph-execution}.

A \emph{task} is a serializable description of a computation that can be executed
repeatedly. The serializability property is crucial, as it allow us to treat computation as data.
That is a powerful concept, because it allows tasks to be sent between different nodes in a
cluster, stored to disk and also to be transparently recomputed an arbitrary number of times.
Enabling the recomputation of tasks is useful for achieving fault tolerance, as tasks might need to
be recomputed later if some failure occurs during their execution.

In practice, a single task will typically represent either the invocation a function (an executable
block of code) or the execution of a complete program. Multiple tasks in a task graph can refer to
the same function or program; each such task can have different inputs. In fact, this is a common
use-case, as task graphs are often used to parametrize a small set of functions or programs with
many different input parameters.

Even though we have defined the inputs and outputs of tasks as sets, in practice they are usually
stored using either ordered sequences or a mapping that associates a name with each input or
output, because it is important to maintain a specific ordering of both inputs and outputs. For
functions, the inputs are passed as arguments and the output is derived from its return value
(which can potentially form a sequence of values). Therefore, we have to be able to associate each
task input to a specific argument index. The same holds for tasks that execute programs. In this
case, inputs can be mapped to command-line arguments and the content of the \texttt{standard input stream},
and the output can be e.g.\ the content of the \texttt{standard output stream} generated by the executed
program or a set of files that it writes to the filesystem.

Each task can define its \emph{resource requirements}, a set of constraints that need to be satisfied by
a computational provider so that it can execute that task. As an example, a task that performs
training of a machine-learning model might require a \gls{gpu} to be present on the
computational node where it will be executed. Other resources might include e.g.\ a specific number
of \gls{cpu} cores or a minimum amount of \gls{ram} necessary to execute
a given task.

A \emph{data object} represents a dynamically computed result of a task; its value is not known
at the time of the task graph creation. Typically, it is a serialized blob of data that is
eventually transferred from the node where its producer was computed to the node where its consumer
should be executed. If a task programming model does not encode direct data transfers between
tasks, then data objects simply serve as ``empty'' markers of dependencies and they do not hold any
actual data. In that case, we could even remove them from the task graph completely and represent
task dependencies directly with arcs between tasks.

It is important to note that not all data used by tasks has to be encoded as a data object in the
task graph. As an example, tasks that represent function invocations are usually created by the
execution of some program (e.g.\ a Python script). A task graph defined in this way is usually
constructed with a specific set of input data for its \emph{source tasks}. This data can be
embedded directly within the definition of the function itself; in that case it is not represented
as an explicit data object. In other words, a task might represent a serializable description of a
computation along with its input data. That is why in the presented formal definition,
\emph{source tasks} do not have any explicit \emph{inputs}; it is expected that input
data is embedded directly within them.

Additionally, tasks can also read and modify the state of the environment in which they are being
executed, in a way that is observable by other tasks. For example, a function can read or modify
the value of a global variable, while a program can read an environment variable or create a file
on a disk, without it being specified as a task output. Such actions, which are usually called side
effects, are also typically not encoded within the task graph. Tasks should ideally contain as few
side effects as possible, because they can make task execution non-deterministic and cause them to
produce different outputs when executed multiple times, which is typically undesirable.

\section{Task execution}
\label{sec:task-graph-execution}
Task graphs merely describe some computation; therefore, they have to be executed in order to
actually produce some outputs and results. This is the responsibility of a \emph{task runtime},
a tool that analyzes task graphs and executes them in some \emph{computational environment}, e.g.\ a
personal computer or a distributed cluster. Such an environment contains a set of computational
providers that are able to execute tasks. We will label these providers as \emph{workers}.
A worker can execute a task by invoking its computation description (typically by calling a
function or executing a program), while passing it the previously computed inputs of the task.

There are many existing task runtimes with varying architectures, features and trade-offs, which
affect factors like performance, fault tolerance or expressivity of the supported variant of the
task-based programming model. Several task runtimes will be discussed throughout this thesis. In
the rest of this section, we will consider a case typical for \gls{hpc} environments;
a distributed task runtime with a central manager that communicates with a set of workers running
on remote nodes that communicate together via a network. We will also define a set of baseline
properties and rules that should be applicable to most task runtimes using this architecture,
without necessarily going into details of execution semantics that could differ across runtimes.

In general, a task runtime oversees all aspects of task graph execution. Its two main
responsibilities can be divided into managing communication with workers and handling the
scheduling and execution of tasks.

Worker management involves handling the lifetime of workers, facilitating data transfers between
them or providing resiliency in case of worker failures. A single worker is typically a program
running on a computational node, which is connected to the runtime through a network connection. It
receives commands from the task runtime, executes tasks and sends information about task execution
results back to the runtime. Each worker typically manages some hardware resources that are
available for tasks during their execution. Hardware resources can be assigned to workers in
various ways. There can be a single worker per the whole computational node or there could be
multiple workers per node, each managing a subset of the available resources (e.g.\ a single worker
per \gls{cpu} core).

The second main aspect that has to be handled by the runtime is the management of tasks. It has to
keep track of which tasks have already been computed, which tasks are currently being executed on
some worker(s) or which tasks are ready to be executed next. Two important responsibilities in this
area are fault tolerance and scheduling.

We will define \emph{fault tolerance} as the ability to gracefully handle various kinds of
failures that can happen during task graph execution, such as task or worker failures. When the
execution of a task fails with some error condition (e.g.\ because a worker executing the task
crashes), a fault-tolerant task runtime will be able to transparently restart it by launching a new
execution of that task. We will use the term \emph{task instance} (or simply \emph{instance}) for a specific execution of a
task. Task runtimes might impose limits on retrying failed tasks, e.g.\ by attempting to execute up
to a fixed number of task instances for each task before giving up, in order to avoid endless
failure loops.

The fact that it is even possible to execute a task multiple times is one of the main advantages of
the task-based programming model, where tasks declaratively describe a self-contained computation
that can be re-executed arbitrarily many times. This crucial property of tasks makes fault-tolerant
execution of task graphs easier to achieve than in other programming models, where individual
computations are not self-contained and serializable.

Below, we provide several definitions related to task graph execution that should be
applicable to most existing task runtimes. First, we will formally define a computational environment
(e.g.\ a cluster), an environment in which a task graph can be executed.

\vspace{2mm}\makedef{def:cluster}{Computational environment} A \emph{computational environment}
is a tuple $C = \clusterinner{\setresourcekindscluster}$, where:
\begin{itemize}[itemsep=0pt]
	\item $W$ is a set of workers (computational providers).
	\item $\setresourcekindscluster$ is a set of \emph{resource kinds}. Each resource kind
	      describes some type of resource (e.g.\ a \gls{cpu} core or a \gls{gpu} device) that
		  can be provided a worker.

	      Note that in all following definitions, we will assume that the set of resource kinds
		  of a computational environment is equal to the set of resource kinds of a task graph
		  computed in that environment.
	\item $\fnworkerres\colon W \times \setresourcekindscluster \rightarrow
	\mathbb{N}_{\geq{}0}$ is a function which defines how many resources are
	      provided by a worker for a specific resource kind.
\end{itemize}

Now we can describe the execution of a task graph. However, formally defining the behavior of
such a dynamic process is much more challenging than defining
the previous (relatively static) concepts, such as the task graph and the computational
environment. Each task runtime has its own set of execution semantics that affect the details of how are tasks assigned to
workers, how they are executed, how is data being transferred over the network, etc. Providing a formal
definition of this process that would be general and could be applied to multiple task runtimes
would thus be infeasible. On the other hand, it would be useful to have an option to examine if a
concrete task graph execution satisfied several constraints related to the dependencies and
resource requirements of tasks that most users would intuitively expect to hold.

Therefore, instead of defining the behavior of an execution itself, we assume that we have
available a set of information about an already performed execution, and we
will then examine if that execution has satisfied various properties. Note that in definitions
related to task graph execution, the set of non-negative real numbers ($\timedomain$) will
be used to represent points in time. For conciseness, the term \emph{execution} will also be used to denote a
\emph{task graph execution} in the rest of the text.

\vspace{2mm}\makedef{def:task_graph_execution}{Task graph execution} A \emph{task graph execution}
is a tuple \taskgraphexecution, where:
\begin{itemize}[itemsep=0pt]
	\item $G = \taskgraphinner{\setresourcekinds}$ forms a \emph{task graph}.
	\item $C = \clusterinner{\setresourcekinds}$ forms a \emph{computational environment}.
	\item $\fntaskworkerassigned\colon \settasks \times \timedomain \rightarrow W \cup \{\bot\}$ is a
	function that returns the worker that was currently
	      executing a specific task at a given point in time in $E$, or $\bot$ if that task was not
	      being executed at the given time.
	\item Each task had to be executed at
	least once: $\forall t\in{}\settasks, \exists tp\in\timedomain\colon \fntaskworkerassigned(t, tp) \neq \bot$
	\item Each task had to eventually finish its computation: \vspace{1mm}\\
	$\forall t\in{}\settasks, \exists tp\in\timedomain, \forall tp'\in\timedomain\colon tp' > tp \Rightarrow \fntaskworkerassigned(t, tp') = \bot$
\end{itemize}

Note that the definition above assumes that each task in $E$ was being executed on at most a
single worker at the same time. It could be generalized for tasks that can be executed on multiple
workers at once; however, that would have to be performed in the context of a specific task runtime,
as the semantics of \emph{multi-node} execution can vary significantly between runtimes.

It is also important to note that based on the definition of the $\fntaskworkerassigned$
function provided above, each task in $E$ could have been started multiple times (in multiple
\emph{instances}), even on different workers. This captures a basic form of fault tolerance.
However, we assume that each task must eventually successfully finish its execution. Additional
semantics of fault-tolerant task execution are not defined, because task runtimes handle task
re-execution in different ways; it would be infeasible to provide a general definition of task
retry semantics.

Next, we will define three helper functions in the context of a \emph{task graph execution} \\
$E = (\taskgraphinner{\setresourcekinds}, \clusterinner{\setresourcekinds}, X)$, which will be
used in later definitions.

\begin{itemize}[itemsep=0pt,topsep=2pt]
	\item Let
	$\fnworkertaskassigned_E\colon W \times \timedomain \rightarrow \powerset{\settasks}$ be a function
	that returns the set of tasks that were currently being
	executed on a given worker at a given point in time in $E$: \vspace{1mm}\\
	$\fnworkertaskassigned_E(w, tp) = \{t \in{} \settasks \mid \fntaskworkerassigned(t, tp) = w \}$
	\item Let $\fntaskstart_E\colon \settasks \rightarrow \timedomain$ be a function that returns the
	earliest point in time at
	which (any \emph{instance} of) a given task started its computation in $E$: \vspace{1mm}\\
	$\fntaskstart_E(t) = \min\limits_{tp \in\timedomain} \fntaskworkerassigned(t, tp) \neq \bot$
	\item Let $\fntaskfinish_E\colon \settasks \rightarrow \timedomain$ be a function that
	returns the latest point in time at which (any \emph{instance} of) a given task finished its computation in $E$: \vspace{1mm}\\
	$\fntaskfinish_E(t) = \max\limits_{tp \in\timedomain} \fntaskworkerassigned(t, tp) \neq \bot$
\end{itemize}

Unless a task runtime has some special semantics, then each execution
should uphold the following three basic constraints, which ensure a reasonable
behavior w.r.t.\ dependencies, task resource requirements and worker resources:

\vspace{2mm}\makedef{def:dependency_constraint}{Dependency constraint} A
\emph{dependency constraint} in the context of \emph{task graph execution} \\
$E = (G, C, X)$, where $G = \taskgraphinner{\setresourcekinds}$ is a \emph{task graph}, is
defined as follows: \vspace{1mm}\\
$\alltaskpairs\colon (t_1, t_2) \in D_G \Rightarrow \fntaskfinish_E(t_1) \leq \fntaskstart_E(t_2)$

\vspace{1mm}Informally, this property states that if a task $t_2$ depends on a task $t_1$, then it
cannot begin executing until $t_1$ has finished executing. This is a common interpretation of
the dependence relation between tasks that is enforced in most task runtimes. We assume that
executions performed by all task runtimes that will be further discussed in this thesis will always
uphold this constraint.

\vspace{2mm}\makedef{def:worker_resource_constraint}{Worker and task resource constraint} A
\emph{worker resource constraint} and a \emph{task resource constraint} in the context of
\emph{task graph execution}
$E = (\taskgraphinner{\setresourcekinds}, (W, \setresourcekinds, \fnworkerres), X)$
are defined as follows: \vspace{1mm}\\
$\forall tp\in\timedomain, \forall w\in{}W, \forall
r\in{}\setresourcekinds\colon (\sum_{t\in{}\fnworkertaskassigned_E(w, tp)} \fntaskres(t, r)) \leq
\fnworkerres(w, r)$

\vspace{1mm}This property both ensures that all resource requirements of all tasks are
satisfied at any point in time when these tasks are being executed and also that resources of
workers are not being oversubscribed.

\section{Task scheduling}
\label{sec:task-scheduling}
One of the most important responsibilities of a task runtime is \emph{task scheduling}. It is the
act of deciding in which order and on which specific worker(s) should each task execute, in a way
that optimizes some key metric. We will use the term \emph{scheduler} for a component of the
task runtime that is responsible for assigning tasks to workers by creating some form of a
\emph{schedule}.

In general terms, a schedule is a mapping that assigns tasks to specific workers that should
execute them and also assigns an order in which the tasks should be executed. However, as with task
graph execution, the semantics of scheduling and the structure of schedules depend on the specific
implementation of a task runtime. Schedules can be \emph{static}, in which case they are
produced just once before the task graph begins executing, or \emph{dynamic}, where the
scheduler generates the assignments on-the-fly, based on the current utilization of workers and the
observed durations of tasks that have already been executed. Some schedulers also retroactively
modify already produced schedules in reaction to dynamic situations that occur during task graph
execution (e.g.\ if a new worker connects to the cluster or if some worker is underloaded), while
others might not use any explicit schedules at all. Furthermore, the semantics of scheduling are
tightly coupled to the semantics of task graph execution in each specific task runtime, such as
fault tolerance, resource management, and other aspects. We will thus not provide a formal
definition of schedules, as it would necessarily have to choose a specific schedule structure that
might not be applicable to all task runtimes describes in this thesis.

What we can examine (and define) is some measure of the quality of a specific task graph execution
performed by a task runtime, which is typically affected by the behavior of its scheduler. There
are various metrics that a scheduler can optimize for, such as the latency to execute specific
critical tasks, but the most commonly used metric is \emph{makespan}:

\vspace{2mm}\makedef{def:makespan}{Makespan} The \emph{makespan} $M_E$ of execution
$E = (\taskgraphinner{\setresourcekinds}, C, \fntaskworkerassigned)$
is defined as follows: $M_E = \max\limits_{t\in\settasks}(\fntaskfinish_E(t)) - \min\limits_{t\in\settasks}(\fntaskstart_E(t))$

\vspace{2mm}Informally, makespan is the duration between the time when the earliest task starts
to be executed to the completion of all tasks.

\begin{figure}[h]
	\centering
	\resizebox{!}{70mm}{
	\begin{tikzpicture}
			\tikzmath{
				\tzerowidth = 15mm;
				\tonewidth = 20mm;
				\ttwowidth = 25mm;
				\tthreewidth = 35mm;
				\ozerowidth = 15mm;
				\oonewidth = 30mm;
			}
			\tikzset {
				taskstyle/.style={fill={rgb,255:red,21; green,66; blue,100}, text=white, draw=none},
				objstyle/.style={fill=black!60!green, text=white, draw=none},
			}

			% T1
			\node[task, taskstyle, minimum size=7.5mm] (t1) at (0, 0.5) {$t_1$};
			\node[data, objstyle, minimum size=5mm] (d1a) at (-1, -1) {$d_1$};
			\node[data, objstyle, minimum size=10mm] (d1b) at (1, -1) {$d_2$};
			\draw [arrow] (t1) edge (d1a.north) (t1) edge (d1b.north);

			% T2 and T3
			\node[task, taskstyle, minimum size=10mm] (t2) at (-2, -2.5) {$t_2$};
			\node[task, taskstyle, minimum size=12.5mm] (t3) at (0, -2.5) {$t_3$};
			\draw [arrow] (d1a) edge (t2.north) (d1a) edge (t3.north);

			% T4
			\node[task, taskstyle, minimum size=17.5mm] (t4) at (2, -2.5) {$t_4$};
			\draw [arrow] (d1b.south) edge (t4.north);

			% Move to the right to draw the timelines
			\tikzset{shift={(4,2)}}

			\node[anchor=west] at (0, 0) {Schedule $S_1$: $w_1$=\{$t_1$, $t_2$\}, $w_2$=\{$t_3$\}, $w_3$=\{$t_4$\}};

			\tikzset{shift={(0,-1)}}
			\node (tim1A) at (0, 0) {$w_1$};
			\draw[arrow] (tim1A.east) -- ++(9, 0);
			\node[below = 0.5 of tim1A.south] (tim1B) {$w_2$};
			\draw[arrow] (tim1B.east) -- ++(9, 0);
			\node[below = 0.5 of tim1B.south] (tim1C) {$w_3$};
			\draw[arrow] (tim1C.east) -- ++(9, 0);

			% Timeline 1, row 1
			\node[taskstyle, minimum width=\tzerowidth, right = 0.2 of tim1A.east] (tim1t0) {$t_1$};
			\node[taskstyle, minimum width=\tonewidth, right = 0.1 of tim1t0.east] (tim1t1) {$t_2$};

			% Timeline 1, row 2
			\node[objstyle, minimum width=\ozerowidth, below = 1 of tim1t1.west, anchor=west]
			(tim1o0) {$d_1$ ($w_1$)};
			\node[taskstyle, minimum width=\ttwowidth, right = 0.1 of tim1o0.east] (tim1t2) {$t_3$};

			% Timeline 1, row 3
			\node[objstyle, minimum width=\oonewidth, below = 1 of tim1o0.west, anchor=west]
			(tim1o1) {$d_2$ ($w_1$)};
			\node[taskstyle, minimum width=\tthreewidth, right = 0.1 of tim1o1.east]
			(tim1t3) {$t_4$};

			\draw[dashed, draw=red] (tim1t0.west) -- ++(0, -2.75) --
			([shift=({0,-0.75})]tim1t3.east) -- (tim1t3.east);

			\node[text=red] at (4.5, -3) {Makespan};

			% Move below to draw the timelines
			\tikzset{shift={(0,-4)}}

			\node[anchor=west] at (0, 0) {Schedule $S_2$: $w_1$=\{$t_1$, $t_4$\}, $w_2$=\{$t_3$\}, $w_3$=\{$t_2$\}};

			\tikzset{shift={(0,-1)}}
			\node (tim2A) at (0, 0) {$w_1$};
			\draw[arrow] (tim2A.east) -- ++(9, 0);
			\node[below = 0.5 of tim2A.south] (tim2B) {$w_2$};
			\draw[arrow] (tim2B.east) -- ++(9, 0);
			\node[below = 0.5 of tim2B.south] (tim2C) {$w_3$};
			\draw[arrow] (tim2C.east) -- ++(9, 0);

			% Timeline 2, row 1
			\node[taskstyle, minimum width=\tzerowidth, right = 0.2 of tim2A.east] (tim2t0) {$t_1$};
			\node[taskstyle, minimum width=\tthreewidth, right = 0.1 of tim2t0.east] (tim2t1)
			{$t_4$};

			% Timeline 2, row 2
			\node[objstyle, minimum width=\ozerowidth, below = 1 of tim2t1.west, anchor=west]
			(tim2o0) {$d_1$ ($w_1$)};
			\node[taskstyle, minimum width=\ttwowidth, right = 0.1 of tim2o0.east] (tim2t2) {$t_3$};

			% Timeline 2, row 3
			\node[objstyle, minimum width=\ozerowidth, below = 1 of tim2o0.west, anchor=west]
			(tim2o1) {$d_1$ ($w_1$)};
			\node[taskstyle, minimum width=\tonewidth, right = 0.1 of tim2o1.east] (tim2t3) {$t_2$};

			\draw[dashed, draw=red] (tim2t0.west) -- ++(0, -2.75) --
			([shift=({0,-1.75})]tim2t2.east) -- (tim2t2.east);
		\end{tikzpicture}
	}
	\caption{Task graph executed with two different schedules}
	\label{fig:scheduling-example}
\end{figure}

\vspace{4mm}Task scheduling is so crucial because it has a profound effect on the
efficiency of the whole workflow execution. We can observe that in~\Autoref{fig:scheduling-example}, which
shows two executions of a simple task graph that demonstrate how a trivial change in the used
schedule can severely affect the resulting makespan. The figure contains a task graph with four
tasks and two data objects. The size of the circles is proportional to the execution duration of
the tasks and the size of the rounded rectangles is proportional to the size of the data objects.

Let us assume that we want to execute this task graph in a computational environment with three
workers $(w_1, w_2, w_3)$. Two different executions using different schedules are shown in the
figure. Schedule $S_1$ assigns tasks $t_1$ and
$t_2$ to worker $w_1$, task $t_3$ to worker
$w_2$ and task $t_4$ to worker $w_3$, while
schedule $S_2$ assigns tasks $t_1$ and $t_4$ to
worker $w_1$, task $t_3$ to worker $w_2$ and
task $t_2$ to worker $w_3$. The two timelines show the execution
of tasks (blue rectangles) and the network transfers of data objects between workers (green
rectangles) for each individual worker. It is clear that with $S_2$, the task
graph will be computed faster than with $S_1$, even though the only difference
between the two schedules is that the tasks $t_2$ and $t_4$ were
swapped between workers $w_1$ and $w_3$. Note that the timeline
assumes that a worker can overlap the computation of a task with the transfer a data object to
another worker over the network, which is commonly supported by existing task runtimes.

Optimal scheduling of tasks to workers is an NP-hard~\cite{Ullman1975} problem even for the
most basic scenarios, when the exact execution duration of each task is known, and even if we do
not consider the duration of transferring data between workers over a network. Task runtimes thus
resort to various heuristics tailored to their users' needs. Some classic task scheduling
heuristics and their comparisons can be found in~\cite{estee,hlfet1974,kwok1998benchmarking,hagras2003static,wang2018list}. \Autoref{ch:estee}
provides a comprehensive survey of various task scheduling algorithms.

Scheduling heuristics have to take many factors into consideration when deciding on which worker
should a task be executed:

\begin{description}[wide=0pt,itemsep=0pt,topsep=4pt]
	\item[Resource requirements] The scheduler should respect all resource requirements specified by tasks. The runtime thus has to
		observe the dynamically changing available resources of each worker and schedule tasks accordingly,
		to uphold their requirements. This can be challenging especially in the presence of complex
		resource requirements.
	\item[Data transfer cost] If the runtime operates within a distributed cluster, one of the most important scheduling aspects
		that it needs to consider is the transfer cost of data between workers over the network. All
		benefits gained by computing a task on another worker to achieve more parallelization might be lost
		if it takes too much time to send the data (task outputs) to that worker.

		The scheduler thus has to carefully balance the communication-to-computation ratio, based on the
		available network bandwidth, sizes of outputs produced by tasks and the current utilization of
		workers.
	\item[Scheduling overhead] The overhead of generating the schedule itself also cannot be underestimated. As was already
		stated, computing an optimal solution quickly is infeasible, but even heuristical approaches can
		have wildly different performance characteristics. Producing a lower quality schedule sooner,
		rather than a higher quality schedule later, can be sometimes beneficial.
\end{description}

\section*{Summary}
This chapter has provided a general definition of the most important terms related to task-based
programming models that will be used throughout this chapter. It has introduced the notion of task
graphs, tasks, data objects, resource requirements, workers, task runtimes and task scheduling.

The following chapter will focus on describing what challenges are faced by users and task runtimes
when they execute task graphs on \gls{hpc} clusters.
