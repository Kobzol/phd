In order to discuss task-based programming models, it is useful to provide a vocabulary of terms
related to them. This is not as straightforward as it might seem, because even though the previous
chapter has specified a very precise subset of task-based programming models that will be examined
and analyzed in this thesis, there is still a large number of tools and systems that fall under
this area of interest, each with their own distinct concepts and semantic rules. Therefore, it is
not feasible to provide a single unifying and complete task theory that would encompass all of
them, without generalizing heavily and glossing over many important details.

Nevertheless, this chapter introduces a basic set of concepts that form a lowest common denominator
baseline that can be applied to all tools and use-cases discussed later in this thesis. In
particular, it defines the most important terms related to tasks, including properties that are
most relevant to task graph execution in \gls{hpc} environments, which will be further
described in~\Autoref{ch:sota}.

\section{Task graphs}
A computational workflow in the task-based programming model is represented with a
\gls{dag} that we will label as a \emph{task graph}. From a high-level
perspective, it describes which individual computational steps should be performed, what are the
constraints for where and in which order should they be computed and how should data be transferred
between the individual steps of the workflow.

There are many variations of task graphs, based on the computational properties that they are able
to describe. In the most basic form, task graph vertices represent computations to be performed,
and the arcs (edges) represent dependencies between the computations, which enforce an order in
which they should be executed. However, numerous other concepts can also be encoded in a task
graph. For example, in addition to dependencies, the arcs could represent abstract communication
channels, through which the outputs of one computation are transferred to become the inputs of
another computation that depends on it. As another example, there could be a special type of arc
which specifies that the outputs of a computation will be streamed to a follow-up computation,
instead of being transferred in bulk only after the previous computation has finished. There could
also be e.g.\ a special type of node that defines iterative computation that is performed
repeatedly until some condition is met.

As was already noted, the exact semantics of vertices and arcs of task graphs depend heavily on the
specifics of tools that implement them, and thus it is not possible to provide a single definition
that would fit all variants used ``in the wild''. To provide a baseline definition, we will
formally define a task graph that can capture dependencies between tasks and the notion of
transferring data between them.

A task graph is a tuple $(T, O, E)$, where $T$ is a set of
\emph{tasks}, $O$ is a set of \emph{data objects} and
$E \subseteq ((T\times{}O) \cup (O\times{}T))$ is a set of arcs. $(T \cup O, E)$ forms a finite directed acyclic
graph. The graph is structured in a way so that for every data object, there is exactly one task
that produces it: $\forall o\in{}O: (\exists t_1\in{}T: (t_1, o) \in E \land
	(\forall
	t_2\in{}T: (t_2, o) \in E \Rightarrow t_1 = t_2))$.

Below we will define several terms that will be useful for describing task graphs and their
properties:
\begin{enumerate}
	\item If there is an arc from a task to a data object ($(t,o) \in (T\times{}O)$), then we call
	      $t$ the \emph{producer} of $o$ and $o$
	      the \emph{output} of $t$.
	\item If there is an arc from a data object to a task ($(o,t) \in (O\times{}T)$), then we call
	      $t$ the \emph{consumer} of $o$ and $o$
	      the \emph{input} of $t$.

	\item Let us introduce a binary relation $D_r$ over the set of tasks. Assume that we
	      have a pair of tasks: $\{(t_1, t_2): (t_1, t_2)\in{}(T\times{}T)\land t_1 \neq t_2 \land
		      \exists{}o\in{}O: (t_1, o)\in{}E
		      \land (o, t_2)\in{}E\}$. When $(t_1, t_2) \in D_r$, we say that
	      $t_2$ \emph{directly depends} on $t_1$. We can also state that
	      $t_2$ consumes the output produced by $t_1$.

	\item Let us introduce a binary relation $D$ over the set of tasks. Tasks
	      $t_1$ and $t_n$ are in this relation if there is a sequence
	      $(t_1, t_2, \ldots, t_n)$ such that $\forall i \in \{
		      1,2,\ldots,n - 1\}: (t_i, t_{i+1}) \in D_r$. When $(t_1, t_2) \in D$, we say that
	      $t_2$ \emph{depends} on $t_1$ and that
	      $t_1$ is a \emph{dependency} of $t_2$.

	\item We label tasks without any dependencies \emph{source tasks}. $S = \{ t \mid t\in{}T \land \forall{}t_d\in{}T:
		      (t_d, t)\notin D\}$. It is a
	      simple observation that unless the task graph is empty ($(T\cup{}O) = \emptyset$), there is always at
	      least one source task in the graph, because the graph is acyclic.
	\item We label tasks that are not depended upon by any other task \emph{leaf tasks}.
	      $L = \{ t \mid t\in{}T \land \forall{}t_d\in{}T: (t,
		      t_d)\notin D\}$.
\end{enumerate}

% \footnote{This representation will be used in all following task graph diagrams.}

An example of a simple task graph is shown in~\Autoref{fig:task-graph-example}. Tasks are represented as
circles, data objects as (rounded) rectangles and arcs as arrows. The source task
$t_0$ generates two data objects, which are then used as inputs for four
additional tasks. The outputs of these four tasks are then aggregated by a final task
$t_5$. This can correspond e.g.\ to a workflow where $t_0$
generates some data, $t_{1-4}$ performs a calculation on that data and
$t_5$ then performs a final post-processing step and stores the results to disk.

\begin{figure}[h]
	\centering
	\resizebox{!}{35mm}{
	\begin{tikzpicture}
			\tikzset{%
				data/.style={rectangle, draw, rounded corners, minimum size=8mm},
			}
            \graph[
                grow right sep=11mm,
            ] {
                "$t_1$"[task] -> {
                    "$o_{1a}$"[data] -> {
                        "$t_2$"[task] -> "$o_{2}$"[data],
                        "$t_3$"[task] -> "$o_{3}$"[data],
                        "$t_4$"[task] -> "$o_{4}$"[data]
                    },
                    "$o_{1b}$"[data] -> {
                        "$t_5$"[task] -> "$o_{5}$"[data]
                    }
                } -> "$t_6$"[task]
            };
        \end{tikzpicture}
	}
	\caption{Simple task graph with six tasks and six data objects}
	\label{fig:task-graph-example}
\end{figure}

Note that the presented definition of a task graph does not describe its semantics -- how will the
graph be created and executed and what will be the interactions between tasks and data objects.
This depends on the specific tool or framework that will execute the task graph. As an example, the
dependence of task $t_2$ on task $t_1$ could define the following
invariant: $t_2$ cannot start to execute until $t_1$ has finished
executing and the data objects produced by $t_1$ have been transferred to the
computational node that will execute $t_2$.

A \emph{task} is a serializable description of a computation that can be executed
repeatedly. The serializability property is crucial, as it allow us to treat computation as data.
That is a powerful concept, because it allows tasks to be sent between different nodes in a cluster
or stored to disk and to be transparently recomputed an arbitrary number of times. Enabling the
recomputation of tasks is useful for achieving fault tolerance, as tasks might need to be
recomputed later if some failure occurs during their execution.

In practice, a single task will typically represent either the invocation a function (an executable
block of code) or the execution of a whole program. Multiple tasks in a task graph can refer to the
same function or program, since each such task can have different inputs. In fact, this is a common
case, as task graphs are often used to parametrize a small set of functions or programs with many
different input parameters.

Even though the inputs and outputs of tasks were defined as sets in the formal definition, in
practice they are usually stored using either ordered sequences or a mapping that associates a name
with each input or output, because it is important to maintain a specific ordering both inputs and
outputs. For functions, the inputs are passed as arguments, and the output is derived from its
return value (which can potentially form a sequence of values). Therefore, we have to be able to
associate each task input to a specific argument index. The same holds for tasks that execute
programs. In this case, inputs can be mapped command-line arguments and the content of the
\texttt{standard input stream}, and the output can be e.g.\ the content of the \texttt{standard output stream}
generated by the executed program.

While inputs and outputs explicitly describe how data flows in and out of tasks, when a task is
executed, it can additionally also read and modify the state of the environment in which it is
being executed, in a way that is observable by other tasks. For example, a function can read or
modify the value of a global variable, while a program can read an environment variable or create a
file on a disk, without it being specified as a task output. Such actions, which we will label as
\emph{side effects}, are typically not encoded within the task graph. Tasks should ideally
contain as few side effects as possible, because they can make task execution non-deterministic,
causing them to produce different outputs when executed multiple times, which might not be
desirable. It might be difficult to completely avoid side effects though, as most task graphs will
eventually store some data to a file-system when executed, usually to persist the results of the
computation.

Apart from specifying the required inputs and produced outputs, tasks can also define various
constraints for the environment in which they will be executed. We will use the term
\emph{resource requirements} for such constraints. As an example, a task that performs training of a
machine learning model might require a \gls{gpu} to be present on the computational
node where the task will be executed. Other resources might include e.g.\ a minimum required amount
of \gls{ram} or a specific number of \gls{cpu} cores necessary to execute
the task.

A \emph{data object} represents the computed result of a task. Typically, it is a serialized
blob of data that is eventually transferred from the node where its producer was computed to the
node where its consumer should be executed. If a task programming model does not encode direct data
transfers between tasks, then data objects simply serve as ``empty'' markers of dependencies, and
they do not hold any actual data. In that case, we could even remove them from the task graph
completely, and represent task dependencies directly with arcs between tasks.

\section{Task execution}
Task graphs merely describe some computation, therefore they have to be executed in order to
actually produce some outputs and results. This is the responsibility of a \emph{task runtime},
a tool that analyzes task graphs and executes them in some \emph{computational environment}, e.g.\ a personal
computer or a distributed cluster. Such an environment contains a set of computational providers
that are able to execute tasks. We will label these providers as \emph{workers}. A worker
can execute a task by invoking the computation assigned to it (typically by calling a function or
executing a program), and passing it the inputs of the task. Usually a task can only be executed
once all of its inputs are ready, which means that the dependencies of the task have been computed,
and their outputs have been transferred to the computational node of the worker.

There are many existing task runtimes, with varying architectures, features and trade-offs, which
affect factors like performance, fault tolerance or expressivity of the supported variant of the
task-based programming model. Selected task runtimes will be discussed in the following chapter,
and~\Autoref{ch:rsds} will then describe the architecture of the \dask{} task
runtime in detail, and examine its performance bottlenecks. In the rest of this section, we will
consider a case typical for \gls{hpc} environments; a distributed task runtime with a
central manager that communicates with a set of workers running on distributed nodes that
communicate together via a network.

In general, a task runtime oversees all aspects of task graph execution. Its two main
responsibilities can be divided into managing communication with the workers, and handling the
scheduling and execution of tasks.

Worker management involves handling the lifetime of workers (connection and disconnection from the
cluster), facilitating data transfers between them or providing resiliency in case of worker
failures. A single worker is typically a program running on a computational node, which is
connected to the runtime manager. It receives commands from it, executes tasks and sends
information about task execution statuses back to the manager. Each worker typically manages some
hardware resources that are available for tasks during their execution. Hardware resources can be
assigned to workers in various ways. There can be a single worker per the whole computational node,
or there could be multiple workers per node, each managing a subset of the available resources
(e.g.\ a single worker per \gls{cpu} core).

The second main aspect that has to be handled by the runtime is the management of tasks. It has to
keep track of which tasks have already been computed, which tasks are currently being executed on
some worker(s) or which tasks are ready to be executed next, because their dependencies have
already been computed. Two important responsibilities in this area are fault tolerance and
scheduling.

In the context of task graph execution, we will define \emph{fault tolerance} as the ability to
gracefully handle task execution failures, and provide ways of retrying failed computations. When
the execution of a task fails with some error condition (e.g.\ because a worker executing the task
crashes), a fault-tolerant task runtime will be able to transparently restart it by launching a new
execution of that task. We will use the term \emph{task instance} for a specific execution of a
task. Runtimes might impose some limits on retrying failed tasks, e.g.\ by attempting to execute up
to a fixed number of task instances for each task before giving up, to avoid endless failure loops.

Tasks are usually considered to be atomic from the perspective of the runtime, i.e.\ they either
execute completely (and successfully), or they fail, in which case they might be restarted from
scratch. Task granularity thus plays an important role here, since when a task is large, then a lot
of work might have to be redone if it fails and is re-executed. Some runtimes try to alleviate this
cost by leveraging task checkpointing~\cite{task_checkpointing}, which is able to save the
computational state of a task during its execution, and then restore it in case of a failure, thus
avoiding the need to start from the beginning.

Fault tolerance is challenging in the presence of dependencies. When a task has inputs, the runtime
might have to store them (either in memory or in a serialized format on disk) even after the task
has started executing. Because there is always a possibility that it will have to restart the task
in case of a failure, and thus it needs to hold on to its inputs. In some cases, it can actually be
a better trade-off to avoid storing the inputs and instead re-execute the dependencies of the task
(even if they have been executed successfully before) to regenerate the inputs. This can help
reduce memory footprint, albeit at the cost of additional computation time, and it also might not
work well if the dependencies are not deterministic.

Note that the fact that it is even possible to execute a task multiple times is one of the core
advantages of the task-based programming model, where tasks declaratively describe a self-contained
computation that can be re-executed arbitrarily many times. This crucial property of tasks makes
fault-tolerant execution of task graphs possible.

\section{Task scheduling}
Arguably the most important responsibility of a task runtime (which also gets the most attention in
research works), is \emph{task scheduling}. It is the act of deciding in which order and on which
specific worker(s) should each task execute, in a way that optimizes some key metric. There are
many metrics that we can optimize for, such as the latency to execute specific critical tasks, but
the most commonly used metric is \emph{makespan} -- the duration between the start of the
execution of the first task to the completion of all tasks within the task graph.

We will use the term \emph{scheduler} for a component of the task runtime that is responsible
for assigning tasks to workers by creating \emph{schedules}. A schedule is a mapping that
assigns tasks to specific workers that should execute them. It can be \emph{static}, in
which case it is produced just once before the task graph begins executing, or
\emph{dynamic}, where the scheduler generates the assignments on-the-fly, based on the
current utilization of workers and the observed durations of tasks that have already been executed.
Some schedulers can also retroactively modify already produced schedules in reaction to dynamic
situations that occur during task graph execution (e.g.\ if a new worker connects to the cluster,
or if some worker is starving).

\begin{figure}[h]
	\centering
	\resizebox{!}{110mm}{
	\begin{tikzpicture}
			\tikzmath{
				\tzerowidth = 15mm;
				\tonewidth = 20mm;
				\ttwowidth = 25mm;
				\tthreewidth = 35mm;
				\ozerowidth = 15mm;
				\oonewidth = 30mm;
			}
			\tikzset {
				taskstyle/.style={fill={rgb,255:red,21; green,66; blue,100}, text=white, draw=none},
				objstyle/.style={fill=black!60!green, text=white, draw=none},
			}

			% T0
			\node[task, taskstyle, minimum size=7.5mm] (t0) at (0, 0.5) {$t_0$};
			\node[data, objstyle, minimum size=5mm] (d0a) at (-1, -1) {$d_0$};
			\node[data, objstyle, minimum size=10mm] (d0b) at (1, -1) {$d_1$};
			\draw [arrow] (t0) edge (d0a.north) (t0) edge (d0b.north);

			% T1 and T2
			\node[task, taskstyle, minimum size=10mm] (t1) at (-2, -2.5) {$t_1$};
			\node[task, taskstyle, minimum size=12.5mm] (t2) at (0, -2.5) {$t_2$};
			\draw [arrow] (d0a) edge (t1.north) (d0a) edge (t2.north);

			% T3
			\node[task, taskstyle, minimum size=17.5mm] (t3) at (2, -2.5) {$t_3$};
			\draw [arrow] (d0b.south) edge (t3.north);

			% Move below to draw the timelines
			\tikzset{shift={(-4,-4)}}

			\node[anchor=west] at (0, 0) {Schedule $S_0$: $w_0$=\{$t_0$, $t_1$\}, $w_1$=\{$t_2$\}, $w_2$=\{$t_3$\}};

			\tikzset{shift={(0,-0.75)}}
			\node (tim1A) at (0, 0) {$w_0$};
			\draw[arrow] (tim1A.east) -- ++(9, 0);
			\node[below = 0.5 of tim1A.south] (tim1B) {$w_1$};
			\draw[arrow] (tim1B.east) -- ++(9, 0);
			\node[below = 0.5 of tim1B.south] (tim1C) {$w_2$};
			\draw[arrow] (tim1C.east) -- ++(9, 0);

			% Timeline 1, row 1
			\node[taskstyle, minimum width=\tzerowidth, right = 0.2 of tim1A.east] (tim1t0) {$t_0$};
			\node[taskstyle, minimum width=\tonewidth, right = 0.1 of tim1t0.east] (tim1t1) {$t_1$};

			% Timeline 1, row 2
			\node[objstyle, minimum width=\ozerowidth, below = 1 of tim1t1.west, anchor=west]
			(tim1o0) {$d_0$ ($w_0$)};
			\node[taskstyle, minimum width=\ttwowidth, right = 0.1 of tim1o0.east] (tim1t2) {$t_2$};

			% Timeline 1, row 3
			\node[objstyle, minimum width=\oonewidth, below = 1 of tim1o0.west, anchor=west]
			(tim1o1) {$d_1$ ($w_0$)};
			\node[taskstyle, minimum width=\tthreewidth, right = 0.1 of tim1o1.east]
			(tim1t3) {$t_3$};

			\draw[dashed, draw=red] (tim1t0.west) -- ++(0, -2.75) --
			([shift=({0,-0.75})]tim1t3.east) -- (tim1t3.east);

			\node[text=red] at (4.5, -3) {Makespan};

			% Move below to draw the timelines
			\tikzset{shift={(0,-4)}}

			\node[anchor=west] at (0, 0) {Schedule $S_1$: $w_0$=\{$t_0$, $t_3$\}, $w_1$=\{$t_2$\}, $w_2$=\{$t_1$\}};

			\tikzset{shift={(0,-0.75)}}
			\node (tim2A) at (0, 0) {$w_0$};
			\draw[arrow] (tim2A.east) -- ++(9, 0);
			\node[below = 0.5 of tim2A.south] (tim2B) {$w_1$};
			\draw[arrow] (tim2B.east) -- ++(9, 0);
			\node[below = 0.5 of tim2B.south] (tim2C) {$w_2$};
			\draw[arrow] (tim2C.east) -- ++(9, 0);

			% Timeline 2, row 1
			\node[taskstyle, minimum width=\tzerowidth, right = 0.2 of tim2A.east] (tim2t0) {$t_0$};
			\node[taskstyle, minimum width=\tthreewidth, right = 0.1 of tim2t0.east] (tim2t1)
			{$t_3$};

			% Timeline 2, row 2
			\node[objstyle, minimum width=\ozerowidth, below = 1 of tim2t1.west, anchor=west]
			(tim2o0) {$d_0$ ($w_0$)};
			\node[taskstyle, minimum width=\ttwowidth, right = 0.1 of tim2o0.east] (tim2t2) {$t_2$};

			% Timeline 2, row 3
			\node[objstyle, minimum width=\ozerowidth, below = 1 of tim2o0.west, anchor=west]
			(tim2o1) {$d_0$ ($w_0$)};
			\node[taskstyle, minimum width=\tonewidth, right = 0.1 of tim2o1.east] (tim2t3) {$t_1$};

			\draw[dashed, draw=red] (tim2t0.west) -- ++(0, -2.75) --
			([shift=({0,-1.75})]tim2t2.east) -- (tim2t2.east);
		\end{tikzpicture}
	}
	\caption{Simple task graph and two different schedules}
	\label{fig:scheduling-example}
\end{figure}

Task scheduling is so crucial because it has a profound effect on the efficiency of the whole
workflow execution. We can observe that in~\Autoref{fig:scheduling-example}, which shows a schedule for a
simple task graph, and demonstrates how a trivial change in the schedule can severely affect the
resulting makespan. The figure contains a task graph with four tasks and two data objects. The size
of the circles is proportional to the execution duration of the tasks and the size of the rounded
rectangles is proportional to the size of the data objects. Let us assume that we want to schedule
this task graph to a cluster with three workers ($w_0$, $w_1$,
$w_2$). Two different schedules for this situation are shown in the figure.
Schedule $S_0$ assigns tasks $t_0$ and $t_1$ to
worker $w_0$, task $t_2$ to worker $w_1$ and
task $t_3$ to worker $w_2$. The timeline of the schedule shows
the execution of tasks (gray rectangles) and the network transfers of data objects between workers
(green rectangles) for each individual worker. It is clear that with the schedule
$S_1$, the task graph will be computed faster than with schedule
$S_0$, even though the only difference between the two schedules is that the tasks
$t_1$ and $t_3$ were swapped between workers
$w_0$ and $w_2$. Note that the schedule timeline assumes that a
worker can overlap the computation of a task with the transfer a data object to another worker over
the network, which is commonly supported by existing task runtimes.

Optimal scheduling of tasks to workers is an NP-hard~\cite{Ullman1975} problem even for the
most basic scenarios, when the exact execution duration of each task is known, and even if we do
not consider the duration of transferring data between workers over a network. Task runtimes thus
resort to various heuristics tailored to their users' needs. Some classic task scheduling
heuristics and their comparisons can be found in~\cite{hlfet1974,kwok1998benchmarking,hagras2003static,wang2018list,estee}. \Autoref{ch:estee}
contains a comprehensive survey of various task scheduling algorithms.

Scheduling heuristics have to take many factors into consideration when deciding on which worker
should a task be executed:

\begin{description}[wide=0pt]
	\item[Resource requirements] The scheduler should respect all resource requirements specified by tasks. The runtime thus has to
		observe the dynamically changing available resources of each worker and schedule tasks accordingly,
		to uphold their requirements. This can be challenging especially in the presence of complex
		resource requirements.
	\item[Data transfer cost] If the runtime operates within a distributed cluster, one of the most important scheduling aspects
		that it needs to consider is the transfer cost of data between workers over the network. All
		benefits gained by computing a task on another worker to achieve more parallelization might be lost
		if it takes too much time to send the data (task outputs) to that worker.

		The scheduler thus has to carefully balance the communication-to-computation ratio, based on the
		available network bandwidth, sizes of outputs produced by tasks and the current utilization of
		workers.
	\item[Scheduling overhead] The overhead of generating the schedule itself also cannot be underestimated. As was already
		stated, computing an optimal solution quickly is infeasible, but even heuristical approaches can
		have wildly different performance characteristics. Producing a lower quality schedule sooner,
		rather than a higher quality schedule later, can be sometimes beneficial.
	\item[Memory consumption] It is desirable to execute as many tasks in parallel on a given worker (with respect to its
		available parallelism), to speed up the completion of the whole workflow. However, the scheduler
		should also balance the number of concurrently executing tasks according to the total amount of
		memory that they consume. In general, it is difficult to predict for how long will a task execute,
		and how much (peak) memory will it consume. When a task executes longer than expected, the workflow
		will still be computed, it will just take more time. But when a task uses more memory than
		expected, or the scheduler puts too many tasks on a worker at the same time, then the worker might
		run out of memory and crash. If this happens repeatedly, it can stall or completely stop the
		execution of the workflow. For memory-intensive workflows, the runtime should assign fewer tasks at
		the same time to a single worker, or reduce overall memory consumption in some other way, e.g.\ by
		keeping less cached data objects in the worker's memory.
\end{description}

\section*{Summary}
Now that we have seen what kind of task-based programming models are the main focus of this thesis,
and we have defined a vocabulary of task related terms, we can move on to the existing challenges
that users and task runtimes face when they execute task graphs on \gls{hpc} clusters.
