The main goal of this thesis was to design and implement approaches that would enable efficient
and ergonomic execution of task graphs on heterogeneous supercomputers. This goal was divided
into three objectives:

\begin{enumerate}
	\item Identify and analyze existing challenges and bottlenecks in this area.
	\item Design a set of general approaches for overcoming these challenges.
	\item Implement and evaluate an \gls{hpc}-optimized task runtime that leverages the proposed
	approaches.
\end{enumerate}

Below you can find a description of how were these objectives fulfilled.

The main challenges affecting \gls{hpc} task graph execution were described in~\Autoref{ch:sota}, which
has identified several areas that can cause problems when executing task graphs on modern
heterogeneous \gls{hpc} clusters, described how are existing tools able to deal with them and
outlined motivation for the work presented in the rest of the thesis.

The following two chapters focused on the performance and efficiency aspects of executing task
graphs.~\Autoref{ch:estee} has introduced \estee{}, a simulation
environment designed for prototyping task schedulers and benchmarking various aspects of task graph
execution. We have used \estee{} to perform a comprehensive study of several task
scheduling algorithms and examine several hypotheses of the effect of various factors on the
performance of the scheduler. Our experiments gave us insight into the quality of the evaluated
schedulers.

\Autoref{ch:rsds} focused on the performance bottlenecks of a real-world task runtime
\dask{} in a non-simulated environment. Our analysis
has shown that \dask{} is severely limited by its implementation
characteristics, moreso than its used scheduling algorithm. It is primarily limited by its choice
of Python as an implementation language, which introduces massive overhead particularly for
\gls{hpc} use-cases. We have proposed and implemented~\rsds{},
a backwards-compatible implementation of the \dask{} server written in Rust,
which was designed to minimize runtime overhead. Our experiments demonstrated that such an
optimized server implementation can improve the scalability and end-to-end performance of
\dask{} workflows by several times, even though it uses a simpler scheduling
algorithm.

Finally,~\Autoref{ch:hyperqueue} introduced a general meta-scheduling approach designed to avoid
complications caused by interacting with \gls{hpc} allocation managers and to efficiently manage
complex resources of modern heterogeneous clusters. The described method completely separates task
submission from computational resource provisioning, which enables fully dynamic and automatic load
balancing even across different allocations. It also introduces resource management concepts
designed to improve hardware utilization of heterogeneous clusters.

The described meta-scheduling and resource management approach, along with insights gained from
the performed scheduler evaluation and task runtime optimization, was leveraged in the
implementation of \hyperqueue{}, an \gls{hpc}-optimized task runtime that facilitates
efficient execution of task graphs in the presence of allocation managers. The most important
features of \hyperqueue{}, such as comprehensive support for heterogeneous resource management and
multi-node tasks, fault-tolerant task execution and automatic submission of allocations, have been described in~\Autoref{ch:hyperqueue}. Its overhead and scalability were also evaluated on several benchmarks designed to
examine its resource management capabilities and push it to its performance limits. The results of
these experiments indicate that it does not introduce significant overhead, it can be used to
scale task graphs to a large amount of computational resources and it can
efficiently utilize heterogeneous resources.

The following list describes how \hyperqueue{} deals with the challenges described
in~\Autoref{ch:sota} and also which improvements could be made to it as future work, in order to
further improve its ability to provide ergonomic and efficient task graph execution on
heterogeneous \gls{hpc} clusters.
\begin{description}[wide=0pt]
	\item[Allocation manager] The used meta-scheduling approach removes the need for the workflow author to think about mapping
		tasks to allocations or dealing with various allocation limits. And thanks to the automatic
		allocator, users do not even need to submit any allocations by themselves. The automatic allocator
		could be extended with task duration or allocation start time predictions in the future, to improve
		its decisions on when to actually submit allocations.
	\item[Cluster heterogeneity] \hyperqueue{} provides comprehensive support for heterogeneous
		clusters by enabling tasks to specify arbitrary resource requirements and by matching these
		requirements with resources provided by workers. In addition to supporting arbitrary resource
		kinds, \gls{cpu} core pinning and time requests, it also supports complex resource
		requirements in the form of non-fungible resources, related resources, fractional resource
		requirements and resource variants, which can deal with the most complex resource management
		scenarios. To our knowledge, there is no other state-of-the-art task runtime that
		implements all these concepts.

		Workers also provide automatic detection of available
		resources, which further improves the ergonomics of deploying computational providers on
		heterogeneous clusters.
	\item[Performance and scalability] Experiments presented in~\Autoref{hq:evaluation} demonstrate that \hyperqueue{} is
		able to scale to \gls{hpc}-sized workflows and that it does not introduce
		significant overhead over executing tasks manually.

		For use-cases that are limited by \gls{io} bandwidth due to creating too many
		output files on distributed filesystems, \hyperqueue{} offers
		\emph{output streaming}, which is able to avoid filesystem limitations by streaming task outputs
		to the server and storing it in a single file.

		The performance of \hyperqueue{} could be further improved by integrating
		\gls{hpc}-specific technologies, such as InfiniBand~\cite{infiniband} or
		\gls{mpi}, to speed up network communication and filesystem
		\gls{io}, or by adding support for stateful task environments. These could help
		avoid the need to create a separate Linux process for each executed task, which can have a
		non-trivial overhead on certain \gls{hpc} clusters, as was demonstrated by our
		experiments.
	\item[Fault tolerance] \hyperqueue{} is fault-tolerant by default; tasks that do not
		finish computing successfully due to reasons outside their control are automatically rescheduled to
		a different worker, without requiring any manual user intervention. Workers are designed to be
		transient; because they are usually executed in relatively short-running allocations, their
		failures are handled gracefully. The server itself is also fault-tolerant and can reload its task
		database after being restarted, which enables continuous execution of task graphs even in the case
		of e.g.\ login node failures.
	\item[Multi-node tasks] \hyperqueue{} provides built-in support for multi-node tasks and
		can even combine them with standard single-node tasks within the same task graph. It also provides
		basic integration with popular \gls{hpc} technologies like
		\gls{mpi} to make their usage in multi-node tasks easier. Multi-node tasks could
		be further extended to be more granular, so that a multi-node task would not necessarily have to
		use its whole node for execution. For some use-cases it would also be useful to have an
		option to combine multi-node tasks with data transfers.
	\item[Deployment] In terms of ease-of-deployment, \hyperqueue{} is essentially optimal; it is
		distributed as a single binary that runs fully in user-space and that does not have any
		dependencies. Its users thus do not have to install any dependencies or deploy complex services on
		the target supercomputer (which can in some cases be very difficult or even impossible) in order to
		use it. It is also simple to make it completely self-contained by statically linking a
		\texttt{C} standard library into it, which would remove its only runtime dependency
		on the \texttt{glibc} \texttt{C} standard library implementation. Its
		Python \gls{api}, which is an optional component, is distributed as a standard
		Python package that can be easily installed using standard Python package management tools on a
		variety of Python versions.
	\item[Programming model] \hyperqueue{} allows defining task graphs through three
		interfaces; a \gls{cli}, a workflow file, or a Python \gls{api},
		so that it can support a wide range of use-cases. Its task graphs can also be modified even after
		being submitted, which allows expressing dynamic use-cases, such as iterative computation.

		\hyperqueue{} does not currently support direct data transfers
		between tasks, i.e.\ enabling tasks to pass their outputs as inputs to dependent tasks through
		network communication. This limits its applicability (or at least its ergonomics) in scenarios that
		need to frequently exchange many outputs between tasks. Adding support for data transfers
		would expand the set of workflows that can be naturally expressed with its Python
		\gls{api} and in certain cases it could also improve performance by avoiding the need to
		exchange task outputs through the filesystem.
\end{description}

%To summarize, this thesis makes the following contributions:
%\begin{itemize}
%	\item It introduces a task graph simulator for evaluating the quality of task schedulers under various
%	      conditions and provides an extensive evaluation of several scheduling algorithms using this
%	      simulator.
%	\item It provides an analysis of the performance bottlenecks of a state-of-the-art task runtime
%	      \dask{} and introduces an alternative implementation of its server that provides
%	      significant performance improvements in \gls{hpc} scenarios while retaining
%	      backwards compatibility.
%	\item Primarily, it proposes a design for effortless and efficient execution of task graphs on
%	      heterogeneous clusters in the presence of \gls{hpc} allocation managers and
%	      provides an implementation of this design in the \hyperqueue{} task runtime.
%\end{itemize}

\section{Impact}
This final section summarizes the impact that \rsds{} and
\hyperqueue{} had on real-world projects.

\vspace{-2mm}\subsection*{\rsds{}}
After we had an indication that \rsds{} could be leveraged to improve the
efficiency of existing \dask{} workflows, we presented our \rsds{} research to maintainers of \dask{}. Although replacing their server implementation or switching from Python
to a different implementation language was not a feasible approach for them, some of the ideas
implemented in \rsds{} have since been adapted in the \dask{}
project. This helped to alleviate some of the bottlenecks that we have discovered with our
experiments and improved the performance of \dask{} in
general\footnoteurl{https://github.com/dask/distributed/issues/3139}\footnoteurl{https://github.com/dask/distributed/issues/3783}\footnoteurl{https://github.com/dask/distributed/issues/3872}.

\vspace{-2mm}\subsection*{\hyperqueue{}}
\hyperqueue{} has already been adopted in several projects and it is also actively
being used by various researchers and teams across several European \gls{hpc}
centers. It has been proposed as one of the designated ways for executing
\gls{hpc} computations in several supercomputing centers, such as
LUMI~\cite{it4i-lumi}, CSC-FI~\cite{puhti-hq,puhti-hq-2},
IT4Innovations~\cite{it4i-hq} or CINECA~\cite{cineca}. It is also
available in a precompiled form on several clusters managed by these centers.

\hyperqueue{} can also be integrated as a general task execution system into other
tools, thanks to its sophisticated resource management and task scheduling capabilities. This has
been leveraged by several workflow management systems that have integrated
\hyperqueue{} as one of their task execution backends, such as
Aiida~\cite{aiida-hq}, NextFlow~\cite{nextflow-hq},
UM-Bridge~\cite{umbridge}, StreamFlow~\cite{streamflow-hq},
ERT~\cite{ert} or HEAppE~\cite{heappe_hq}.

\hyperqueue{} is also used in various research projects. Scientists from the Czech
Academy of Sciences use it to execute simulations that analyze data from the
ATLAS~\cite{atlas} experiment performed at CERN. Thanks to
\hyperqueue{}, they were able to improve the achieved hardware utilization on the
IT4Innovations Karolina~\cite{karolina} supercomputer by 30\%, which saves them tens of
hundreds of node hours per year~\cite{cern-hq}. \hyperqueue{} has also
been used to execute workflows in several projects funded by the European Union, such as
EVEREST~\cite{everest}, ACROSS~\cite{across},
EXA4MIND~\cite{exa4mind} and MaX~\cite{max}. It was especially useful
for the LIGATE~\cite{ligate} project, where it was used to implement several
\gls{md} workflows that were executed using hundreds of thousands of
\gls{cpu} and \gls{gpu} hours on the most powerful European
supercomputers.

Given the use-cases mentioned above, I believe that the practical applicability of the proposed
task graph execution design has been demonstrated and that this thesis has thus achieved the goals
that it originally set out to. I am confident that \hyperqueue{} provides a tangible
benefit in terms of ergonomic and efficient execution of task graphs on supercomputers and that it
resolves most of the challenges that have been described extensively in this thesis through its
\gls{hpc}-driven design. I hope that \hyperqueue{} will eventually
see even more widespread usage in the \gls{hpc} community.
