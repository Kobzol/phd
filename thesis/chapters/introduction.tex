\gls{hpc} infrastructures are crucial for the advancement of scientific
research, as they offer unparalleled computational power that can be leveraged to perform the most
complex scientific experiments. This massive performance is required (among other use-cases) in
various scientific areas, such as weather forecasting~\cite{wrf}, computational fluid
dynamics~\cite{cfd}, bioinformatics~\cite{bioinformatics} or deep
learning~\cite{hpcdl}.

Over the last several decades, the performance of \gls{hpc} clusters
(\emph{supercomputers}) has been steadily increasing, effectively doubling every few years, in
line with Moore's Law and Dennard scaling~\cite{mooreslaw}. However, it also became more
difficult for \gls{hpc} users to tap into that performance increase. Thirty years ago,
it was possible to get essentially double the performance for free, just by using a new
(super)computer every two years, without having to modify existing programs. This phenomenon has
started to diminish by the end of the last century, as chip designers became limited by the memory
wall~\cite{memorywall} and especially the power wall~\cite{powerwall}.

To keep up with the expectations of exponential performance increases, \glspl{cpu} had
to become more complex. Processor manufacturers started implementing various buffers and caches,
multiple cores, simultaneous multithreading, out-of-order execution and a plethora of other
techniques that would allow the \gls{cpu} to run faster, without requiring massive
increases of power draw or memory bandwidth. The existence of multiple cores and sockets and the
need for ever-increasing memory sizes has also made the memory system more complex, with
\gls{numa} memories becoming commonplace in \gls{hpc}. To achieve even
more performance, \gls{hpc} clusters started massively adopting various accelerators,
like the Intel Xeon Phi~\cite{xeonphi} manycore coprocessor or general-purpose NVIDIA or
AMD \glspl{gpu}, which eventually became the backbone of the majority of current
supercomputers~\cite{top500gpu}. Some clusters have also adapted more unconventional
accelerators, like reconfigurable hardware, such as \glspl{fpga}, or
\gls{ai} accelerators, such as \glspl{tpu}. This trend gave rise to
heterogeneous clusters that offer various types of hardware, each designed for specific workloads.

These hardware improvements have managed to keep up with Moore's Law, but no longer without
requiring changes to the software. The increasing complexity and heterogeneity of
\gls{hpc} hardware has caused the ``\gls{hpc} software stack'' and the
corresponding programming models to become more complex, making it far from trivial to leverage the
available performance offered by supercomputers. Individual computers of \gls{hpc}
clusters (called \emph{computing nodes}) can consist of hundreds of \gls{cpu} cores
each, yet it is challenging to write programs that can scale to such high core counts. The
\gls{ram} of each node contains multiple levels of complex cache hierarchies, and it
has such a large capacity that it has to be split into multiple physical locations with varying
access latencies (\gls{numa}), which requires usage of specialized programming
techniques to achieve optimal performance. And the ever-present accelerators, for example
\glspl{gpu}, might require their users to adopt completely different programming models
and frameworks.

Historically, optimized \gls{hpc} software was usually written using system or
scientifically focused programming languages (e.g.~\texttt{C}, \texttt{C++}
or \texttt{Fortran}) and specialized libraries for parallelizing and distributing
computation, such as \gls{openmp}~\cite{openmp}, CUDA~\cite{cuda} or
\gls{mpi}~\cite{mpistudy}. While these rather low-level technologies are able
to provide the best possible performance, it can be quite challenging and slow to develop (and
maintain) applications that use them. It is unreasonable to expect that most domain scientists that
develop software for \gls{hpc} clusters (who are often not primarily software
developers) will be able to use all these technologies in an efficient manner without making the
development process slow and cumbersome. This task should be left to specialized performance
engineers, enabling the scientists to focus on the problem domain~\cite{dace}.

With the advent of more powerful hardware, \gls{hpc} systems are able to solve new
problems, which are more and more demanding, both in terms of the required computational power, but
also in terms of data management, network communication patterns and general software design and
architecture. Areas such as weather prediction, machine learning model training or big data
analysis require executing thousands or even millions of simulations and experiments. These
experiments can be very complex, consisting of multiple dependent steps, such as data ingestion,
preprocessing, computation, postprocessing, visualization, etc. It is thus imperative for
scientists to have a quick way of prototyping these applications, otherwise their development
process would be too slow.

The growing complexity of \gls{hpc} hardware, software and use-cases gave rise to the
popularity of (distributed) task-based programming models and paradigms. Task-oriented programming
models allow users to focus on their problem domain and quickly prototype, while still being able
to describe complicated computations with a large number of individual steps and to efficiently
utilize the available computational resources. Using a task-based approach, a computational
workflow is described using a set of atomic computational blocks (\emph{tasks}) that are
composed together in a \emph{task graph} which captures dependencies between the individual
tasks. Task graphs abstract away most of the complexity of network communication and
parallelization, and they are general enough to describe a large set of programs in a practical and
simple way. At the same time, they remain amenable to compiler-driven optimization and automatic
parallelization, which helps to bring the performance of programs described by a task graph close
to manually parallelized and distributed programs, at a fraction of the development cost for the
application developer. They are also relatively portable by default, as the task graph programming model
typically does not make many assumptions about the target platform, therefore the same task graph can be
executed on various systems and clusters, provided there is a task execution runtime implemented
for that cluster.

Combined with the fact that task-based tools often allow users to implement their workflow in very
high-level languages, such as Python, or various \glspl{dsl}, it makes them an ideal
tool for rapid scientific prototyping.

Task graphs are already commonly being used and deployed on various distributed
systems~\cite{pegasus, workflows_at_scale, large_scale_modelling}, yet there are certain challenges that limit their usage ergonomics
and performance efficiency when deployed specifically on \gls{hpc} systems. These
challenges stem from various factors, such as the interaction of task graphs with
\gls{hpc} allocation managers, the heterogeneity and complexity of
\gls{hpc} cluster hardware, or simply from the potentially enormous computational
scale. When task graph authors encounter these problems, they might have to step out of the comfort
zone of this easy-to-use programming model, and implement parts of their applications using other,
more complicated approaches, to either meet their performance goals or to even make it possible to
execute their application on \gls{hpc} clusters at all. Removing or alleviating some
of those challenges could lower the barrier of entry, make task graph execution better suited for
various \gls{hpc} use-cases and turn it into an actual first-class citizen in the
world of supercomputing\@.

This thesis sets out to identify and analyze existing challenges and bottlenecks of task graph
execution on \gls{hpc} clusters, particularly in the areas of usage ergonomics and
efficient hardware utilization, and design approaches to alleviate them.

The primary objective of this thesis is to design approaches that would enable users to execute
task graphs on \gls{hpc} clusters with minimum effort, while ensuring efficient
utilization of the used \gls{hpc} hardware. It makes the following contributions
towards this objective.

\begin{itemize}
	\item It introduces a task graph simulator for evaluating the quality of task schedulers under various
	      conditions, and provides an extensive evaluation of several scheduling algorithms using this
	      simulator.
	\item It provides an analysis of the performance bottlenecks of a state-of-the-art task runtime
	      \dask{}, and introduces an alternative implementation of its server that provides
	      significant performance improvements in \gls{hpc} scenarios, while retaining backwards
	      compatibility.
	\item Primarily, it proposes an approach for effortless execution of task graphs in the presence of
	      \gls{hpc} allocation managers, and provides an implementation of this approach in
	      \hyperqueue{}, an \gls{hpc}-tailored task runtime that enables ergonomic and
	      efficient execution of task graphs on heterogeneous supercomputers.
\end{itemize}

The thesis is structured as follows. \Autoref{ch:distributed-computing} describes various approaches for
designing parallelized programs on distributed clusters and introduces a specific subset of
task-based programming models relevant for this thesis. \Autoref{ch:taskgraphs} then describes the
relevant task-based programming model in detail, by defining key terms related to tasks and task
graphs. It is followed by~\Autoref{ch:sota}, which discusses various ergonomic challenges and
performance bottlenecks faced by state-of-the-art distributed task runtimes when executing task
graphs on \gls{hpc} systems. The following three chapters then discuss designs for
overcoming these challenges. \Autoref{ch:estee} evaluates the performance of various task
scheduling algorithms and introduces \estee{}, a task graph execution simulator.
\Autoref{ch:rsds} analyzes the runtime performance of \dask{}, a
state-of-the-art task runtime, and proposes an alternative implementation of its server that is
able to outperform it in various \gls{hpc} use-cases. \Autoref{ch:hyperqueue} then
focuses on improving both the ergonomical and performance aspects of task execution using a
meta-scheduling approach designed to facilitate task graph execution on \gls{hpc}
clusters. This approach has been implemented in the \hyperqueue{} task runtime, which is
also described in this chapter in detail. Finally,~\Autoref{ch:conclusion} summarizes the thesis and
outlines future work.

%\begin{figure}
%	\centering
%	\begin{tikzpicture}[>=latex,line join=bevel,every text node part/.style={align=center}]
%		\tikzset {chapter/.style={rectangle, draw, minimum width=5cm, font=\footnotesize}}
%
%		\newcommand{\chname}[1]{\emph{\nameref{#1}}}
%		\newcommand{\chnum}[1]{\Autoref{#1}}
%
%		\node (distributed-computing) [chapter]
%		{\chname{ch:distributed-computing} \\\chnum{ch:distributed-computing}};
%
%		\node (taskgraphs) [chapter, below=of distributed-computing]
%		{\chname{ch:taskgraphs} \\\chnum{ch:taskgraphs}};
%
%		\node (estee) [chapter, below=of taskgraphs, xshift=3cm]
%		{\chname{ch:estee} \\\chnum{ch:estee}};
%
%		\node (challenges) [chapter, below=of taskgraphs,left=of estee, yshift=-1cm]
%		{\chname{ch:sota} \\\chnum{ch:sota}};
%
%		\node (rsds) [chapter, below=of estee]
%		{\chname{ch:rsds} \\\chnum{ch:rsds}};
%
%		\node (hq) [chapter, below=of rsds, xshift=-3cm]
%		{\chname{ch:hyperqueue} \\\chnum{ch:hyperqueue}};
%
%		\draw [->] (distributed-computing.south) -- (taskgraphs.north);
%		\draw [->] (taskgraphs.south) -- (challenges.north);
%		\draw [->] (challenges.south) -- (hq.north);
%		\draw [->] (taskgraphs.south) -- (estee.north);
%		\draw [->] (estee.south) -- (rsds.north);
%		\draw [->] (rsds.south) -- (hq.north);
%	\end{tikzpicture}
%	\caption{Diagram of the thesis chapters}
%	\label{fig:thesis-chapter-diagram}
%\end{figure}
