\gls{hpc} infrastructures are crucial for the advancement of scientific
research, as they offer unparalleled computational power that can be leveraged to perform the most
complex scientific experiments. The performance offered by \gls{hpc} clusters is
crucial (among other use-cases) in various scientific areas, such as weather
forecasting~\cite{wrf}, computational fluid dynamics~\cite{cfd},
bioinformatics~\cite{bioinformatics} or deep learning~\cite{hpcdl}.

Over the last several decades, the performance of \gls{hpc} clusters has been steadily
increasing, effectively doubling every few years, in line with Moore's Law and Dennard
scaling~\cite{mooreslaw}. However, it also became more difficult for \gls{hpc}
users to tap into that performance increase. Thirty years ago, it was possible to get essentially
double the performance for free, just by using a new (super)computer every two years, without
having to modify existing programs. This phenomenon has started to diminish by the end of the last
century, as chip designers became limited by the memory wall~\cite{memorywall} and especially
the power wall~\cite{powerwall}.

To keep up with the expectations of exponential performance increases, \glspl{cpu} had
to become more complex. Processor manufacturers started implementing various buffers and caches,
multiple cores, simultaneous multithreading, out-of-order execution and a plethora of other
techniques that would allow the \gls{cpu} to run faster, without requiring massive
increases of power draw or memory bandwidth. The existence of multiple cores and sockets and the
need for ever-increasing memory sizes has also made the memory system more complex, with
\gls{numa} memories becoming commonplace in \gls{hpc}. To achieve even
more performance, \gls{hpc} clusters started massively adopting various accelerators,
like the Intel Xeon Phi~\cite{xeonphi} manycore coprocessor or general-purpose
\glspl{gpu} from NVIDIA or AMD, which eventually became the backbone of the majority of
current supercomputers~\cite{top500gpu}. Some clusters have also adapted more unconventional
accelerators, like reconfigurable hardware, such as \glspl{fpga}, or
\gls{ai} accelerators, such as \glspl{tpu}. This trend gave rise to
heterogeneous clusters that offer various types of hardware, each designed for specific workloads.

These hardware improvements have managed to keep up with Moore's Law, but no longer without
requiring changes to the software. The increasing complexity and heterogeneity of
\gls{hpc} hardware has caused the ``\gls{hpc} software stack'' and the
corresponding programming models to become more complex, making it far from trivial to leverage the
available performance offered by supercomputers. Individual computers of \gls{hpc}
clusters (called \emph{computing nodes}) can consist of hundreds of \gls{cpu} cores
each, yet it is quite challenging to write programs that can scale to such high core counts. The
operational \gls{ram} of each node contains multiple levels of complex cache
hierarchies, and it has such a large capacity that it has to be split into multiple physical
locations with varying access latencies (\gls{numa}), which requires usage of
specialized programming techniques to achieve optimal performance. And the ever-present
accelerators, for example \glspl{gpu}, might require their users to adopt completely
different programming models and frameworks.

Historically, optimized \gls{hpc} software was usually written using system or
scientifically focused programming languages (e.g.~\texttt{C}, \texttt{C++}
or \texttt{Fortran}) and specialized libraries for parallelizing and distributing
computation, such as \gls{openmp}~\cite{openmp}, CUDA~\cite{cuda} or
\gls{mpi}~\cite{mpistudy}. While these rather low-level technologies are able
to provide the best possible performance, it can be quite challenging and slow to develop (and
maintain) applications that use them. It is unreasonable to expect that most domain scientists that
develop software for \gls{hpc} clusters (who are often not primarily software
developers) will be able to use all these technologies in an efficient manner without making the
development process slow and cumbersome. This task should be left to specialized performance
engineers, enabling the scientists to focus on the problem domain~\cite{dace}.

With the advent of more powerful hardware, \gls{hpc} systems are able to solve new
problems, which are more and more demanding, both in terms of the required computational power, but
also in terms of data management, network communication patterns and general software design and
architecture. Areas such as weather prediction, machine learning model training or big data
analysis require executing thousands or even millions of simulations and experiments. These
experiments can be quite complex, consisting of multiple dependent steps, such as data ingestion,
preprocessing, computation, postprocessing, visualisation, etc. It is thus imperative for
scientists to have a quick way of prototyping these applications, otherwise their development
process would be too slow.

The growing complexity of \gls{hpc} hardware, software and use-cases gave rise to the
popularity of (distributed) task-based programming models and paradigms. Task-oriented programming
models allow users to focus on their problem domain and quickly prototype, while still being able
to describe complicated computations with a large number of individual steps and to efficiently
utilize the available computational resources. Using a task-based approach, a computational
workflow is described using a set of atomic computational blocks (\emph{tasks}) that are
composed together in a \emph{task graph} which captures dependencies between the individual
tasks. Task graphs abstract away most of the complexity of network communication and
parallelization, and they are general enough to describe a large set of programs in a practical and
simple way. At the same time, they remain amenable to compiler-driven optimization and automatic
parallelization, which helps to bring the performance of programs described by a task graph close
to manually parallelized and distributed programs, at a fraction of the development cost for the
application developer. They are also quite portable, as the task graph programming model typically
does not make many assumptions about the target platform, therefore the same task graph can be
executed on various systems and clusters, provided there is a task execution runtime implemented
for that cluster.

Combined with the fact that task-based tools often allow users to implement their workflow in very
high-level languages, such as Python, or various \glspl{dsl}, it makes them an ideal
tool for rapid scientific prototyping.

Task graphs are already commonly being used and deployed on various distributed
systems~\cite{pegasus, workflows_at_scale, large_scale_modelling}, yet there are certain challenges that limit their usage ergonomics
and performance efficiency when deployed specifically on \gls{hpc} systems. These
challenges stem from various factors, such as the interaction of task graphs with
\gls{hpc} allocation managers, the heterogeneity and complexity of
\gls{hpc} cluster hardware, or simply from the potentially enormous computational
scale. When task graph authors encounter these problems, they might have to step out of the comfort
zone of this easy-to-use programming model, and implement parts of their applications using other,
more complicated approaches, to either meet their performance goals or to even make it possible to
execute their application on \gls{hpc} clusters at all. Removing or alleviating some
of those challenges could lower the barrier of entry, make task graph execution better suited for
various \gls{hpc} use-cases and turn it into an actual first-class citizen in the
world of supercomputing\@.

This thesis sets out to identify and analyse the existing challenges and bottlenecks, and design
approaches to alleviate them, in order to improve the execution of task graphs on
\gls{hpc} systems in two main areas, namely usage ergonomics and efficient hardware
utilization. It aims to achieve this objective via the following contributions: It introduces a
task graph simulator for evaluating the quality of task schedulers under various conditions, and
provides an extensive evaluation of several scheduling algorithms using this simulator. It provides
an analysis of the performance bottlenecks of a state-of-the-art task runtime
\dask{}, and introduces an alternative implementation of its server that provides
significant performance improvements in \gls{hpc} scenarios, while retaining backwards
compatibility. And primarily, it proposes an approach for effortless execution of task graphs in
the presence of \gls{hpc} allocation managers, and provides an implementation of this
approach in \hyperqueue{}, an \gls{hpc}-tailored task runtime that enables
ergonomic execution of task graphs on heterogeneous supercomputers with a focus on efficient usage
of hardware resources.

\Autoref{fig:thesis-chapter-diagram} contains a simple diagram that shows in which order you can read
the individual chapters of the thesis, and also how do they relate to the two main topics of this
thesis (\emph{ergonomics} and \emph{performance} of task graph execution in
\gls{hpc} environments).

% TODO: add performance/ergonomics to diagram

\begin{figure}
	\centering
	\begin{tikzpicture}[>=latex,line join=bevel,every text node part/.style={align=center}]
		\tikzset {chapter/.style={rectangle, draw, minimum width=5cm, font=\footnotesize}}

		\newcommand{\chname}[1]{\emph{\nameref{#1}}}
		\newcommand{\chnum}[1]{\Autoref{#1}}

		\node (distributed-computing) [chapter]
		{\chname{ch:distributed-computing} \\\chnum{ch:distributed-computing}};

		\node (taskgraphs) [chapter, below=of distributed-computing]
		{\chname{ch:taskgraphs} \\\chnum{ch:taskgraphs}};

		\node (estee) [chapter, below=of taskgraphs, xshift=3cm]
		{\chname{ch:estee} \\\chnum{ch:estee}};

		\node (challenges) [chapter, below=of taskgraphs,left=of estee, yshift=-1cm]
		{\chname{ch:challenges} \\\chnum{ch:challenges}};

		\node (rsds) [chapter, below=of estee]
		{\chname{ch:rsds} \\\chnum{ch:rsds}};

		\node (hq) [chapter, below=of rsds, xshift=-3cm]
		{\chname{ch:hyperqueue} \\\chnum{ch:hyperqueue}};

		\draw [->] (distributed-computing.south) -- (taskgraphs.north);
		\draw [->] (taskgraphs.south) -- (challenges.north);
		\draw [->] (challenges.south) -- (hq.north);
		\draw [->] (taskgraphs.south) -- (estee.north);
		\draw [->] (estee.south) -- (rsds.north);
		\draw [->] (rsds.south) -- (hq.north);
	\end{tikzpicture}
	\caption{Diagram of the thesis chapters}
	\label{fig:thesis-chapter-diagram}
\end{figure}

\Autoref{ch:distributed-computing} introduces various approaches for designing parallelized programs
on distributed clusters, and categorizes programming models based on computational workflows.
\Autoref{ch:taskgraphs} then describes the task-based programming model in detail, and defines key
terms related to tasks and task graphs. \Autoref{ch:challenges} outlines various ergonomic
challenges and performance bottlenecks that affect execution of task graphs on
\gls{hpc} systems. The two subsequent chapters focus on the performance aspects of
task graph execution. \Autoref{ch:estee} evaluates the performance of various task scheduling
algorithms, and introduces \estee{}, a task graph execution simulator.
\Autoref{ch:rsds} then analyses the runtime performance of \dask{}, a
state-of-the-art task runtime, and proposes an alternative implementation of its server that
is able to outperform \dask{} in various \gls{hpc} use-cases.
\Autoref{ch:hyperqueue} focuses on improving both the ergonomics and performance aspects of task
execution in the presence of \gls{hpc} allocation managers, using a meta-scheduling design
implemented by the \hyperqueue{} task runtime. Finally,~\Autoref{ch:conclusion} summarizes
the thesis and outlines future work.
