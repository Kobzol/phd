% Related
@article{rsds,
	title = {Runtime vs {Scheduler}: {Analyzing} {Dask}'s {Overheads}},
	copyright = {All rights reserved},
	shorttitle = {Runtime vs {Scheduler}},
%	url = {http://arxiv.org/abs/2010.11105},
	doi = {10.1109/WORKS51914.2020.00006},
	abstract = {Dask is a distributed task framework which is commonly used by data scientists to parallelize Python code on computing clusters with little programming effort. It uses a sophisticated work-stealing scheduler which has been hand-tuned to execute task graphs as efficiently as possible. But is scheduler optimization a worthwhile effort for Dask? Our paper shows on many real world task graphs that even a completely random scheduler is surprisingly competitive with its built-in scheduler and that the main bottleneck of Dask lies in its runtime overhead. We develop a drop-in replacement for the Dask central server written in Rust which is backwards compatible with existing Dask programs. Thanks to its efficient runtime, our server implementation is able to scale up to larger clusters than Dask and consistently outperforms it on a variety of task graphs, despite the fact that it uses a simpler scheduling algorithm.},
%	urldate = {2021-10-22},
	journal = {2020 IEEE/ACM Workflows in Support of Large-Scale Science (WORKS)},
	author = {Böhm, Stanislav and Beránek, Jakub},
	month = nov,
	year = {2020},
%	note = {arXiv: 2010.11105},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing},
	pages = {1--8},
%	file = {arXiv.org Snapshot:/home/kobzol/Zotero/storage/WYDPUR69/2010.html:text/html},
	options = {maxbibnames = 99}
}
@article{estee,
	author="Ber{\'a}nek, Jakub
    and B{\"o}hm, Stanislav
    and Cima, Vojt{\v{e}}ch",
	title="Analysis of workflow schedulers in simulated distributed environments",
	journal="The Journal of Supercomputing",
	year="2022",
	month="Sep",
	day="01",
	volume="78",
	number="13",
	pages="15154--15180",
	issn="1573-0484",
	doi="10.1007/s11227-022-04438-y",
	%url="https://doi.org/10.1007/s11227-022-04438-y",
	options = {maxbibnames = 99}
}
@article{spin,
	title = {Network-{Accelerated} {Non}-{Contiguous} {Memory} {Transfers}},
	copyright = {All rights reserved},
	url = {http://arxiv.org/abs/1908.08590},
	doi = {10.1145/3295500.3356189},
	%	abstract = {Applications often communicate data that is non-contiguous in the send- or the receive-buffer, e.g., when exchanging a column of a matrix stored in row-major order. While non-contiguous transfers are well supported in HPC (e.g., MPI derived datatypes), they can still be up to 5x slower than contiguous transfers of the same size. As we enter the era of network acceleration, we need to investigate which tasks to offload to the NIC: In this work we argue that non-contiguous memory transfers can be transparently networkaccelerated, truly achieving zero-copy communications. We implement and extend sPIN, a packet streaming processor, within a Portals 4 NIC SST model, and evaluate strategies for NIC-offloaded processing of MPI datatypes, ranging from datatype-specific handlers to general solutions for any MPI datatype. We demonstrate up to 10x speedup in the unpack throughput of real applications, demonstrating that non-contiguous memory transfers are a first-class candidate for network acceleration.},
	%	urldate = {2021-10-22},
	journal = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
	author = {Di Girolamo, Salvatore and Taranov, Konstantin and Kurth, Andreas and Schaffner, Michael and Schneider, Timo and Beránek, Jakub and Besta, Maciej and Benini, Luca and Roweth, Duncan and Hoefler, Torsten},
	month = nov,
	year = {2019},
	%	note = {arXiv: 1908.08590},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Networking and Internet Architecture},
	pages = {1--14},
	%	file = {arXiv.org Snapshot:/home/kobzol/Zotero/storage/QIY8ZF6E/1908.html:text/html},
	options = {maxbibnames = 99}
}
@inproceedings{spin2,
	title = {A {RISC}-{V} in-network accelerator for flexible high-performance low-power packet processing},
	copyright = {All rights reserved},
	doi = {10.1109/ISCA52012.2021.00079},
	%	abstract = {The capacity of offloading data and control tasks to the network is becoming increasingly important, especially if we consider the faster growth of network speed when compared to CPU frequencies. In-network compute alleviates the host CPU load by running tasks directly in the network, enabling additional computation/communication overlap and potentially improving overall application performance. However, sustaining bandwidths provided by next-generation networks, e.g., 400 Gbit/s, can become a challenge. sPIN is a programming model for in-NIC compute, where users specify handler functions that are executed on the NIC, for each incoming packet belonging to a given message or flow. It enables a CUDA-like acceleration, where the NIC is equipped with lightweight processing elements that process network packets in parallel. We investigate the architectural specialties that a sPIN NIC should provide to enable high-performance, low-power, and flexible packet processing. We introduce PsPIN, a first open-source sPIN implementation, based on a multi-cluster RISC-V architecture and designed according to the identified architectural specialties. We investigate the performance of PsPIN with cycle-accurate simulations, showing that it can process packets at 400 Gbit/s for several use cases, introducing minimal latencies (26 ns for 64 B packets) and occupying a total area of 18.5 mm2 (22 nm FDSOI).},
	booktitle = {2021 {ACM}/{IEEE} 48th {Annual} {International} {Symposium} on {Computer} {Architecture} ({ISCA})},
	author = {Di Girolamo, Salvatore and Kurth, Andreas and Calotoiu, Alexandru and Benz, Thomas and Schneider, Timo and Beránek, Jakub and Benini, Luca and Hoefler, Torsten},
	month = jun,
	year = {2021},
	note = {ISSN: 2575-713X},
	keywords = {Programming, Task analysis, Computational modeling, Computer architecture, in-network compute, Next generation networking, Open source software, packet processing, Silicon-on-insulator, special-ized architecture, sPIN},
	pages = {958--971},
	%	file = {IEEE Xplore Abstract Record:/home/kobzol/Zotero/storage/UNICS6V2/9499874.html:text/html},
	options = {maxbibnames = 99}
}

% Unrelated
@inproceedings{haydi,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Haydi: {Rapid} {Prototyping} and {Combinatorial} {Objects}},
	copyright = {All rights reserved},
	isbn = {978-3-319-90050-6},
	shorttitle = {Haydi},
	doi = {10.1007/978-3-319-90050-6_8},
%	abstract = {Haydi (http://haydi.readthedocs.io) is a framework for generating discrete structures. It provides a way to define a structure from basic building blocks and then enumerate all elements, all non-isomorphic elements, or generate random elements in the structure. Haydi is designed as a tool for rapid prototyping. It is implemented as a pure Python package and supports execution in distributed environments. The goal of this paper is to give the overall picture of Haydi together with a formal definition for the case of generating canonical forms.},
	language = {en},
	booktitle = {Foundations of {Information} and {Knowledge} {Systems}},
	publisher = {Springer International Publishing},
	author = {Böhm, Stanislav and Beránek, Jakub and Šurkovský, Martin},
	editor = {Ferrarotti, Flavio and Woltran, Stefan},
	year = {2018},
	keywords = {OwnRelated},
	pages = {133--149},
%	options = {maxbibnames = 99}
}
@inproceedings{traffic_simulator_1,
	address = {Cham},
	series = {Advances in {Intelligent} {Systems} and {Computing}},
	title = {Alternative {Paths} {Reordering} {Using} {Probabilistic} {Time}-{Dependent} {Routing}},
	copyright = {All rights reserved},
	isbn = {978-3-030-29029-0},
	doi = {10.1007/978-3-030-29029-0_22},
%	abstract = {In this paper we propose an innovative routing algorithm which takes into account stochastic properties of the road segments using the Probabilistic Time-Dependent Routing (PTDR). It can provide optimal routes for vehicles driving in a smart city based on a global view of the road network. We have implemented the algorithm in a distributed on-line service which can leverage heterogeneous resources such as Cloud or High Performance Computing (HPC) in order to serve a large number of clients simultaneously and efficiently. A preliminary experimental results using a custom traffic simulator are presented.},
	language = {en},
	booktitle = {Advances in {Networked}-based {Information} {Systems}},
	publisher = {Springer International Publishing},
	author = {Golasowski, Martin and Beránek, Jakub and Šurkovský, Martin and Rapant, Lukáš and Szturcová, Daniela and Martinovič, Jan and Slaninová, Kateřina},
	editor = {Barolli, Leonard and Nishino, Hiroaki and Enokido, Tomoya and Takizawa, Makoto},
	year = {2020},
	pages = {235--246},
%	options = {maxbibnames = 99}
}
@inproceedings{traffic_simulator_2,
	address = {Cham},
	series = {Advances in {Intelligent} {Systems} and {Computing}},
	title = {A {Distributed} {Environment} for {Traffic} {Navigation} {Systems}},
	copyright = {All rights reserved},
	isbn = {978-3-030-22354-0},
	doi = {10.1007/978-3-030-22354-0_27},
%	abstract = {Effective navigation and distribution of traffic flow in large cities has become a hot topic in recent years. The authors have developed an advanced server side routing system which, together with client side navigation systems, is able not only to navigate cars according to their routing requests, but also to distribute traffic flow within a city. The main goal of the paper is to propose a distributed environment used for an advanced server side navigation system with a focus on effective usage of computational resources for different tasks that need to be solved within the system. A combination of cloud and high performance computing resources in one environment is proposed. The authors also developed a simulator for testing these distributed computational resources. The system and the simulator were tested on the infrastructure at IT4Innovations National Supercomputing Center in the Czech Republic.},
	language = {en},
	booktitle = {Complex, {Intelligent}, and {Software} {Intensive} {Systems}},
	publisher = {Springer International Publishing},
	author = {Martinovič, Jan and Golasowski, Martin and Slaninová, Kateřina and Beránek, Jakub and Šurkovský, Martin and Rapant, Lukáš and Szturcová, Daniela and Cmar, Radim},
	editor = {Barolli, Leonard and Hussain, Farookh Khadeer and Ikeda, Makoto},
	year = {2020},
	pages = {294--304},
%	options = {maxbibnames = 99}
}
@article{di_girolamo_pspin_2021,
	title = {{PsPIN}: {A} high-performance low-power architecture for flexible in-network compute},
	copyright = {All rights reserved},
	shorttitle = {{PsPIN}},
	url = {http://arxiv.org/abs/2010.03536},
%	abstract = {The capacity of offloading data and control tasks to the network is becoming increasingly important, especially if we consider the faster growth of network speed when compared to CPU frequencies. In-network compute alleviates the host CPU load by running tasks directly in the network, enabling additional computation/communication overlap and potentially improving overall application performance. However, sustaining bandwidths provided by next-generation networks, e.g., 400 Gbit/s, can become a challenge. sPIN is a programming model for in-NIC compute, where users specify handler functions that are executed on the NIC, for each incoming packet belonging to a given message or flow. It enables a CUDA-like acceleration, where the NIC is equipped with lightweight processing elements that process network packets in parallel. We investigate the architectural specialties that a sPIN NIC should provide to enable high-performance, low-power, and flexible packet processing. We introduce PsPIN, a first open-source sPIN implementation, based on a multi-cluster RISC-V architecture and designed according to the identified architectural specialties. We investigate the performance of PsPIN with cycle-accurate simulations, showing that it can process packets at 400 Gbit/s for several use cases, introducing minimal latencies (26 ns for 64 B packets) and occupying a total area of 18.5 mm 2 (22 nm FDSOI).},
%	urldate = {2021-10-22},
	journal = {arXiv:2010.03536 [cs]},
	author = {Di Girolamo, Salvatore and Kurth, Andreas and Calotoiu, Alexandru and Benz, Thomas and Schneider, Timo and Beránek, Jakub and Benini, Luca and Hoefler, Torsten},
	month = jun,
	year = {2021},
%	note = {arXiv: 2010.03536},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Networking and Internet Architecture},
%	file = {arXiv.org Snapshot:/home/kobzol/Zotero/storage/NIIWBVEI/2010.html:text/html},
%	options = {maxbibnames = 99}
}
@inproceedings{sisa,
	author = {Besta, Maciej and Kanakagiri, Raghavendra and Kwasniewski, Grzegorz and Ausavarungnirun, Rachata and Ber\'{a}nek, Jakub and Kanellopoulos, Konstantinos and Janda, Kacper and Vonarburg-Shmaria, Zur and Gianinazzi, Lukas and Stefan, Ioana and Luna, Juan G\'{o}mez and Golinowski, Jakub and Copik, Marcin and Kapp-Schwoerer, Lukas and Di Girolamo, Salvatore and Blach, Nils and Konieczny, Marek and Mutlu, Onur and Hoefler, Torsten},
	title = {SISA: Set-Centric Instruction Set Architecture for Graph Mining on Processing-in-Memory Systems},
	year = {2021},
	isbn = {9781450385572},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
%	url = {https://doi.org/10.1145/3466752.3480133},
	doi = {10.1145/3466752.3480133},
%	abstract = {Simple graph algorithms such as PageRank have been the target of numerous hardware accelerators. Yet, there also exist much more complex graph mining algorithms for problems such as clustering or maximal clique listing. These algorithms are memory-bound and thus could be accelerated by hardware techniques such as Processing-in-Memory (PIM). However, they also come with non-straightforward parallelism and complicated memory access patterns. In this work, we address this problem with a simple yet surprisingly powerful observation: operations on sets of vertices, such as intersection or union, form a large part of many complex graph mining algorithms, and can offer rich and simple parallelism at multiple levels. This observation drives our cross-layer design, in which we (1) expose set operations using a novel programming paradigm, (2) express and execute these operations efficiently with carefully designed set-centric ISA extensions called SISA, and (3) use PIM to accelerate SISA instructions. The key design idea is to alleviate the bandwidth needs of SISA instructions by mapping set operations to two types of PIM: in-DRAM bulk bitwise computing for bitvectors representing high-degree vertices, and near-memory logic layers for integer arrays representing low-degree vertices. Set-centric SISA-enhanced algorithms are efficient and outperform hand-tuned baselines, offering more than 10 \texttimes{} speedup over the established Bron-Kerbosch algorithm for listing maximal cliques. We deliver more than 10 SISA set-centric algorithm formulations, illustrating SISA’s wide applicability.},
	booktitle = {MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture},
	pages = {282–297},
	numpages = {16},
	keywords = {Clique Listing, Clique Mining, Clique Enumeration, Graph Learning, Processing In Memory, Instruction Set Architecture, Subgraph Isomorphism, Parallel Graph Algorithms, Graph Pattern Matching, Graph Mining, Processing Near Memory, Graph Accelerators},
%	location = {Virtual Event, Greece},
	series = {MICRO '21},
%	options = {maxbibnames = 99}
}
@article{graphminesuite,
	author = {Besta, Maciej and Vonarburg-Shmaria, Zur and Schaffner, Yannick and Schwarz, Leonardo and Kwasniewski, Grzegorz and Gianinazzi, Lukas and Beranek, Jakub and Janda, Kacper and Holenstein, Tobias and Leisinger, Sebastian and Tatkowski, Peter and Ozdemir, Esref and Balla, Adrian and Copik, Marcin and Lindenberger, Philipp and Konieczny, Marek and Mutlu, Onur and Hoefler, Torsten},
	title = {GraphMineSuite: Enabling High-Performance and Programmable Graph Mining Algorithms with Set Algebra},
	year = {2021},
	issue_date = {July 2021},
	publisher = {VLDB Endowment},
	volume = {14},
	number = {11},
	issn = {2150-8097},
%	url = {https://doi.org/10.14778/3476249.3476252},
	doi = {10.14778/3476249.3476252},
%	abstract = {We propose GraphMineSuite (GMS): the first benchmarking suite for graph mining that facilitates evaluating and constructing high-performance graph mining algorithms. First, GMS comes with a benchmark specification based on extensive literature review, prescribing representative problems, algorithms, and datasets. Second, GMS offers a carefully designed software platform for seamless testing of different fine-grained elements of graph mining algorithms, such as graph representations or algorithm subroutines. The platform includes parallel implementations of more than 40 considered baselines, and it facilitates developing complex and fast mining algorithms. High modularity is possible by harnessing set algebra operations such as set intersection and difference, which enables breaking complex graph mining algorithms into simple building blocks that can be separately experimented with. GMS is supported with a broad concurrency analysis for portability in performance insights, and a novel performance metric to assess the throughput of graph mining algorithms, enabling more insightful evaluation. As use cases, we harness GMS to rapidly redesign and accelerate state-of-the-art baselines of core graph mining problems: degeneracy reordering (by &gt;2X), maximal clique listing (by &gt;9\texttimes{}), k-clique listing (by up to 1.1\texttimes{}), and subgraph isomorphism (by 2.5\texttimes{}), also obtaining better theoretical performance bounds.},
	journal = {Proc. VLDB Endow.},
	month = {jul},
	pages = {1922–1935},
	numpages = {14},
%	options = {maxbibnames = 99}
}
@inproceedings{smi,
	author = {De Matteis, Tiziano and de Fine Licht, Johannes and Ber\'{a}nek, Jakub and Hoefler, Torsten},
	title = {Streaming Message Interface: High-Performance Distributed Memory Programming on Reconfigurable Hardware},
	year = {2019},
	isbn = {9781450362290},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	%	url = {https://doi.org/10.1145/3295500.3356201},
	doi = {10.1145/3295500.3356201},
	%	abstract = {Distributed memory programming is the established paradigm used in high-performance computing (HPC) systems, requiring explicit communication between nodes and devices. When FPGAs are deployed in distributed settings, communication is typically handled either by going through the host machine, sacrificing performance, or by streaming across fixed device-to-device connections, sacrificing flexibility. We present Streaming Message Interface (SMI), a communication model and API that unifies explicit message passing with a hardware-oriented programming model, facilitating minimal-overhead, flexible, and productive inter-FPGA communication. Instead of bulk transmission, messages are streamed across the network during computation, allowing communication to be seamlessly integrated into pipelined designs. We present a high-level synthesis implementation of SMI targeting a dedicated FPGA interconnect, exposing runtime-configurable routing with support for arbitrary network topologies, and implement a set of distributed memory benchmarks. Using SMI, programmers can implement distributed, scalable HPC programs on reconfigurable hardware, without deviating from best practices for hardware design.},
	booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
	articleno = {82},
	numpages = {33},
	keywords = {reconfigurable computing, high-level synthesis tools, distributed memory programming},
	location = {Denver, Colorado},
	series = {SC '19},
	options = {maxbibnames = 4}
}
